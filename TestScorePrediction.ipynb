{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Project\n",
    "\n",
    "####  This project makes use of ANN( Artificial Neural Network) of predict the memory test when under certian medication. The data is based on the experiment of anti-anxiety medicine on memory recall when being primed with happy and sad memories. The drugs that are used are ( Alprazolam (label =A)), (Triazolam)(label= T), (Sugar Tablet)(label = S). The label for happy and sad memores are ( S for sadness) and (H for happyness). The participants were from various age and were given various doses of drug. \n",
    "#### The project has a correlation chart that gives good idea about the relationship between age and testscore, effect of medication on test socre and its effectiveness\n",
    "\n",
    "#### Source of data: http://www.jstor.org/stable/43854146, https://www.sciencedirect.com/science/article/pii/S0896627314008484,\n",
    "#### http://www.jstor.org/stable/40064315"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "# neural network imports\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from time import strftime\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a folder of tensorboard performace logs\n",
    "\n",
    "LOG_DIR='tensorboard_logs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Cleaning our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_data=pd.read_csv(\"Islander_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>age</th>\n",
       "      <th>Happy_Sad_group</th>\n",
       "      <th>Dosage</th>\n",
       "      <th>Drug</th>\n",
       "      <th>Mem_Score_Before</th>\n",
       "      <th>Mem_Score_After</th>\n",
       "      <th>Diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bastian</td>\n",
       "      <td>Carrasco</td>\n",
       "      <td>25</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>63.5</td>\n",
       "      <td>61.2</td>\n",
       "      <td>-2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Evan</td>\n",
       "      <td>Carrasco</td>\n",
       "      <td>52</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>41.6</td>\n",
       "      <td>40.7</td>\n",
       "      <td>-0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Florencia</td>\n",
       "      <td>Carrasco</td>\n",
       "      <td>29</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>59.7</td>\n",
       "      <td>55.1</td>\n",
       "      <td>-4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Holly</td>\n",
       "      <td>Carrasco</td>\n",
       "      <td>50</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>51.7</td>\n",
       "      <td>51.2</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Justin</td>\n",
       "      <td>Carrasco</td>\n",
       "      <td>52</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>47.0</td>\n",
       "      <td>47.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_name last_name  age Happy_Sad_group  Dosage Drug  Mem_Score_Before  \\\n",
       "0    Bastian  Carrasco   25               H       1    A              63.5   \n",
       "1       Evan  Carrasco   52               S       1    A              41.6   \n",
       "2  Florencia  Carrasco   29               H       1    A              59.7   \n",
       "3      Holly  Carrasco   50               S       1    A              51.7   \n",
       "4     Justin  Carrasco   52               H       1    A              47.0   \n",
       "\n",
       "   Mem_Score_After  Diff  \n",
       "0             61.2  -2.3  \n",
       "1             40.7  -0.9  \n",
       "2             55.1  -4.6  \n",
       "3             51.2  -0.5  \n",
       "4             47.1   0.1  "
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the first 5 rows of our original data\n",
    "org_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Converting expressions and drugs into a dummy variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H</th>\n",
       "      <th>S</th>\n",
       "      <th>A</th>\n",
       "      <th>S</th>\n",
       "      <th>T</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>age</th>\n",
       "      <th>Happy_Sad_group</th>\n",
       "      <th>Dosage</th>\n",
       "      <th>Drug</th>\n",
       "      <th>Mem_Score_Before</th>\n",
       "      <th>Mem_Score_After</th>\n",
       "      <th>Diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Bastian</td>\n",
       "      <td>Carrasco</td>\n",
       "      <td>25</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>63.5</td>\n",
       "      <td>61.2</td>\n",
       "      <td>-2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Evan</td>\n",
       "      <td>Carrasco</td>\n",
       "      <td>52</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>41.6</td>\n",
       "      <td>40.7</td>\n",
       "      <td>-0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Florencia</td>\n",
       "      <td>Carrasco</td>\n",
       "      <td>29</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>59.7</td>\n",
       "      <td>55.1</td>\n",
       "      <td>-4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Holly</td>\n",
       "      <td>Carrasco</td>\n",
       "      <td>50</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>51.7</td>\n",
       "      <td>51.2</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Justin</td>\n",
       "      <td>Carrasco</td>\n",
       "      <td>52</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>47.0</td>\n",
       "      <td>47.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   H  S  A  S  T first_name last_name  age Happy_Sad_group  Dosage Drug  \\\n",
       "0  1  0  1  0  0    Bastian  Carrasco   25               H       1    A   \n",
       "1  0  1  1  0  0       Evan  Carrasco   52               S       1    A   \n",
       "2  1  0  1  0  0  Florencia  Carrasco   29               H       1    A   \n",
       "3  0  1  1  0  0      Holly  Carrasco   50               S       1    A   \n",
       "4  1  0  1  0  0     Justin  Carrasco   52               H       1    A   \n",
       "\n",
       "   Mem_Score_Before  Mem_Score_After  Diff  \n",
       "0              63.5             61.2  -2.3  \n",
       "1              41.6             40.7  -0.9  \n",
       "2              59.7             55.1  -4.6  \n",
       "3              51.7             51.2  -0.5  \n",
       "4              47.0             47.1   0.1  "
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting happy_sad_gropy and Drug to a dummy variable\n",
    "# This is really important step as we don't want the srting or variable 1, 2 or 3 to affect our prediction\n",
    "dummy_Hap_Sad=pd.get_dummies(org_data['Happy_Sad_group'])\n",
    "dummy_Drug=pd.get_dummies(org_data['Drug'])\n",
    "\n",
    "# concating the dummy variable to our original data\n",
    "new_org_data=pd.concat([dummy_Hap_Sad,dummy_Drug,org_data], axis=1)\n",
    "new_org_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing first and last name as it doesn't play any role in data prediction\n",
    "# Also removing Drug and happy_sad gorup as they have been converted to a dummy variable\n",
    "# we are also removing the drug and happy sad grop \n",
    "features=new_org_data.drop(['first_name','last_name','Mem_Score_After','Diff','Drug','Happy_Sad_group'], axis=1)\n",
    "target=new_org_data['Mem_Score_After']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H</th>\n",
       "      <th>S</th>\n",
       "      <th>A</th>\n",
       "      <th>S</th>\n",
       "      <th>T</th>\n",
       "      <th>age</th>\n",
       "      <th>Dosage</th>\n",
       "      <th>Mem_Score_Before</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>63.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>41.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>59.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>51.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   H  S  A  S  T  age  Dosage  Mem_Score_Before\n",
       "0  1  0  1  0  0   25       1              63.5\n",
       "1  0  1  1  0  0   52       1              41.6\n",
       "2  1  0  1  0  0   29       1              59.7\n",
       "3  0  1  1  0  0   50       1              51.7\n",
       "4  1  0  1  0  0   52       1              47.0"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    61.2\n",
       "1    40.7\n",
       "2    55.1\n",
       "3    51.2\n",
       "4    47.1\n",
       "Name: Mem_Score_After, dtype: float64"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the coulumn we are going to predict so its named target\n",
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H</th>\n",
       "      <th>S</th>\n",
       "      <th>A</th>\n",
       "      <th>S</th>\n",
       "      <th>T</th>\n",
       "      <th>age</th>\n",
       "      <th>Dosage</th>\n",
       "      <th>Mem_Score_Before</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>198.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>198.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.338384</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.328283</td>\n",
       "      <td>39.530303</td>\n",
       "      <td>1.989899</td>\n",
       "      <td>57.967677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.501267</td>\n",
       "      <td>0.501267</td>\n",
       "      <td>0.474359</td>\n",
       "      <td>0.472599</td>\n",
       "      <td>0.470779</td>\n",
       "      <td>12.023099</td>\n",
       "      <td>0.818504</td>\n",
       "      <td>15.766007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>27.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>46.525000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>54.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>68.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>110.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                H           S           A           S           T         age  \\\n",
       "count  198.000000  198.000000  198.000000  198.000000  198.000000  198.000000   \n",
       "mean     0.500000    0.500000    0.338384    0.333333    0.328283   39.530303   \n",
       "std      0.501267    0.501267    0.474359    0.472599    0.470779   12.023099   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000   24.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.000000   30.000000   \n",
       "50%      0.500000    0.500000    0.000000    0.000000    0.000000   37.000000   \n",
       "75%      1.000000    1.000000    1.000000    1.000000    1.000000   48.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000   83.000000   \n",
       "\n",
       "           Dosage  Mem_Score_Before  \n",
       "count  198.000000        198.000000  \n",
       "mean     1.989899         57.967677  \n",
       "std      0.818504         15.766007  \n",
       "min      1.000000         27.200000  \n",
       "25%      1.000000         46.525000  \n",
       "50%      2.000000         54.800000  \n",
       "75%      3.000000         68.400000  \n",
       "max      3.000000        110.000000  "
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the description of our feature data\n",
    "features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Visualizing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Visualizing data using Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H</th>\n",
       "      <th>S</th>\n",
       "      <th>A</th>\n",
       "      <th>S</th>\n",
       "      <th>T</th>\n",
       "      <th>age</th>\n",
       "      <th>Dosage</th>\n",
       "      <th>Mem_Score_Before</th>\n",
       "      <th>Mem_Score_After</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>63.5</td>\n",
       "      <td>61.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>41.6</td>\n",
       "      <td>40.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>59.7</td>\n",
       "      <td>55.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>51.7</td>\n",
       "      <td>51.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>47.0</td>\n",
       "      <td>47.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   H  S  A  S  T  age  Dosage  Mem_Score_Before  Mem_Score_After\n",
       "0  1  0  1  0  0   25       1              63.5             61.2\n",
       "1  0  1  1  0  0   52       1              41.6             40.7\n",
       "2  1  0  1  0  0   29       1              59.7             55.1\n",
       "3  0  1  1  0  0   50       1              51.7             51.2\n",
       "4  1  0  1  0  0   52       1              47.0             47.1"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the correlation between all our features \n",
    "# we concat both features and mean score after to find correlatin between all our data\n",
    "\n",
    "filtered_all_data=pd.concat([features,target], axis=1)\n",
    "filtered_all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwsAAAMhCAYAAACnkS3qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdeXxTVfrH8W/SNGloS2mhFMpOaVllh1ERHBUUEVR0pMjPXVFQQBAQVBTECiiiKCqCCigiFoRRAR1nGBVG0XHYVPYd2SnQQpvuTX5/VKOhtxFq20Tv5/165TXcc8/NeU4LTp4859xr8Xg8HgEAAADAWayBDgAAAABAcCJZAAAAAGCIZAEAAACAIZIFAAAAAIZIFgAAAAAYIlkAAAAAYIhkAQAAAAhy3333nW699dYS7Z999pluvPFGJScna9GiRZKk3NxcDR06VAMGDNDAgQN16tSpMo9LsgAAAAAEsddff13jxo1TXl6eT3tBQYEmT56sOXPmaP78+UpNTVVaWpoWLlyopKQkvfvuu7r++uv16quvlnlskgUAAAAgiNWvX18zZswo0b57927Vr19fUVFRstvt6tChg9auXat169apa9eukqRu3brp66+/LvPYtjJfCQAAAPzBFZzYE+gQtPTf/1Nqaqr3ODk5WcnJyd7jq666SgcPHixxXVZWliIjI73H4eHhysrK8mkPDw9XZmZmmWMjWQAAAAAC6Ozk4FxFRETI5XJ5j10ulyIjI33aXS6XqlatWubYWIYEAAAA/AElJCRo//79ysjIUH5+vtauXat27dqpffv2WrVqlSRp9erV6tChQ5nHoLIAAAAA83IXBTqC87Zs2TJlZ2crOTlZY8eO1d133y2Px6Mbb7xRcXFxuvnmmzVmzBjdfPPNCg0N1bRp08o8lsXj8XjKMXYAAADgD6Pg+M5Ah6DQmomBDqFULEMCAAAAYIhlSAAAADAvjzvQEQQ1KgsAAAAADFFZAAAAgHm5qSz4Q2UBAAAAgCGSBQAAAACGWIYEAAAA0/KwwdkvKgsAAAAADJEsAAAAADDEMiQAAACYF3dD8ovKAgAAAABDVBYAAABgXmxw9ovKAgAAAABDJAsAAAAADLEMCQAAAOblLgp0BEGNygIAAAAAQ1QWAAAAYF5scPaLygIAAAAAQyQLAAAAAAyxDAkAAADmxROc/aKyAAAAAMAQlQUAAACYlocNzn5RWQAAAABgiGQBAAAAgCGWIQEAAMC82ODsF5UFAAAAAIZIFkzi8ssv1+LFi0u0r1mzRk2bNg1ARAAAAEHA4w78K4iRLAAAAAAwRLIAAAAAwBAbnAEAAGBe7qJARxDUqCwAAAAAMGTxeDyeQAeBinf55ZcrLS1NNptvMamoqEh5eXnavn17gCIDAAAInLxtqwIdghzNLg10CKViGZKJDBkyRD179vRp27Bhg8aMGROgiAAAAAIsyO9GFGgkCyYSExOjBg0a+LQdOnQoQNEAAAAg2JEsAAAAwLx4grNfbHAGAAAAYIhkAQAAAIAh7oYEAAAA08rb9K9AhyBHqx6BDqFUVBYAAAAAGGKDMwAAAMyLDc5+UVkAAAAAYIhkAQAAAIAhliEBAADAtDyeokCHENSoLAAAAAAwRGUBAAAA5uVhg7M/VBYAAAAAGCJZAAAAAGCIZUgAAAAwL56z4BeVBQAAAACGqCwAAADAvNjg7BeVBQAAAACGSBYAAAAAGGIZEgAAAMzLzROc/aGyAAAAAMAQlQUAAACYFxuc/aKyAAAAAMAQyQIAAAAAQyxDAgAAgHnxBGe/qCwAAAAAMESyAAAAAMAQy5AAAABgXtwNyS8qCwAAAAAMUVkAAACAebHB2S8qCwAAAAAMkSwAAAAAMMQyJAAAAJgXy5D8orIAAAAAwBCVBZwTm71OoEOoVIX5hwIdAgAAqAQeT1GgQwhqVBYAAAAAGCJZAAAAAGCIZUgAAAAwLzY4+0VlAQAAAIAhKgsAAAAwLw+VBX+oLAAAAAAwRGUBAAAACFJut1sTJkzQ9u3bZbfblZKSogYNGkiStm7dqkmTJnn7bty4Ua+88opat26tq666SklJSZKk7t276/bbby/T+CQLAAAAMK8g3+C8cuVK5efnKzU1VRs3btSUKVM0c+ZMSVLz5s01f/58SdInn3yimjVrqlu3blqzZo169+6txx9//HePzzIkAAAAIEitW7dOXbt2lSS1bdtWmzZtKtEnOztbM2bM0GOPPSZJ2rRpkzZv3qxbbrlFw4YN0/Hjx8s8PpUFAAAAmFcQbHBOTU1Vamqq9zg5OVnJycmSpKysLEVERHjPhYSEqLCwUDbbLx/j33//ffXs2VMxMTGSpMaNG6tVq1a6+OKL9dFHHyklJUUvvfRSmWIjWQAAAAAC6NfJwdkiIiLkcrm8x2632ydRkKRly5b5JAMXXnihnE6nJKlHjx5lThQkliEBAAAAQat9+/ZavXq1pOINzD9vWv5ZZmam8vPzVbt2bW/buHHj9Omnn0qSvv76a7Vs2bLM41NZAAAAgHkF+QbnHj166KuvvlL//v3l8Xg0adIkzZ07V/Xr19cVV1yhvXv3qk6dOj7XjBw5Uo8++qgWLlwop9OplJSUMo9v8Xg8nt87Cfz52ex1frvTn0hh/qFAhwAAACpBzj9fDXQIcl55f6BDKBWVBQAAAJhXEGxwDmbsWQAAAABgiGQBAAAAgCGWIQEAAMC8gnyDc6BRWQAAAABgiGQBAAAAgCGWIQEAAMC8WIbkF5UFAAAAAIaoLAAAAMC8eM6CX1QWAAAAABgiWQAAAABgiGVIAAAAMC82OPtFZQEAAACAISoLAAAAMC82OPtFZQEAAACAIZIFBL2EhIbKPL1LISEhgQ4FAADAVFiGhKBWt268PvzgLTmdzkCHAgAA/ozY4OwXlQWTKSws1KuvvqoePXqoVatW6tq1qx5//HGdPHky0KGVcO21V+nbbz5RXl5+oEMBAAAwJSoLJjNt2jStXr1aEyZMUMOGDXXkyBFNnTpVAwcO1JIlS2SxWAIdoteVPf6qx594Rrt27dW/V74f6HAAAMCfERuc/aKyYDJLly7V0KFD1aVLF9WpU0cdO3bUc889p82bN+u7774LdHg+hgx9RG/OeTfQYQAAAJgWyYIJffPNNyoqKvIe16tXTytWrFCzZs0CGBUAAACCDcmCydx2221auHChLrvsMo0bN04rVqzQmTNn1KRJE4WFhQUsrrFjhirj1A7v65IunQMWCwAAMBG3O/CvIMaeBZN54IEH1KhRI7377rtaunSpFi9eLIfDoWHDhumee+4JWFyzZs/X4veXeY8PHToasFgAAABQjGTBhHr16qVevXrpzJkzWrNmjVJTUzV16lQ1bNhQ3bt3D0hM6ekZSk/PCMjYAADAxIL8m/1AYxmSiWzbtk0pKSne46pVq6pnz56aM2eOWrVqpTVr1gQwOgAAAAQbkgUTKSoq0vz587Vx40afdovFosjISMXExAQoMgAAAAQjliGZSMuWLXXZZZdpyJAhGjlypDp27KiMjAytXLlSW7du1ZQpUwIdoqFVq7+WzV4n0GEAAIA/I48n0BEENYvHw0/ITHJzczV79mx9/PHHOnz4sOx2uzp16qSRI0eqSZMmpV5ntg/rhfmHAh0CAACoBDmpTwY6BDmTxwc6hFKRLOCckCwAAIA/I5IF/1iGBAAAAPPibkh+scEZAAAAgCEqCwAAADAvKgt+UVkAAAAAYIhkAQAAAIAhliEBAADAvDwsQ/KHygIAAAAAQ1QWAAAAYF5scPaLygIAAAAAQyQLAAAAAAyxDAkAAADm5fEEOoKgRmUBAAAAgCEqCwAAADAvNjj7RWUBAAAAgCGSBQAAAACGWIYEAAAA82IZkl9UFgAAAAAYorIAAAAA8/JQWfCHygIAAAAAQyQLAAAAAAyxDAkAAACm5XHzBGd/qCwAAAAAMERlAQAAAObFrVP9orIAAAAAwBDJAgAAAABDLEMCAACAefGcBb+oLAAAAAAwRLIAAAAAwBDLkAAAAGBePGfBL5IFnJOvavwl0CFUmtCQIq2vd12gw6hU7Q98GOgQAABAECJZAAAAgHnxnAW/2LMAAAAAwBDJAgAAAABDLEMCAACAebEMyS8qCwAAAAAMUVkAAACAeXm4dao/VBYAAAAAGCJZAAAAAGCIZUgAAAAwLzY4+0VlAQAAAIAhKgsAAAAwLzcbnP2hsgAAAADAEMkCAAAAAEMsQwIAAIB5edjg7A/JAgAAABCk3G63JkyYoO3bt8tutyslJUUNGjTwnk9JSdH69esVHh4uSXr11VdVUFCgUaNGKTc3VzVr1tTkyZPldDrLND7JAgAAAMwryDc4r1y5Uvn5+UpNTdXGjRs1ZcoUzZw503t+8+bNeuONNxQTE+NtS0lJUe/evXXDDTdo9uzZSk1N1R133FGm8dmzAAAAAASpdevWqWvXrpKktm3batOmTd5zbrdb+/fv1xNPPKH+/fvr/fffL3FNt27dtGbNmjKPT2UBAAAACKDU1FSlpqZ6j5OTk5WcnCxJysrKUkREhPdcSEiICgsLZbPZlJ2drVtuuUV33nmnioqKdNttt6lVq1bKyspSZGSkJCk8PFyZmZlljo1kAQAAAKblCYInOP86OThbRESEXC6X99jtdstmK/4I73Q6ddttt3n3I1x44YXatm2b95qwsDC5XC5VrVq1zLGxDAkAAAAIUu3bt9fq1aslSRs3blRSUpL33L59+zRgwAAVFRWpoKBA69evV8uWLdW+fXutWrVKkrR69Wp16NChzONTWQAAAIB5BfkG5x49euirr75S//795fF4NGnSJM2dO1f169fXFVdcoT59+qhfv34KDQ3Vddddp8TERA0ePFhjxozRokWLFB0drWnTppV5fIvH4wnunxCCwn/jbwh0CJUmNKQo0CFUuvYHPgx0CAAABITr6dsCHYLCH3s70CGUimVIAAAAAAyxDAkAAADmxROc/aKyAAAAAMAQyQIAAAAAQyxDAgAAgHkF+d2QAo3KAgAAAABDVBYAAABgXkHwBOdgRmUBlcJeJ1bNFj6hjrveVetVLynqsnZ++8f0uVhtvnpFHXcvVNLcsbJVjzLs12zhE4od0N3wnMURqtarXlLkRS1/d/znymK3qf6U+9X6hwW6YN08xQ3qW2pfZ/OGSvrgGbXdsUjNVkxTlTZNDPvVGnaTGr44otT3afD8MNUe0f93xw4AAHA2kgWTWr58uZo2bao5c+ZUynhJ88aqID1Tm69+WGmLP1fiGw/LUa+mYd/wNk2UMH2oDr34vjb3HquQCKcSXhrm28liUYOUexR1aVvD97CE2ZX42kg5E+uW91T8qvPYnQrv2Ew7BzyhHx95VbWG9VP0tV1L9LM6HUqYP16uDTu0rddDyvp2ixLmPS5ruNOnX/R1XVV7xM2ljhf3wI2qftMV5T4PAAAAiWTBtJYvX64GDRro73//e4WPVbVLKzkbx2vv6JnK2XlQR17+u7LWblPszcYfcuPu6qVTH3+jE4s+V87W/dr94Euqdlk7ORrWkiSF1opR80UTFH1lJxVmZJW43tm0vloumyJ7XeNkpKJYnQ7VGNBDBye8qZwfduv0P7/VsdeWKvb2XiX6Rl97iTwFhTo0cY5ydx3UwSffVFFmtqL7XFLcIcSqepMGqcFzQ5W3/2iJ60Oqhqvx7LGKu+965R8+UdFTAwDgz8vtCfwriJEsmNDp06f15ZdfasiQIdqxY4e2bNlSoeNFtG8q1+a9crtyvW2Z325TRIempfRP0plvfokp//BJ5R04rsif+oe3aqTc/ce0qecoFWVml7g+8sIWOv35em25/tFynol/zhaNZLGHKut/v8Se9e1WVWmTKIX4/lMLb9dUrrVbJc8v/4Fwrd2q8J/mGBLuVFhCXW3vM1qu9dtKjGVvUEset0dbe45Q/sHjFTQjAABgdmxwNqFPP/1UdrtdvXr10iuvvKKlS5eqRYsWFTZeaFy0Co6m+7QVpGXIHl/dsL89LloFx06V7F+7uH/GynXKWLmu1PGOv/WP4j+EVG4uHFozWkUZWfLkFXjbCk9kyOoIla16lAqPp/v0zd1zyOf6grQMVWnZWJJUdMalncnjSh0r54fd2jvomXKeAQAAJsQTnP2ismBCy5YtU7du3WSz2XTFFVdo+fLlKigo+O0Ly8jqtMud7/v+nvwCWe2hfvoX+rS58wtkcRj3DxZWp6PEPH8+PnuuFqdDnrPm6MkvlMVO/g4AAIIHyYLJHDt2TGvXrlX37sV3ELryyiuVnp6uVatWldsY8UNvVMedC7wvR92aJT8s20PlzskzvN6dVyDrWR+arX76Bwt3Xn6Jef58fHbsnryCEomBxW4L+jkCAABz4WtMk/n4448VEhKiSy+9VJLUpk0b1axZUx988IE3gfi9js3/VCeXfeU9rn5tF1W7vL1Pn9Ca1ZR/LP3sSyVJ+UdOKbRm9Fn9o1VQSv9gUXD0lEKiwmUJtclTUFw1sMVGy52br8KMzLP6nlRo7FlzjI1WwfHgniMAAH86Qb7BONCoLJjMz0uO/vKXv6hFixZq2bKl0tLS9MUXX+jUqVO//QbnoCgjS3n7jnpfWeu2q0rLhrI6Hd4+kZ2aK2v9DsPrs9bvUGSnZt5je3x1OerGKrOU/sEie/MeeQoKFd7hl9gjOjdX9g+7pSLf9ZCuDdt9+klSeMdmcq3fXimxAgAAnAuSBRPZt2+fNm3apEceeUQffPCB9zVr1iwVFBRoxYoVFTLuma+3KO9gmhpPHypnUj3VfqCvIjok6fiCf0mSLKE2hcZWk6zFfx2Pv/0PVe/bVbEDusvZrL4aTx+mjH+vU97eIxUSX3nx5Obr5OLPVe/pQarSNlFRPTor7t7rlTZ3uSTJFltNljC7JCl9xRpZw8NU96l7FZZYT3XG362QiCpK/+jLQE4BAADT8bjdAX8FM5IFE1m+fLmqVq2qm2++WUlJSd7XpZdeqnbt2lXcMxfcbu24c4pCa0Sp1T+mqsbfLtXOu59R/sE0SVJEx6Zq/90c792Rstbt0N7RM1Vn+E1quWyyijKztfvBGRUTWzk7OPFNZX+3U4nvPaV6kwfryIupSl9WnAC0Xv+W9zkK7qwc7b7jKUV0aKZmHz+viI7NtOv2iXK7cgIZPgAAgA+Lx+NhoZZJXH311brwwgs1fvz4Euc+/PBDPfzww1q2bJmSkpJKnP9v/A2VEWJQCA0pCnQIla79gQ8DHQIAAAGR9ciNgQ5BEZOXBDqEUrHB2UQ++eSTUs9dd911uu666yoxGgAAgCDABme/WIYEAAAAwBCVBQAAAJgXlQW/qCwAAAAAMESyAAAAAMAQy5AAAABgXp7gfs5BoFFZAAAAAGCIZAEAAACAIZYhAQAAwLy4G5JfVBYAAAAAGKKyAAAAANPyUFnwi8oCAAAAAEMkCwAAAAAMsQwJAAAA5sUyJL+oLAAAAAAwRGUBAAAA5uXmCc7+UFkAAAAAYIhkAQAAAIAhliEBAADAvNjg7BeVBQAAAACGqCwAAADAvKgs+EVlAQAAAIAhkgUAAAAAhliGBAAAANPyeFiG5A+VBQAAAACGqCwAAADAvNjg7BeVBQAAAACGSBYAAAAAGGIZEgAAAMyLZUh+UVkAAAAAYIjKAs5JiNUd6BAqjdtjCXQIleoVW4jU8MZAh1Fp5u5bEugQAABBxENlwS8qCwAAAAAMkSwAAAAAMMQyJAAAAJgXy5D8orIAAAAAwBDJAgAAAABDLEMCAACAeZnnho9lQmUBAAAAgCEqCwAAADAtnrPgH5UFAAAAAIZIFgAAAAAYYhkSAAAAzItlSH5RWQAAAABgiMoCAAAAzItbp/pFZQEAAACAIZIFAAAAAIZYhgQAAADT4jkL/lFZAAAAAGCIygIAAADMiw3OflFZAAAAAGCIZAEAAACAIZYhAQAAwLTY4OwflQUAAAAAhqgsAAAAwLzY4OwXlQUAAAAAhqgsAAAAAEHK7XZrwoQJ2r59u+x2u1JSUtSgQQPv+Xnz5mnFihWSpEsvvVRDhgyRx+NRt27d1LBhQ0lS27ZtNXLkyDKNT7IAAAAA0/IE+TKklStXKj8/X6mpqdq4caOmTJmimTNnSpIOHDigjz76SIsXL5bFYtGAAQPUvXt3OZ1OtWzZUq+99trvHp9lSAAAAECQWrdunbp27SqpuEKwadMm77latWrpjTfeUEhIiKxWqwoLC+VwOLR582YdO3ZMt956qwYOHKg9e/aUeXwqCwAAADCvIKgspKamKjU11XucnJys5ORkSVJWVpYiIiK850JCQlRYWCibzabQ0FDFxMTI4/Ho2WefVYsWLdSoUSOdOHFC9957r66++mqtXbtWo0eP1pIlS8oUG8kCAAAAEEC/Tg7OFhERIZfL5T12u92y2X75CJ+Xl6dHH31U4eHhGj9+vCSpVatWCgkJkSR17NhRx44dk8fjkcViOe/YWIaEcudoWEvtd6VKIf7/etW86xq1+s+rard9oZp9OEXhHZpW6PgRnVuo+cfT1G7He2rx6Quq2q3t7xrHYrep5coXFXlJ61L7RHZprebLp6rd9oVqteoV1ejf/XeN+Vtjh9auoSZzH1PbLQt0wdezFTfw2nIZ73xUrxOrkW8/rte2LNDTK1/UBX9tV2pfi8WiWdve1dx9S3xezqpVSvQd+fbj6pZ8RUWGDgBA0Gnfvr1Wr14tSdq4caOSkpK85zwej+6//341bdpUEydO9CYIL7/8st566y1J0rZt2xQfH1+mREGisoByFlq7hprMGydrmMNvv5gbLlX8qAHaP/plZW/eqxr9uyvpnfHadNkQFRw9Ve7j26pHqcncx3T01SVKX75G0X0uUcKbj2jzX4co/1DaeY9jcYSq8csPydmsQal9HI1qK/Gtx3R4+mKlD5mm8HZJajh1iApOnNbplf877zHPZeyE10Yp//BJbb1mtJyJddXo5YeUf/iE0lesKfN452vY62N0eNchTbx2jNr26KQHZo7WuB7DdeLg8RJ9Y+vHyWYP1agug1SQX+BtzzmT7f2zxWLRgPF3qVW3tvpfJc4DAGAOwb7BuUePHvrqq6/Uv39/eTweTZo0SXPnzlX9+vXldrv17bffKj8/X//5z38kSQ899JDuvfdejR49WqtWrVJISIgmT55c5vFJFkymsLBQs2fP1t///ncdOXJE0dHR+utf/6rhw4erevXqv+u9q131FzV4ZrAKjqf/Zt8aN12utLc/UfqKryVJh6a8o+heF6la905Ke+fTch8/olMzSdLRV5YW/+/L76vWfdcqvH3SeScLYYl11fjlkdJvJOgxfS5R9ua9Ovry+5KkvH1HFXlhK1Xv263MyYK/sUOiwhXRoZk2PzJCeXsPK2/vYZ35YoMiu1xQaclC84taqVbjeE2+aZxyXbk6vOugWnRprW7JV2jptIUl+scn1tWpwyd0spTfQbW4GN07/UHF1ouT63RWRYcPAEDQsVqtmjhxok9bQkKC988//PCD4XWzZ88un/HL5V3whzFt2jStWLFCEyZM0KeffqoXXnhBO3bs0MCBA+XxeH7Xe1e9tK0OPbtAB8a/+Zt9D01doLQF//Rt9EgWh11S8TKbehPuVpvv3lLbH95W45mjZasRVebxC9MzZYsKV/Q1F0sqTiys4U7lbN1/jrP7RUTnFjqzeqO2XTvGb79Ty77Sj+Ne9230eLxzlKQa/3elLvjqNbXbvlBNl05SlTZNyjy2OzdfRdm5qtHvcllsIXI0jldEx2bK/mH3uU/ud2rcLkk/bt6rXFeut23n2q1KaJ9k2D8+sZ6O7jlc6vs1aNlIafuP6sk+o5WTmV1qPwAAUDGoLJjM0qVL9eSTT6pLly6SpDp16ui5555T9+7d9d1336lt27Kv4//x0VmSpMiLWv1mX9f6HT7HVf/aTmEJdZT1383FcY25VeEdmmrXHSkqys5T/EP9lThvnLb2Hl2m8bP+u0XH5q5Q45mjJLdHFluI9o16Wbm7Dp7z/H524uwkpxR5e30/BNtqRCn62kt05IXiux1Ede+kOqMGaP+YV5Wz84BiendR09SntKnb/aVWZ/yN7ckr0I+PzlL9lHtV845esthCdOL9z3Vi4cpznNnvV61mtNKP+cZ+5sRpRdcyrlrFN6mrsPAwjU19SnGNauvHzXu18Km53gTiu8/W6bvP1lV43AAAEwvyZUiBRmXBhL755hsVFRV5j+vVq6cVK1aoWbNmAYnH0ShejaY/qBOLP1P2pj2yhtlV845e2j92plwbdip3+4/a++B0hTWtr4jOLco0hrVKmBz1aurIS4u15ZpROvj0W6r35N0KL+Ub7/JmdTrU5PWxKjh2Smnz/yFJqjW4r46+skQZ//xWeXuP6MiM9+X6fpdq3NyjzOOEJdTR6c/Xa+t1Y7R70FRFXdZeNe/uU17T+E12p0OFv9p7IEmFeQUKdYQa9o9vUldVoiL04fRUvTRwigry8vXwwifljCy5wRkAAFQ+Kgsmc9ttt+mll17SZ599pm7duumiiy5S165d1aSJ/+UvZ6s15G+qPfRG7/HOW59S1rdbzjuesKb1lbRggnJ3HdL+scVPI7Q3qCWrI1TNlk7y6Wt1hCqscbwiOrc477FrDb5eVnuoDj9XvG4+Z/NehSXVU+1hN2nXHU+fd9znIySyiprMGyd7/Thtv+FRuXPzJRXvP6jT5hbFjx7g7Wuxh6rgyElFdG6hxPmPe9uPzFji3ftQmsiLWyn2tqv1fce75M7JU/Z3u2St4lC98Xfr+NwVkrv8vzq55v4b1PuBG7zHezbulOu0y6ePzRGqvJw8w+sn9Rsnq9Wq/J/Ov/bgdD2/Zpba9eikNUtXlXu8AACcLdg3OAcayYLJPPDAA2rUqJHeffddLV26VIsXL5bD4dCwYcN0zz33nPP7pL3zD6Uv/9J7nF+GOxhVaZ2gpAXjlbPtR+28PUWevOJvpC0/3fZr+9/GqSjT94Nn4ckzUoj1vMcOb5OonO0/+rRl/7Bbsbf0PO+4z4ctOlKJ705QaGw1bb9pnPL2H/Wes4SE6MDEeTqzeoPPNUWuXBVlZmvLVSO8bYUZv725N7xNorbV6vUAACAASURBVPIPHJP7Vx/Ms3/YLVtUuGzRkSo8ebocZuTriwX/9LlDUefeXUrcKjUqtppOl7KsqjCvZBUi7cBxRdeKKfdYAQDA+SNZMKFevXqpV69eOnPmjNasWaPU1FRNnTpVDRs2VPfu5/YcgKKMLBWdwwfY0tjrxynxnfHK/mGPdt01yfttuyTl7T8iT2GRbDGR3s25IZFV1OjF4To0dYFytu4/77Hzj52Ss3lDn7awJnV9PryXN0uoTU3eGidbTFVtv/GxEmPl7jkke3x15e37pb3+pPuU+c1mpX/0pU/7ucg/dkqOxvGyOEK9iVdYk7oqysyukERBklyns3zuUrR7/XZdc39f2Z0Ob7UgsWMz7d6ws8S11hCrnvvqNb339Fv6dtlXkiRHlTDFNaqtI7sPVUi8AADg/LBnwUS2bdumlJQU73HVqlXVs2dPzZkzR61atdKaNRV4e02rVbbYarKEFuenDZ6+V+7sXO0bO1PWyCqyxVaTLbaarFXC5HblKu3df6p+yr2KvPgChSXUUcMXHpSzeUPl7j1SpuFPLPinIi9sqVqD+8peP041+ndXjX5X6NjrH5XnLGWLqSprlTBJUtzAPqpyQYL2PTRDRdm53jmGVCt+ZPux2R+q5l29Vf1vl8nRoJZqj0hWjeTuyt1Vtg/KGf/8VkVnXGo4bagcjeMVeUlr1X3sdh17c1m5ze+3bPvvFp08dEL3PDdE8Yn11GvQ9Upol6RV7/1LkhQSalPV2GqyWK1yF7m1afVG3ThqgJI6t1CdpHq678UHlXE8XRtXrq20mAEA5uZxB/4VzKgsmEhRUZHmz5+v3r17+9z1yGKxKDIyUjExFbf0wx5fQ62/ma3tN42T6/tdirqsgySp9ZpZPv0Ov7hIh6e+qwMT56reuNvVeOYoWR12Za3dph0DJsjzqwrE+XBt3Kldd01S/KgBqj28n/IOHNeeYS8oc43xvYnLqvmK53Ry8Wc6/Px7ir6mi6yhNjVN9b03cua3W7T9hkeVvuwrhdaopvgRyQqNi1Hu7kPadc9k5WzZW6ax3Vk52t7vcdV/8m41XzZVRWdcOpH6bx15cXF5TO2ceNxuvTRwiu589n5NWP6sju8/phn3PauTB4ufo9CkQ1ONfW+iRl0ySCcPpumd8W/qpjG3aPDLD8kZ4dSWr77X87enyF0U5P/lBADAJCye33tzffyhDBo0SJs2bdLIkSPVsWNHZWRkaOXKlXrvvff04YcfqlatWobXra17fSVHisryii0k0CFUqrn7lgQ6BABAEDl22aWBDkFxnwfvTT2oLJjM9OnTNXv2bM2aNUvjx4+X3W5Xp06dtGDBglITBQAAAJgTyYLJhIWFadiwYRo2bFigQwEAAECQI1kAAACAeXksgY4gqHE3JAAAAACGqCwAAADAtIL91qWBRmUBAAAAgCGSBQAAAACGWIYEAAAA0/K42eDsD5UFAAAAAIaoLAAAAMC02ODsH5UFAAAAAIZIFgAAAAAYYhkSAAAATMvDE5z9orIAAAAAwBDJAgAAAABDLEMCAACAaXE3JP+oLAAAAAAwRGUBAAAApsUTnP2jsgAAAADAEMkCAAAAAEMsQwIAAIBpeTyBjiC4UVkAAAAAYIjKAgAAAEyLDc7+UVkAAAAAYIhkAQAAAIAhliEBAADAtFiG5B+VBQAAAACGqCwAAADAtLh1qn9UFgAAAAAYIlkAAAAAYIhlSAAAADAtNjj7R7KAc2K1mGdBX0FRSKBDqFQWmes/knc1/FugQ6hUc/a9H+gQAAB/YCQLAAAAMC2Px1xfmp0v9iwAAAAAMESyAAAAAMAQy5AAAABgWh53oCMIblQWAAAAABiisgAAAADTcrPB2S8qCwAAAAAMkSwAAAAAMMQyJAAAAJgWz1nwj8oCAAAAAEMkCwAAAAAMsQwJAAAApuVxswzJHyoLAAAAAAxRWQAAAIBpeTyBjiC4UVkAAAAAYIhkAQAAAIAhliEBAADAtNjg7B+VBQAAAACGqCwAAADAtNw8wdkvKgsAAAAADJEsAAAAADDEMiQAAACYlodlSH5RWQAAAABgiMoCAAAATIsnOPtHZQEAAACAIZIFAAAAAIZYhgQAAADT4jkL/lFZAAAAAGCIygIAAABMi1un+kdlARXKYrep/pT71fqHBbpg3TzFDepbal9n84ZK+uAZtd2xSM1WTFOVNk0M+9UadpMavjii1Pdp8Pww1R7R/3fHXlb2OrFqtvAJddz1rlqveklRl7Xz2z+mz8Vq89Ur6rh7oZLmjpWtepRhv2YLn1DsgO6G5yyOULVe9ZIiL2r5u+Mvb9Xr1NBDbz+umVveUcrK6brgr6X/PCwWi17btkBz9r3v83JWrVKJEZ+785nbryV2aq439ixS9bqx3raI6EjdN2OEXto4V89+OVM97u5dUWEDAHDOSBZMZOzYsWratGmpr6VLl5b7mHUeu1PhHZtp54An9OMjr6rWsH6KvrZriX5Wp0MJ88fLtWGHtvV6SFnfblHCvMdlDXf69Iu+rqtqj7i51PHiHrhR1W+6otzncT6S5o1VQXqmNl/9sNIWf67ENx6Wo15Nw77hbZooYfpQHXrxfW3uPVYhEU4lvDTMt5PFogYp9yjq0raG72EJsyvxtZFyJtYt76mUi6Gvj5ErI1NPXTtWa5Z8oftnjlKNusY/j9j6cbLZQzW6yyAN73SP95VzJruSoz435zO3n9kcobpzyiBZrb7/+R0y62HFNayl529N0ZzRL+vKu65R9zt7VWT4AAD8JpYhmchjjz2mkSNHSpLWrl2r4cOH68svv/Sej4yMLNfxrE6Hagzood13Pq2cH3Yr54fdOvbaUsXe3kvpH/3Hp2/0tZfIU1CoQxPnSB6PDj75pqpe0VHRfS7Ryff+JYVYVe+pe1X9psuVt/9oibFCqoarwXNDFXFhS+UfPlGu8zgfVbu0krNxvLZc/5jcrlzl7DyoqK6tFXvzFTr47MIS/ePu6qVTH3+jE4s+lyTtfvAltVv7uhwNaylv31GF1opRkxkPytGglgozskpc72xaXwkvD6/weZVVs4taqVbjeE256XHlunJ1eNdBtejSWl2TL9ffp71Xon98Yl2dOnxCJw8F7nd4rs53bj+7fkSyzpw8o1oJdbxtDVo1VlLn5nr08mE6uuewJGnxlHfUf9ztWjn34wqfCwCYGc9Z8I/KgolERkYqNjZWsbGxiooqXury83FsbKzCwsLKdTxni0ay2EOV9b8t3rasb7eqSptEKcT3r154u6Zyrd3q8y/WtXarwjs0lSSFhDsVllBX2/uMlmv9thJj2RvUksft0daeI5R/8Hi5zuN8RLRvKtfmvXK7cr1tmd9uU8RP8yjZP0lnvvnl55N/+KTyDhxX5E/9w1s1Uu7+Y9rUc5SKMkt+ux55YQud/ny9tlz/aDnPpHwktEvSj5v3KfdXP4+da7cpob3xzyM+sa73w3KwO9+5ScVJwcV9u2nRpLd92mPrxykrI9Nn7ge27FO1uBifpUoAAPNxu9164oknlJycrFtvvVX79+/3Ob9o0SLdcMMN6tevnz7/vPjLx1OnTumuu+7SgAEDNHz4cOXk5JR5fCoLqDChNaNVlJElT16Bt63wRIasjlDZqkep8Hi6T9/cPYd8ri9Iy1CVlo0lSUVnXNqZPK7UsXJ+2K29g54p5xmcv9C4aBUcTfdpK0jLkD2+umF/e1y0Co6dKtm/dnH/jJXrlLFyXanjHX/rH8V/CAnOvD+qZjVlnDW/0ycyFFMrxrB/fJN6Cgt3akzqRMU1qq0fN+/Ve0/NC8oE4nznFmIL0V1T71fq028pKz3T59yZExlyRlRRWHiYN/n4OUmIjK6qkwfTKmAGAAAp+G+dunLlSuXn5ys1NVUbN27UlClTNHPmTElSWlqa5s+fryVLligvL08DBgxQly5d9Oqrr6p379664YYbNHv2bKWmpuqOO+4o0/jB+QkDfwpWp0Pu/AKftp+PrfZQn3aL0yFPfqFPmye/UBb7HyuftTrtJebsyS8oMV/f/r7zducXyOIw7v9H43A6VHDW/ArzCmUrZX61m9RRlahwfTh9kWYMfEYFeQV6eOEEOSODb4Pz+c6t1/19derISf33o69KnNuzcadOHTmhW1PuVVh4mKrFxei64f0kSSGhf6x/AwCA8rVu3Tp17Vq837Nt27batGmT99z333+vdu3ayW63KzIyUvXr19e2bdt8runWrZvWrFlT5vH5fyFUGHdefokPyT8fu3PyfNo9eQUlEgOL3VaiX7CJH3qj4ofd4D3OWr9TRaddPn0s9tBS5+HOK5D1rHlb/fQPdtfcf4OueeCXO17t2bhL2ad991rYHDbllzK/yf0el9Vq9Z6f9eB0TVvzmtr16KQ1S1dVXODn4PfMLT6xrrrf0UtP9n7Y8L0L8wv1yqDnNGjGCL38/VvKycrR+1PeUUK7JOVmBefmbgBA5cjKylJERIT3OCQkRIWFhbLZbMrKyvLZcxoeHq6srCyf9vDwcGVmZpZ433NFsoAKU3D0lEKiwmUJtclTUPwNrC02Wu7cfBVmZJ7V96RCY6N92kJjo1Vw3HdJT7A5Nv9TnVz2yzfF1a/tomqXt/fpE1qzmvKPGc8j/8gphdY8a941o1VQSv9g98WCf+p/K3759qJz74tL3E40KraaMo5nGF5fmFdQ4jjtwHFVK2VpT2X6PXPr2OsiVakarpR/vSCp+BaxkpTyzxf01iOz9M2H/9GPm/fq0cuHqWqNKLlOuxTXoJbcRUU6GcAN+wBgBsHwnIXU1FSlpqZ6j5OTk5WcnCxJioiIkMv1yxeRbrdbNpvN8JzL5VJkZKS3PSwsTC6XS1WrVi1zbCxDQoXJ3rxHnoJChXdo5m2L6Nxc2T/slorcPn1dG7b79JOk8I7N5Fq/vVJiLauijCzl7TvqfWWt264qLRvK6nR4+0R2aq6s9TsMr89av0ORnX6Ztz2+uhx1Y5VZSv9g5zqdpeP7j3pfu9bvUL0WDWX/1c8jsWNz7dlQcn7WEKumfTNLnft08bY5qoQprlFtHd19qET/yvZ75vbveZ/osSse1IReozWh12i9ePcUSdILd07SxpX/U5Wq4Rq76ClFVq+qMydOq6igUG17dNT+TXuVm1X2TWkAgD+G5ORkLV261Pv6OVGQpPbt22v16tWSpI0bNyopKcl7rnXr1lq3bp3y8vKUmZmp3bt3KykpSe3bt9eqVcUV+dWrV6tDhw5ljo1kARXGk5uvk4s/V72nB6lK20RF9eisuHuvV9rc5ZIkW2w1WcLskqT0FWtkDQ9T3afuVVhiPdUZf7dCIqoo/aMv/Q0RdM58vUV5B9PUePpQOZPqqfYDfRXRIUnHF/xLkmQJtSk0tpr00z32j7/9D1Xv21WxA7rL2ay+Gk8fpox/r1Pe3iOBnEa52f7fLTp56ITufm6I4hPr6upB1yuhXaJWvbdSUvF6/Kqx1WSxWuUucmvT6u90w6ibldS5ueok1dO9Lz6o08fTtXHl2gDPpKTzmdvZicapn6oFJw+lKdeVq+wzLtmddiU/drti68epY6+L1Gfo37RsxvuBnCIAIAj06NFDdrtd/fv31+TJk/XII49o7ty5+ve//63Y2FjdeuutGjBggG6//XaNGDFCDodDgwcP1ooVK9S/f39t2LBBt9xyS5nHZxkSKtTBiW+q/qTBSnzvKRVl5ejIi6lKX1acALRe/5b2PfSiTi3+TO6sHO2+4ynVn3y/avTvoZxt+7Tr9olyu/5g36q63dpx5xQ1nvaAWv1jqnL3H9XOu59R/k93s4no2FQtljylDZ3vU/7BNGWt26G9o2eq7uibZYuO0OnV32vv6JkBnkT58bjdmjHwGd357GCNX/6sju8/ppfvm+q9u0+TDk015r0nNfqSwTp5ME0Lxr+pv435Pw16+SE5I5za8tUPev72p+U+qxIVDM53br/ltaEv6PZJ92niP6Yp/chJvfXIrKBMkgDgzybY74ZktVo1ceJEn7aEhATvn/v166d+/fr5nK9Ro4befPPNchnf4vHwKAozWrNmje68805t335uy3zW17uugiMKHgVFIYEOoVLNslNg/DObs4/qBAD489/4G367UwX7y+GlgQ6hVFQWTOriiy8+50QBAADgz4pvzf3jK0UAAAAAhkgWAAAAABhiGRIAAABMK9g3OAcalQUAAAAAhqgsAAAAwLSC4QnOwYzKAgAAAABDJAsAAAAADLEMCQAAAKblDnQAQY7KAgAAAABDVBYAAABgWh6xwdkfKgsAAAAADJEsAAAAADDEMiQAAACYltsT6AiCG5UFAAAAAIaoLAAAAMC03Gxw9ovKAgAAAABDJAsAAAAADLEMCQAAAKbFcxb8o7IAAAAAwBCVBQAAAJiWO9ABBDkqCwAAAAAMkSwAAAAAMMQyJAAAAJgWG5z9o7IAAAAAwBDJAgAAAABDLEMCAACAaXE3JP+oLAAAAAAwRGUBAAAApkVlwT8qCwAAAAAMUVnAOckpCA10CJUmMiwv0CFUquGBDqAS7c+NCHQIlappdLp2NO8Z6DAqTdLWfwQ6BAD40yFZAAAAgGnxnAX/WIYEAAAAwBCVBQAAAJiWm8KCX1QWAAAAABgiWQAAAABgiGVIAAAAMC03G5z9orIAAAAAwBCVBQAAAJiWJ9ABBDkqCwAAAAAMkSwAAAAAMMQyJAAAAJiWO9ABBDkqCwAAAAAMUVkAAACAabkt3DrVHyoLAAAAAAyRLAAAAAAwxDIkAAAAmBbPWfCPygIAAAAAQyQLAAAAAAyxDAkAAACmxXMW/KOyAAAAAMAQlQUAAACYlpvHLPhFZQEAAACAIZIFAAAAAIZYhgQAAADTcot1SP5QWQAAAABgiMoCAAAATIsnOPtHZQEAAACAIZIFAAAAAIZYhgQAAADT4jkL/pEsoFI46tZQwnODVbVzU+UdPKG9E95SxmcbSu1f/dqL1OCRAbLHxShj9ffaPWqmCk6ckSTZ46LVeNI9iuraSu7cfB1ftEr7J78rFRU/sD0kwqlGT9+l6ld3ljs3X8fe+bd+fPa9SpmnxW5T/IT7FHXNJfLkF+jEGx8obdZSw75hzRuqztMPyNm8kXJ3HdChx15Vzvc7i0+GWFV77B2qdv1fZXU6lPnFeh2eMEuFJzK818fef5Nq3HaNrFXClLlqnQ499qqKzrgqY5qSKneuP6s5pJ9i+l+pbZfcU5FTO2fOujXU5rmBiu6cpJyDJ7Vlwnwd/+y737wucXhfRTSJ14Yhr3jbQqo41Hzczap1dSeFhNl1cs0WbXpsnnKPplfkFM6JJTRUseMGK/KqrvLkFyj9raVKf/N9w772po0UN2GYHE0bKX/PAR2bMEN5m3Z4z1e79TpVu+U6hURHKfvLdTqe8oqKTp2urKkAAM4Ty5BQKZrNG6PC9Ex913Osji/6Qs3eGCVH/ZqGfSPaJijxxSE6OH2Jvr/mUdkinUp8aaj3fNPZD8nisOn7ax7V9nufV2zfS1R3yPXe84kzhiq8WX1t6jteOx98RbXuvEpxA66o8DlKUu1H7lJ4h+ba+3/jdOjRV1RzSLKi+nQt0c/idKjRvAnK3rBdO3sPl+t/m9VozhOyhjslSXHDB6hqj79o/+Ap2tV3tEKiI1XvhYe819e461rF3ttXB8a8pN3JY+VoVEfxT95XKXP8WWXN9WeOhLqqOTS5wud1PjrNG6n89Cz9p+c4HVy0Wh3fGCFn/Vi/18Rff7GSRt1Yor3lU7ep+sUttG7gdK3p+6RCnHZ1mjdSsgT+K68ao++Rs10LHbzrER2b8JJiBt2syF6XluhncTpUd3aKcr/bqh//NlQ56zapzmsTZalS/LuOuqmnaoy4U6defVcH+g+Xx+1WndkpQTFHAOblDoJXMCNZQIWL6tJKzoR47R71mnJ2HNShlz9Q5trtirv5csP+te/upZMr/qvjqV8oe+t+7Rg6Q9GXt1NYw1oKCQ9T3uGT2j3mdeXsPKQz32zViWVfq+pFLSVJzqS6irmyg3bc/6Jcm/cp4/ONOjxruSLaJ1b4PC1Oh2JuvlKHJ76unE27deZf/1XarCWqcVvvEn2r9e4qT0GRjqS8qbzdB3Vk4hsqysxWtd7FH7YtIVYdfvJ1Za/dorydP+rk3I8U3rnVTwNZFDvoRh2dMk9Zq9Yrd+s+HZk0R2HNG1Xah65Km6t3QIvqPjtMOd/trIzpnZPqXVoqIiFe3416XVk7DmnXyx/p1Nodqn/zZYb9LSFWXfDMXWr7wn3K3nfM95wtRHX7dtGWCe8ofe1OZW47qI0jZqta2wRFJNSujOmUyuJ0KOqmnkqbPEt5W3bJ9dk3Sn/zfVUbcG2JvpFXXypPYaHSnnld+XsOKG3yLLmzXN7Eotot1ylj/oc68+HK4qrDuBcUWqeWqlzcvrKnBQA4RyQLqHCRHZLk2rRPRa5cb9uZb7cpsmNT4/7tE3Xmmy3e4/zDJ5V74LgiOyapyJWrHYOnK//QCUmSs2ldxVzVUae//EGSFHVJK2VvO6Cc3Ye91x+cvkS7R71WEVPz4WzeSBZ7qFxrf4ndtXaLnK0TpRDff2pV2jWVa91WyfPLDdtc67aqSvvin8nRZ99W5udrJUm2GtUUk3ylsr7+XpLkSKwnW41qOv3JGu+1WWu+186eQ33eryJV1lx/VuPOPnLn5Cl9yWcVNaXzFt2hiU6f9ff61LfbFd3RODENCQ9TRJN4/afX40pfd1bSY5H+d9fzOvXt9l/afvp5WR2h5R77+XA0bSyLPVQ56zZ523LWbZbjgpK/67A2zZSzfovP7zpn/RY52zaXJIXWra2cjVu95zy5ecrff8h7HgAQfEgWUOFC46op/9gpn7aCtAw5ascY9rfHRSv/WPpZ/U/LXru6T9sFH6Wo/arpKjqTrSNzPpEkORvUUu6Px1V74DVq//UMtf96hur8aolSRbLVjFFRRpY8eQXetsK0DFkdobLFRPn0Da0Zo8KzfiaFJ9IVWruGT1vcqFvVYu18VenUQkeeekOS5GhQW+6sbDkvaKImHz2v5t/MU91nhsoa4aygmZVUWXOVJHu9ONV8oJ8OPfqKgklYXLRyz/p7mpd2Ws5S/l4XnsnW1zemKHPrjyXOeQqKlPbF9yrKzvO2NRrYU/knM5W5/WD5Bn6ebLExcp/OlCf/l9910cl0We12hcRUK9G38PhJn7aik+myxRX/rgtPpstW61e/d4tFobVqKCS6asVNAAB+gycIXsGMZOEPaMOGDRowYIDatGmjtm3b6u6779axY8XLGr788kv16dNHrVu31j333KOnnnpKY8eO9V67cuVKXXPNNWrTpo369u2r1atXV3i8IU6HPHmFPm3u/EJZSvnG1Oq0y/2rD6GS5M4rKPEN655H3tCmGyfIGmZX05nDi8eKCFPUxS1VrVtr7Rg8Xfsnvas691+n2gOvKccZGbM6HT4fqCR5j8+eq8XpkPvsvnmFsth9+6W/v1I7+4yQ6+sf1Gj+RFkjnLKGO2Wxh6r2Y3fp6NT5+nHYVDlbJaje8yXX+VeUypqrJNWZMlRps5Yo/8ej5T2N3yXE6O9pfmG5VAJqX9NZTe7voy1PLZCnsOh3v9/vYXGGlf67DvWdqzXM+O/Fz7/rzI9XKeaefrI3bSSF2lT9gf8rTjhCudcGAAQrkoU/mKysLN133326+OKLtXz5cr355ps6ePCgZs6cqQMHDmjw4MG66qqr9MEHH+iCCy7QggULvNdu27ZNo0eP1sCBA7Vs2TL169dPQ4YM0datW/+fvfsOb6rqAzj+TZo0TQd00BY6oUBLmdIyFEUFQRQEFETAxVKGMnxVpogIAoIK4gAEVLQqFhAHryAq8jIERIZs6KCFtnTTPZK0yftHICVNqCgtRfv7+NznMfeee3N+OffSe+4Zt4pv/OsCJg7g1vgoy6IJ9EGhsb4ZUDqqMJbo7O5vr2Kg1Kht0hedSCTv1+PE/mcZnr06ogn0xlRmRKFWETN2CYV/xJO9aS/J735No2H3VmuM9ph0epsb4MufK+fdpNOjrJxWY/ub6BNTKTkWx/n/LEbp7ET9+7pgKi9H6aQhde5qCncdpmj/CZKnvUf9e29F5e1RA5HZulGxeg65F4d6LmSu/rYGovhrmk3sz/3xH1sWbaC37XnqqKL8Kuf1tfLrfxsRyycQv+J7kr7ccV3Hqg5VlbWptNRqvfGKisGVaS+nu7hiLSUHjxP81Xs0P/gN6mB/inb+jrGwuAYjEEKIqhkVtb/czORxzj9MSUkJY8aMYeTIkSgUCgIDA7n33ns5fPgw69evp1WrVowfPx6ASZMmsXfvXsu+H374IQMHDuTBB83dcoKCgjh69ChRUVHMnz+/2vKY9umPZH1X0Z++Qf8ueHRvb5VG7eOOPt12akwAfepFHL3d7aTPQeXuivtdbcn6tuL4JTHmbhpqz3ro0y+iT7to1Y+8JC4FR3/rLi81wZCWjUN9FxRqFSaDuSVF5eOBUaenPLfAJq3Ku3IXDg/KMi6CQkG9Hp0oPnzGMn2oqVSHITkDB4966I+Yp6Esja/onqI7mwKA2t+bssyan2rzRsVar3sHnEKDaXXMPPWtQuWAQq2i1Yl1xPR8FsOFzBqP9bJzn/7Mhe/2WT77978Nn+7trNJofNwpvcp5fS2CHu1G2zee4uzKzZya+8XfPk51KkvPQlnP1fz0/1JZOzS4VNZ5BTZpVQ2su2E5NPCwnJOmUh1pUxaRPvsdFCoVxvxCgqKXUrzn0I0JRgghxF8mLQv/MN7e3jz00EOsWbOGKVOmMGDAAD766CPKy8s5c+YMrVtbzyLTrl3FzUx8fDxffvkl7du3tyybNm0iMTGxWvNYlltIaWKaZSk4EINL68YonTWWNPU6hVNwMMbu/gWHYnHr3MLy2dHPC6cAbwoOxqBydyXsg+dxadPEst21aiKPbQAAIABJREFUXQimsnJK4i+QfyAGTUADVB6ulu3OYYHokmr+prLkZAImQxnOkRV5d+nQkpJjcZZ3QFxWfPgMzpHWgzpdIsIpPnwGTCb8Xh2De/+KqSmVbs44BjdCF5dEyYmzGHV6tK2bWrZrmgdiMhoxpGTUUHTWblSs5597i5h7nyW29yRie08ifelaDOkXie09CUO6dd/4mmbILaI4Md2yXDwQS73WjXG44rz27BRGbuXBy9eo4f0daPvmU8S9+y0nX/38z3e4QXSnz2IylKG9paVlnTayNboTsTZlXXrkNE7trcta274lpUfMrZcNXhhJvYG9MBWXYswvROXbAE14U4r3Ww9oF0IIcfOQysI/THp6Ov369WPPnj20atWKGTNmMGLECAAcHBxs0puumJWkvLycUaNG8c0331iW77//nrfeeqtG85y39yS65CyaLx2PNiwA//EP4hbZnPTPfwZAoVah9nYHpfl0TFuzFe+H7sD3sR44twii+TvjubjtEKUJ5spHzi+HafrGGFxaNabebS1p+sZYUj/cQnlhCXm7j1N86jyh709CGxaAR89I/J/pT9onW2s0RjA/Nc356hf8545D26459Xp0wvvph8haswkAlbc7Co2j+TfZ8isOzk74vToGTbNAGr38FEpXLbmbdgGQ/en3+Dz7CK53RaAJDSLo7RfRJV6g4H8HMRaVkP35FvxeGY1zx5ZoWzUl4LVnyN+6j7LMv/9U+2aMtSz9IvpzqZalPDsfysvRn0u1uVG90bL3nqQkOYtblo7FNSyApuP74hHZjHOfm2dsUqgd0HjXB+Wfty87OGto++bTpP90mIQPt6Lxrm9ZFGrb6/pGMpXqyP/mZ3xmPYtTm1Bcut2Kx4iB5Hxm7hrm0MDDUtaFW3ejdNbiM/MZHJsG4T1tNEoXZ/K/N3enKkvPxuuZx3C6JRzH0CY0WjqTwl/2oo87V2vxCSFEbb9j4WZ/z4J0Q/qH+emnn3BxcWHVqlWWdVFRUZhMJpo3b85vv/1mlf7EiRMEBgYC0KRJE5KSkggODrZsf+edd3B3d+fJJ5+suUwbjZwatpBmS8Zxy9ZFlJ5L5/SINyxP+906htFm46sc6DgOXVImBQdjiHtxBUFThqD2cCV3x1HiJn9gOVzMM0tpMncErda/AiYjGet3cm7e55bvOvnEApoueJp2W16nvKCECys2kfrhlpqL7woX5n6I/7xxhHwxD2NhMenvfEnef3cD0PL3KJJefJucDdswFpaQMHIOAfOfxXPIvZSeSiRxxKsYi0oAyFz5NQq1ioAF41G5u1Gw6zCJo+ZapqRMnf8xlBlp/MFLKNQq8n7cy4VXPrhqvv7Jsd60jCb2D3uTW5aM4c6t8yg+l8GBEYspSTJP6+vZMZQuG2fxc8cJlnVX43V7SzRe9WjYK5KGvSKttu0dNI+sXcevsueNkblwJT6vjCfg44UYi4q5uOxzCreYJ0doumstadPfIv+bnzAWFZMydha+r04k6OH70J9JIGXMy5iKzWWd+8UmVH4++L3/CgqlkoKtu8lceGPPWyGEEH+NwmS62f8iiyv997//5eWXX+a9994jKCiILVu2sGTJEsLDw3n33Xfp1asX48ePp1evXvz4448sXryYAQMGsGDBAv744w8effRRpkyZQrdu3dizZw9z585l2bJl3H333VV+768NH74xAd4E3Jyub4CquHmdK3X980T/ImEeNT9+5WYSeuqH2s6CEOIf6IOAx2s7C4xJ/qy2s3BV0g3pH+b++++nX79+PPfccwwYMIB9+/Yxffp0EhIS8PLy4p133uHrr7+mb9++HDp0iB49eqC+NL3hLbfcwptvvsm6devo06cPa9asYf78+X9aURBCCCGEEHWTtCz8i8TExFBWVkbLlhUDEUePHk2bNm2YMGHCdR1bWhbEv4G0LPy7ScuCEOLv+Ke2LJSWljJ58mSys7NxcXFh4cKFeHpaz0i3cOFCDh06RFlZGYMHD+aRRx4hNzeXXr16ERoaCkCPHj0YNmzYVb9HWhb+Rc6fP8/w4cP59ddfSUlJYf369ezdu5eePXvWdtaEEEIIIW5KJkXtL3/H2rVrCQ0N5YsvvuDBBx9k2bJlVtv37dvH+fPniY6OZu3ataxatYq8vDxOnjzJAw88QFRUFFFRUVVWFEAGOP+r9OjRg9jYWF566SWys7Np0qQJS5YsoUWLFn++sxBCCCGE+Mc4ePAgTz31FAB33nmnTWWhffv2hIdXTGddXl6OSqXi+PHjnDhxgscffxxPT09mzpyJj4/PVb9HKgv/MuPGjWPcuHG1nQ0hhBBCCHGNoqOjiY6OtnwePHgwgwcPtnxev349n3zyidU+Xl5euLm5AeDi4kJBgfWLMjUaDRqNBoPBwLRp0xg8eDAuLi6EhITQunVrunTpwnfffcdrr73GO++8c9W8SWVBCCGEEELUWTfDew4qVw4qGzRoEIMGDbJaN378eIqKigAoKiqiXr16Nvvl5eUxceJEOnXqxJgxYwC49dZb0Wq1APTs2bPKigLImAUhhBBCCCH+cSIiItixw/zSy507dxIZaf2entLSUoYPH87AgQN59tlnLetnzpzJ1q3ml9Xu3buXVq1aVfk9MhuSuCYyG5L4N5DZkP7dZDYkIcTf8V5g7c+GND7pr8+GVFJSwtSpU8nMzEStVvPWW2/h7e3NokWLuO+++zh06BDvvfee1biF+fPnAzBjxgwAtFotr732WpVjFqSyIK6JVBbEv4FUFv7dpLIghPg7/qmVhRtFuiEJIYQQQggh7JIBzkIIIYQQos6SLjZVk5YFIYQQQgghhF3SsiCEEEIIIeos4998g3JdIS0LQgghhBBCCLuksiCEEEIIIYSwS7ohCSGEEEKIOutmeIPzzUxaFoQQQgghhBB2ScuCEEIIIYSos6RloWrSsiCEEEIIIYSwSyoLQgghhBBCCLukG5IQQgghhKiz5A3OVZOWBSGEEEIIIYRd0rIghBBCCCHqLHmDc9WkZUEIIYQQQghhl1QWhBBCCCGEEHZJNyQhhBBCCFFnyXsWqiYtC0IIIYQQQgi7pGVBCCGEEELUWTJ1atWkZUEIIYQQQghhl1QWhBBCCCGEEHZJNyQhhBBCCFFnGaUjUpWksiCuiVZtqO0s3DAlenVtZ+GGUirqzj+S/uri2s7CDZWXr63tLNwwGnUZRxv3re1s3FBtEzfVdhaEEHWAdEMSQgghhBBC2CUtC0IIIYQQos6S9yxUTVoWhBBCCCGEEHZJy4IQQgghhKiz6s7Ivb9HWhaEEEIIIYQQdkllQQghhBBCCGGXdEMSQgghhBB1lgxwrpq0LAghhBBCCCHskpYFIYQQQghRZxkVtZ2Dm5u0LAghhBBCCCHsksqCEEIIIYQQwi7phiSEEEIIIeoso7xpoUrSsiCEEEIIIYSwS1oWhBBCCCFEnSXtClWTlgUhhBBCCCGEXVJZEEIIIYQQQtgl3ZCEEEIIIUSdJW9wrpq0LAghhBBCCCHskpYFIYQQQghRZ8nUqVWTlgUhhBBCCCGEXVJZEEIIIYQQQtgl3ZCEEEIIIUSdJZ2QqiYtC0IIIYQQQgi7pLIghBBCCCGEsEu6IQkhhBBCiDpL3rNQNWlZEEIIIYQQQtglLQtCCCGEEKLOkvcsVE1aFoQQQgghhBB2SWVBCCGEEEIIYZd0QxI1SuGoInDOaNz73I5JbyBj1bekr/jablpteGMCF4zDuWUTSmOTOD9jOcVH4mzSNZw4CKemASROWmJZp3R2wm/6k7jfdytKJ0cK9x4nadZKDGkXayy2q3H09ybkzXG4dmyBPiWLc7M/Jm/74aum9+zbhcBpj6Fu6En+ziOcfXE5Zdl5NularJ1F9qY9ZH7xs2WdJtiX4DkjcesYjrFYR/Z3v5K08HNMOkONxAbm+Bq/8YwlvqRXPyZv+6Grpvfo24WAqY9b4kucvMwqPv+pj+H9aE8UKgeyvvyZpHlRYLQdbhb6xSsY0i+S8J93Let8RvbBd2Rv1D4elJw+T9Kcjyk8cKZa4qzOc9f9gdvxm/oEjr6e5O/6g/NT3rf8BgqNGv8Zw/HoewcoFeT9sI/kOR9hLC7Fc1B3Gi+eZPc7j3UeheFCVrXEetnNVLZtdr2PU4ifVbrj9/6HkhOJ1x9oJQpHFX6zx1C/zx2Y9AayVn9D5gcb7aZ1Cm+M/7xn0YY3oTQuiZSXllFyNNa80UFJo2nDcX/wbpRaDQX/O8SF2R9QlpULgHNEC5ptfMPqeCUnzxLb234ZCyFuDOmEVDVpWahm3bt3JywsjLCwMFq0aEH79u0ZMmQIu3btqu2s1Qr/l0bg0qEFsY/O4vz0ZTSc+Age/brapFNqNTSNeoWiwzGc7v08hftP0nTNyyhdtFbpPPp3pdF/htrsHzD7Kdxua03C2EXEPDwDhZMjIatngEJRY7FdTeiaaRhyCjhx/xQy12+n+eopaAJ97KZ1adeMpm9PIGXpBk48MA0HVy1N35lonUihIPi1p6h/1y3Wq9Uqwj6ZgUlXxol+04kbvwSP+zoROPWxmgoNgOYfT6csp4CTvSeTtX47TVdNwbGK+EKWTOTCO+s51XcqDm5aQpZWxOc7uh8NBnUjfswbxD31Op79u9Jw3IM2x2kw+B6b+L0G3IX/5KEkL/iME/c+T8Ge44R+Ngt1Q89qibO6zl3nds1ovGQSae+s40y/yTi4OhP89nOW/Rs9NwS329sQP2Iu8cPm4NKxJf4zhgGQs2k3RyOGWZZjHUZQfDSOnO/3VHtFAW6eslU4qtAE+XKq/zQO3zLCspScPl+9AV/SaPpIXCLDSXhsJikz3sdn/GDq97Uta4VWQ5M1syk+fIbYB56j6PcTNPlolqWsfZ97lHo9O3Nu3OvEPTQZBw83Apc8b9nfqXkQxUdiOdnxCcty9tGZNRKTEEJUF6ks1IBp06axe/duduzYQXR0NBEREYwZM4Y9e/bUdtZuKKVWQ4NHe5I8+0NKjsWT9+N+0ldsxHtYb5u0Hv3uwGQoI2XOR5TGJZP86oeUFxSbn7YCOCgJnD+W4DcnoDuXZr2zygHPB+8kee7HFB08TemZ85yf/B4u7ZqjqfRksqbVu7012hA/EiYvpyQ2mdT3vqbwwGm8h95jN73vyN5c3LyPrHXbKTl1jvhJ7+DerT2axg0BUDf0JHzdbDzu7UhZbqHVvi7tm6Fp3JD4596lNC6Fgn0nSX5jLV4DbG9yqovb7W1wCvEjccoySmOTSXt/46X4ethN7zOyNzmb95F9Kb6ESUup3y3CEp/vUw+Q8taXFOw7QcHeEyTPj8J3+P1Wx1D7eBAw7TEKD8darfd6pBuZn/xAzvd70SWmkfz6Zxgyc3Hv2fG646zOc9d7+APkbt7LxfW/UHL6HInPvU39uyt+g3rdI8la+xPFR+IoPhJHVtQW3Lq0AcBUqqcsM9eyuN93K2p/b85Pfe+6Y6zsZipbp6b+mEwmiv6Is4qf8uqf4FCh1eA59F4uzFlFyfF48n/6jcwPvqLBkw/YpHV/oCsmQzmpr32ILj6Z1DmrKS8oxv0B8zWncFBy4dVVFB84iS72PNkff4dLp9aW/TXNAymNPW8VU3luQbXHJIT4a4w3wXIzk8pCDXB1dcXb2xtfX19CQ0OZMmUKffr0YcGCBbWdtRtK27IJCkc1hb+ftKwr3H8K53bNwcH61HNpH0bRgVNgqmgMLDpwCpfIMAAcXLQ4NQ3gTN/JFB06bbWvQqEg/ukFFP1+yrLOdOk4So1jtcdVFdeIMIpOJGAsKrWsK9h/GtdLcdimDyV/X8Xvo7+QjS4pA7dL6V1aN6H0XDrH73uR8oJiq31L4y5w5ol5GIsrvguTCaWjuhojss1vcaX4CvefqiK+MAp+O2H5fDk+18gw1L4eaPy9KfitIv6C/Sdx9GuAo5+XZV3wgjGkf7IF3dkLVsdOWfQFGZ//aLXOZDKh1Fx//NV57rpEhFr9BobULHRJ6bhEtACgPKcAj9634eDuikM9F9zvv43iY/E2eVK6aGn4nyGkvvUF5XlF1x1jZTdT2WqbB6I7n46prLy6wrsqbbi5rIsOVOS16MBJtG1ty9q5fRhFByuV9cFTOEeYf6O0RZ9SsP0AAKoG7ngOvpfCvUctaZ2aB6I7m1yT4QghRLWTysINMnjwYGJiYjh37hx5eXm8/PLLdOnShYiICF544QVyc3MtaZcuXUrXrl1p06YNgwcP5vDhiv7u27dv56GHHqJNmzZERkby3HPPUVhY8cT5u+++o0ePHrRr144XXniB559/nnffregHHB0dzT333EP79u0ZOnQoR49W/CGrbmofD8pzC636z5dl5aLUqFF51bdJa0i3Hl9gyMzFsVEDAMrzi4gdPJOS0+dsvsdkKKNgx2Grm2afkX0pu5hPSUzNdFu4GrWvB4a0HKt1hsxcqxukKzn6Xi1uc/rcnw+S8OIyyi7aPn0su5hP/q4ryk+hwHdEbwr2n7JJW13UPh7oK+c3Kxd1I/vxmdPb+T0aeaH28TB/vmJciSHL3N9dfancPfvdjia4IWnv2fYfLzoUgz4pw/K53t3t0Tb1p+CKytffVZ3nrtrH02Z7WVae5TdLfu1j1I0a0PZIFG2PfYbK043zMz+wyVODx3uZ+9Ov/em647PnZipbp9BAKDfSPOplbjn8EWEbXsOlffO/H1wVVD6etmWdeamsPSuXtSdlNmWZY4npMt8Xn6DlgSicO7Ykde5qy3pNsyCc2zYndOt7tPj1I/znP4vSzbkGohJCiOojlYUbpGnTpgDExcUxfvx4Tp06xYoVK1izZg0JCQlMmTIFgJ9++onPP/+cN998k82bN9OyZUsmTpyI0WgkKSmJCRMmMGTIELZs2cLSpUvZt28fa9euBeDAgQPMmDGDkSNHsnHjRrRaLZs3b7bk4ZdffmHp0qVMnz6dr7/+mjvvvJNhw4aRkZFhm+FqoNRqMOqtB9pe/lz56bdCq8GkL7NaZ9KXoXD862Pw3e+/Dd+xD5I8bw3cgCeTV1JqHW1iNukNV33ab05vHbdRb0DxN56OB80egXPrJiTN/+wv73utlFqNzeBpk66syvhs0usNKBzVKLUay+eKY10+P1Tm/t6zR5I4+f0/fcKsCfEjZOkkstZvp/j42b8cl22+q+/cVWodrWK8fKzLx9EENcSQmk3cY7OJHTwTk76MxktsB7w2eKwXmR9/X2Pn9M1UttrmATjUdyHj0y3EPPEapbFJhK2bg2OA93XFaD8OjU35XP5c+TpU2DkvTLoyFJV+o5wNPxPb9z8U7T1Gk6g5KF21KJ2dcPT3BqWSpBeWkDztXVwiwwla+mK1xySE+GtMN8F/NzOZDekGcXNzA+DUqVPs37+fzZs3WyoQb775Jvfffz+xsbGkpKSgUqnw8/MjMDCQF154gXvvvRej0Uh5eTkvvfQSgwcPBiAgIIAuXboQF2eedWXt2rX06tWLRx99FIDZs2eze/duSx5Wr17N6NGj6dHD3Ad53Lhx7Nmzh/Xr1/Pss89We8xGnd7mRuPyZ2OJzmq9SWewqRgoHFU26f6MR7+uBC+ZRMbKb7m4btvfyPVf4zdhIH4TB1g+Fx6KtekionBUXzUOo86AslLcyirSX03wnJH4DLuPuNFvUBKT9Jf2rUqjCQNpNGGg5XPR4VjK8qzHTig0Vy8no8624nP597h886hwVFtuGC+nNZboCZ4zipxNeyiq1J+9Mm1YEKFfvEJpXDKJU5f/tQCvojrPXaPOYHMzebmMla5agt+aSNwTsyk6YO5ed3bsQlrvWYVz22YUHzVf29o2TdEEN+Tixv9VS3xwc5ft2WcXo3ByxFhYAsC56R/g2qEFDR7uxoW31/2NaK/OpNPblI/iqmVte17Y+430iakAnP/PYsL3fUz9+7qQs2Ebx9sOMcd0aUaopBeW0Py/b6P288ZwIbNa4xJCiOoilYUb5HJXIX9/f1xcXCwVBYCQkBDq169PfHw8ffv2ZePGjfTs2ZM2bdrQvXt3Hn74YVQqFY0bN8bR0ZHly5cTGxtLbGwscXFx9OnTB4AzZ87w8MMPW46rUqlo3bpicF18fDyLFy9m6dKllnV6vZ6GDRvWSMyGtIs41HdBoVZhMpifvKq8PTCW6imrNKjPkJaN2tvDap3a2wNDhnU3h6p4DelJ0OvjyFj9HSnzP7n+AK5BetRWsjf9WpGHfrfj3j3CKo3ax92mu8Zl+tSLli4bFek9MFwlvQ2FgpDFz+L1UFfixr5Fztbf/1oAfyIjaisXr4jPs98d1O9WKb4qysmQdhG1t7t1eh93DBk56NOyLZ91iWmXjmVOa8i4iNeAuygv0dHg0uDwyzdwLu2acby7+cm7c9umhH3xCsWnzhE7bF61TRlbneeuve0qb/Nv4NQsAAdXLSVnKrrL6ZMyKMstwDHI11JZqN8tgqLDMTbdma7HzVy2prJyTJcqCpeVxqegblQ9M11Zx5FtW9Y+Hhh1epvBx4a0bFSVYlZ5e1CWcREUCur16ETx4TOWqVJNpToMyRk4eNQDwJhv/SChNM48fkHd0FMqC0LUopt9gHFtk25IN8iZM+a53/Pz8+1uLy8vx2g04uXlxcaNG1m1ahXt2rUjOjqahx56iPT0dE6fPk2fPn2IjY0lMjKSefPm0bt3xewsDg4OloG9l135uby8nKlTp/LNN99Yls2bN1u6QFW34hNnMRnKcIlsYVnn2incPHiz0qwmRYfPWKUDcOnQgqJD1zZnfv37biVo4TOkL9tIymtrrjvv16o8txBdYpplKTx4BudWjS3dMADcOoZTeCjG7v6Fh2Jw61gRt6OfF5oAbwqukr6yoFeG4/VgV2KfWkTOlt+uLxg7rim+TlXFdwa3TuGWz+b4fCg8FIMhPQddcgauV2x36xSOPjUb/YVsjt4+jhM9nuPEvc9z4t7nydt2kNwffyfmidcA0AT5Evr5LIqOxRP7xFzrgd7XqTrP3aJDMVYxqhs1QBPgQ9GhM5Y+/drwxpbtKm93HOq5WM365dI+jMJ9FYOJq8PNXLbh/11Ew2ceqvgyhQJteDClcSnV+hsAlJxMwGQow/mKMnTp0JKSY3E2ZV18+AzOkeFW61wiwik+fAZMJvxeHYN7/7ss25RuzjgGN0IXl4S2bXNaHY9G5VNR4dG2CsFUVo4uIbXa4xJCiOoilYUb5KuvvqJVq1Z07dqVoqIi4uMrZjuJi4ujsLCQJk2a8L///Y/o6Gi6du3KzJkz+eGHHygqKuLgwYN8++23REREsHjxYh577DHatm3LuXPnLBWCZs2acfz4cctxy8vLOXWqYrBrkyZNSEtLIzg42LJ89NFH7N+/v0ZiNpXqyV6/ncB5Y3G+pTn1e3bCd/SDZH78X8B8U6RwMs9WlPP9HpQuTgTMHY1T80D8XxmFg6szOd/truorAHOf46DXnyFv2wEyPv4vKm93y6JQ39jGs/y9J9ElZxLy9gS0oYE0evYhXCNDyfjcPChVoVaZn7AqzZdexqc/4PVQV7wf7YG2RRAhb08kd9vBa7p5cI0IpdHoviS/9SVFR+JRe7tblppSsPcE+uRMmrw9EafQQBo+8xAuEaFkXpqVSKFWmZ+8WuLbiueDXWnwaE+0LYJo8vYkq/gyPt1KwLTHcbu9DW63tiJg+uOkf2g+P668kdUlpmEsKsVYVII+xfwENmjeaIzFpZybugKlm7OlzJXOTtcdZ3Weu1lRW/DofydeQ3vi1CKYxksmkffLAXSJqRjSssn7+XcC5zyN8y2haMMb0+TdFyjcf5KSK2ZEcgoLorSGB+vfTGWb9/MBGo57kPrdI3Bq6kfw62NQubuR9WX1dy00lerI+eoX/OeOQ9uuOfV6dML76YfIWrMJuFTWl2ZVy9vyKw7OTvi9OgZNs0AavfwUSlctuZvM79HJ/vR7fJ59BNe7ItCEBhH09ovoEi9Q8L+DlJ5KwJB+kYBFE9E0D8KlUysCFk7g4rqfKM+x/xBJCCFuBtINqQYUFhaSmZmJyWQiJyeHDRs2sHnzZj766CNCQkLo1q0bU6dOZdasWQC8+uqrREZGEh4eTmpqKosWLcLLy4vWrVuzd+9e9Ho9LVq0ICkpiZiYGI4cOUL9+vX58ssvOXbsGH5+5ncJPP744zzxxBN07tyZjh078vnnn5OSkoLi0ovJRowYwYwZMwgJCSEyMpLvvvuOr776iiFDhtTYb5E850OC5o+j+ZdzKS8sIXVpNDmbzDdRbQ99QuLzS7m4/heMhSXED59L0IJnaDCkJyWnE4kbNgdjUcmffAO4dmmD2qs+7j074d6zk9W22KGzKNh9pEZis8toJGbE64S89Sytf3iD0nNpxI5aiD7ZfBPk2iGMll/N5XCnMeiTMyk8GEPC5OUETB6KysOVvJ1HSZh8bf3uPR+4DYCgGU8QNOMJq22/BT5cI3PSYzQSO3IBjd8cT6stb6I7l0bcqNet4mux4TWOdB6NPjmTooNnSJyyHP8Xh6DycDO/5XdKRXxpy79B7VWPZiunYDIayYr+hbQV3/5pNpQuTpbuXm33rrDadmHpelIWfXHdoVbXuVt06Aznp75PoxceReXhRsGuI5yf+r7lexImLibgpeE0XT0dVA7k/+8QybNXW+VF7e1u0/2p2t0kZQtw4Z0N4KAk+PWxqD3rUXg4hjODZ9lMH1xdLsz9EP954wj5Yh7GwmLS3/mSvP+ay7rl71Ekvfg2ORu2YSwsIWHkHALmP4vnkHspPZVI4ohXLWWdufJrFGoVAQvGo3J3o2DXYRJHzQWTCZOhjMThs/Gb9TTNvlqIqcxI7rf/I3XBxzUSkxDi2hlv8gHGtU1hqtxvRVyX7t27k5JS0VTu5eVFy5YtGTt2LB06dAAgNzeXuXPnsn37dhwcHLjnnnuYPn069eubp+n7+OOPiYqKIiMjg6CgICZOnMjvNiv/AAAgAElEQVR9991HcXEx06dPZ9euXTg6OtKxY0dCQ0P57rvv+Okn85Pr9evX8/7775OTk0OvXr1ISkri7rvvZsyYMQBERUWxZs0aMjIyCAkJ4fnnn+euu+7izxwK7F/dP9VNy1DuUNtZuKGUirrzT4CDsm71TC031p3GY4267M8T/cu0TdxU21kQ4l/hmcaP1HYWWJZYvZM3VCepLPyLHD16FFdXV0JCQizr+vTpw6hRoxgwYEAVe/45qSz8e0ll4d9LKgv/blJZEKJ6jLsJKgvLb+LKQt35S1IHHD58mNGjR3Po0CGSkpJYsWIFqampdO3atbazJoQQQggh/oFkzMK/yGOPPUZycjITJkygoKCA8PBwVq1ahbd39b/ISAghhBBC/PtJZeFfRKVS8dJLL/HSSy/VdlaEEEIIIf4RZIBz1aQbkhBCCCGEEMIuaVkQQgghhBB1Vt2a+uKvk5YFIYQQQgghhF1SWRBCCCGEEELYJd2QhBBCCCFEnWWSAc5VksqCEEIIIYQQ/zClpaVMnjyZ7OxsXFxcWLhwIZ6enlZpxo4dS25uLmq1Go1Gw+rVqzl37hzTpk1DoVDQvHlzXnnlFZTKq3c2km5IQgghhBBC/MOsXbuW0NBQvvjiCx588EGWLVtmk+b8+fOsXbuWqKgoVq9eDcCCBQt47rnn+OKLLzCZTGzbtq3K75HKghBCCCGEqLOMN8Hydxw8eJCuXbsCcOedd7J3716r7VlZWeTn5zN27FiGDh3K9u3bAThx4gSdOnWy7Ldnz54qv0e6IQkhhBBCCHETW79+PZ988onVOi8vL9zc3ABwcXGhoKDAarvBYGDkyJE8+eST5OXlMXToUNq2bYvJZEKhUFx1v8qksiCEEEIIIeqsm2GAc3R0NNHR0ZbPgwcPZvDgwZbPgwYNYtCgQVb7jB8/nqKiIgCKioqoV6+e1fYGDRowZMgQVCoVXl5ehIeHk5CQYDU+wd5+lUllQQghhBBCiFpUuXJwLSIiItixYwdt27Zl586dREZGWm3fs2cPn3/+OStXrqSoqIjY2FhCQkJo2bIlv/32G507d2bnzp3ceuutVX6PjFkQQgghhBDiH2bo0KHExsYydOhQoqOjGT9+PACLFi3i6NGj3HXXXQQHB/PII48watQonn/+eTw9PZk6dSrvvvsugwcPxmAw0KtXryq/R2EymWq/7UXc9A4F9q/tLNwwhnKH2s7CDaVU1J1/AhyUf3cY2T9TubHuPA/SqMtqOws3XNvETbWdBSH+FYY1HljbWeCTxK9qOwtXVXf+kgghhBBCCCH+EhmzIIQQQggh6iyjdLKpkrQsCCGEEEIIIeySyoIQQgghhBDCLumGJIQQQggh6izphFQ1aVkQQgghhBBC2CUtC0IIIYQQos4ySttClaRlQQghhBBCCGGXVBaEEEIIIYQQdkk3JCGEEEIIUWeZpBtSlaRlQQghhBBCCGGXtCwIIYQQQog6y1jbGbjJScuCEEIIIYQQwi5pWRDXxFDuUNtZuGHUDuW1nQVRQxSK2s7BjaWvQ9etq7OutrNwQ5WVOXCsSd/azsYN0yZhU21nQYg6SyoLQgghhBCizpL3LFRNuiEJIYQQQggh7JKWBSGEEEIIUWfJ1KlVk5YFIYQQQgghhF1SWRBCCCGEEELYJd2QhBBCCCFEnSXvWaiatCwIIYQQQggh7JLKghBCCCGEEMIu6YYkhBBCCCHqLJNJZkOqirQsCCGEEEIIIeySlgUhhBBCCFFnyRucqyYtC0IIIYQQQgi7pLIghBBCCCGEsEu6IQkhhBBCiDpL3rNQNWlZEEIIIYQQQtglLQtCCCGEEKLOMskA5ypJy4IQQgghhBDCLqksCCGEEEIIIeySbkhCCCGEEKLOkvcsVE1aFoQQQgghhBB2ScuCEEIIIYSos0wmaVmoirQsCCGEEEIIIeySyoIQQgghhBDCLumGJIQQQggh6ix5g3PVpGVBCCGEEEIIYZe0LAghhBBCiDpL3uBcNWlZEEIIIYQQQtgllQUhhBBCCCGEXdINSQghhBBC1FnyBueqSWVB1AhHf29C3hyHa8cW6FOyODf7Y/K2H75qes++XQic9hjqhp7k7zzC2ReXU5adZ9keMPVRfB7riULlQOaX2zj/WhQYjfi/MJiAFwbbHM9kNLI/4GEA6t3emqBZw3EKaUTxiUTOvfIxRUfiqiVOhaOKwDmjce9zOya9gYxV35K+4mu7abXhjQlcMA7nlk0ojU3i/IzlFF+RD/cHbsdv6hM4+nqSv+sPzk95n7LsPFxvbU3o+nl2jxnz8HQKfzuJY4APgfPH4tohHENGDqlvR5PzzY5qifFqqjP2yxpOHIRT0wASJy2xe5zgxRPRJ2WQuuTLao3lWigcVQRcjldnIGP1t2RUFe/8cWhbNqE0Lomk6cspPnopXgclDScNwXPg3ajc3Sg+EkvSrJXo4pIBUPl6EjhnNG63t8VYqufiV79wYdFnUH7j5+vQBDSg6ZvjqNcpDF1yFgmzPyH3l6tfx179biN4+qM4+nqSu/Mo8S8ux5CVD4Cjrwch85+iftfWGEv1ZKzbwbkFX1jiCpg4gOAZj1od78LK/5Iwa02NxXeZwlGF78vjqHf/HZj0BrI//oaLqzbYTatp0YSGc8bj1KIJuvgk0ma9R+mxWMt21+6d8Jk8ErW/D6Unz5I26z10MYmW7V5jH8HjsQdQumgp3HGAtFfex5hfWNMhWlE4qvCbPYb6ve/AqDeQ9eE3ZH2w0W5ap/DG+L/2LE7hTdDFJZEycxklR2Nt0jl3bEnIlws4c+fTGFIybLb7vz4Bx+BGJAydUe3xCCFqlnRDEjUidM00DDkFnLh/Cpnrt9N89RQ0gT5207q0a0bTtyeQsnQDJx6YhoOrlqbvTLRsbzi6L96PdCN29BvEjFqIV/87aPRMfwBSl3/LoXYjLcsfnceiu5BF2spNADg18yfs85fJ23mE4/dNJufH3wlfNxvHRl7VEqf/SyNw6dCC2EdncX76MhpOfASPfl1t0im1GppGvULR4RhO936ewv0nabrmZZQuWgCc2zWj8ZJJpL2zjjP9JuPg6kzw288BUHTwNEcjhlkteT/tp/DgaQoPnEbhqKLZ2jkolErODJhGyoJPCJw7mvq9OldLjDUd+2Ue/bvS6D9Dr/p9vs8OxGvQPdUex7Xye2kELpEtiHt0FknTl9FwwiO4971KvJ9eirePOd6QK+L1fWYgXoPvIWnaMs70fRF9ahbNPn0FpVYDQJNlk1E4qjnz4BQSnlmER/+78B038IbGelmLNVMpyyngyH3TyFj3P1qsfhFNkP3r2PWWpjRfOp7kt7/iaJ8ZqNy0NH9ngmV72MrnUWhUHO0zgzOjF+P90B0EjH/Qsl0bGsCFVd+zv81TluX8whtTKfSZOgptRDjnh71E6svv0eCZIdTrc6dNOoVWQ+DqOZT8cZqEhyZRcuAEgatetZStU+vmBLz7EjnRW0joNwH9uRQCVrwMKgcAPIb3x2vUQFJnLOXco1PQNPGn4ayxNyTGKzWcPhLniHDOPj6TlJfex+fZwdR/wPZcVmg1NP54NsWHzxDX9zmKfj9B4w9n2Vy7Ckc1Aa9PQKG0f0vh0qUtnoPvrZFYhBA1708rC2FhYYSFhZGUlGSzbe3atYSFhbFkif2ngDUtKSmJ5557js6dO9O2bVv69evHF198USt5+TPJycmW3zIsLIzw8HDuuOMOFi5cSFlZ2TUf59SpU9xzzz20a9eOHTtq9snx31Xv9tZoQ/xImLyckthkUt/7msIDp/Eeav9Gz3dkby5u3kfWuu2UnDpH/KR3cO/WHk3jhgA0fPoBkt+KpmDfSQr2nuD8vCh8h98PgLG4FENmrmXxHXE/xsISkuZ/Zj728PspOnqWpHlRlMZfIHXZNxQcOIPv8PuuO06lVkODR3uSPPtDSo7Fk/fjftJXbMR7WG+btB797sBkKCNlzkeUxiWT/OqHlBcU49H3DgC8hz9A7ua9XFz/CyWnz5H43NvUvzsCTeOGmAxllGXmWhbnts1w63qL+el7uZH693RE7eNBwvg3KT19jrytv5G+fCO+4wZcd4w3InYclATOH0vwmxPQnUuz2d+hngshK6fhO+ZB9Beyaiymqii1GhoM7UnKq5fi/elSvMNt43XveyneuR+hi0smpVK8XoPuIe2ddRTs+gPd2RSSpi3DwcMNl04tUbpoMVzIIuml5ejikinaf5Lc73/F9dZWNzpk6t/eGm1TP+JfXEFJTDIp7126doZ2t5u+0ajeZH//GxnR/6P41DliJryLR/f2ODVuiIOLE7oL2cRPXUVJbAr5+06RtWkv9W6riMs5NICi4wlW13N5UWmNx6nQanB/5D7S562k9EQchdv2kb1qAx6P97VJW6/PnVBWTsaC1ejjk0iftxJjQRH1epsrFl5PP0z+D7vJWfMt+sQU0l5dDoCmSQAoFDR4ehAZb3xE0a6D6E4nkL7wQzQtQkChqPE4r4zXc8i9XJi7itLj8RT89BuZK7/C68kHbNK6P9AVU1k5qfM+RBefTOrc1ZQXFNtULHz/86hVS3Dl7/OfP56i30/WSDxCVAeTyVTry83smloW1Go1v/zyi836n3/+GcUN/EfuSqWlpTz55JO4ubnxySef8P333zN8+HDeeOMNPvnkk1rJ07WIjo5m9+7dbN++nTfeeIPNmzezatWqa97/3XffpXHjxnz//fd07lyzT47/LteIMIpOJGC84g99wf7TuEaGXSV9KPn7Kv6Q6C9ko0vKwC0yDLWvBxp/bwr2nbjiWKfQ+DXA0c+6dcDR3xvfEb059+oaTGXlAGiCfCk8eMYqXfHJxKvm5a/QtmyCwlFN4RV/BAv3n8K5XXNwsL60XNqHUXTgFFzxD0LRgVO4XMqHS0QoBb9VxGhIzUKXlI5LRAvrL1Uq8Z8+jIyPNqG/dGPtGNQQXXwK5bkVXRlKTibg0raZ5YlmdavO2B1ctDg1DeBM38kUHTpt812OwQ0xGU2cuu8/6JNtuzfcCPbiLfr9FM5t7cQbEUahvXgjzPGen/Y+eVv3WbaZjEZQKFBqHDEWlZA44S0MlypFTqGB1O/ZiYJfj9ZkeHa5RYZSdDzR6oY9f/9p3DrYv3bcIprbXMelSRm4dQilvKiUmHFvo08xx6UNC8CzVwfydh+zpNc286Mk7kINRXN1Ti1CUDiqKTlQcf2VHDiBU9tQm7LVtmtB8aGTVmVbfOgk2vbm69T51rYU/LDbss1UXEp891HoYs+haRaEQwN38rf+WrHv3iMkPPCM1fFqmjbcfC4XX3EuF/9+Eq2dc9m5fRjFlc7l4oOncG5fcQ44tW6K+0PdSJ3/kd3va/jiExTtO0bRb8fsbhdC3PyuqbLQoUMHm8pCYWEhhw8fpmXLljWSsT+zZ88e8vLymDNnDi1atCAwMJABAwYwfPhw1q5dWyt5uhYeHh54e3vTsGFDbrvtNgYMGMDmzZuvef+CggJat25NQEAATk5ONZjTv0/t64EhLcdqnSEz1+bm/jJHXw8M6Rdt0zfyQu3jAYD+iuMZssxPsCp3JWr0TH+KTyRYjY0wZOXapNMEeKPyrPcXo7Kl9vGgPLcQk85gWVeWlYtSo0blVd8mrf0YG1za7mmzvSwrD3WlvLvfdyuaIF/Sl399Rbpc8+90RRcAxwAfFGoVDm7O1xfkVVRn7OX5RcQOnknJ6XN2v6vkWDwJYxdabqBrg8pOvIa/EG9ZVi7qS/EW7jlGWVbFU1ivofeiUDlQdNi6Utt84+uE//we5flFZK35vrpD+lNqX3f0dspN08jTbnpHXw/06ZWv+zyb66/Nd68RseNtyvOLSf1oCwCaIB8cnJ3wfbwHkQeW037n2/g/0++GPHFX+XhSnluASX/FuZydi9JRjcqzvk3asvRsq3VlWbmoGjZA6epsTq9QELhqNs33fk7gx6/h2MQfAHVQI4yFxWhbN6fxxqU02/UpjeZPQulq3aWnppnjLbSO9/K5bCdeQ0blcznHci6jciBg0SRS531EeU6BzXc5tw+jfu87rlqREOJmYcRU68vN7JoqC/fccw8HDx6koKDiH4MdO3bQoUMHXFxcrNL+/PPP9OnTh3bt2vHQQw+xc+dOy7YnnniClStXMmLECNq2bcugQYM4f/48M2fOpH379vTq1YsDBw5cU8YVCgUlJSUcOnTIav2IESOsntQnJSUxZswY2rdvz5133smKFSss29LS0pg0aRKdOnWic+fOzJkzB51OB8DGjRt55JFHmDhxIpGRkaxfvx6TycSyZcvo2rUrkZGRjBo1isTExGvK79V4elr/4dXr9cybN49bb72Vzp07M2nSJLKyzDdJ3bt3Z//+/axYsYLu3bvfNDFUptQ6YrziDxGASW9A6aiuIr11Vyyj3oBCo8bhUj/uK/+wXb5hU1xxPKVWQ4OBd5F6aazCZRe/3Y1n71vx6H0rOChxvycS93s7olBf/9h+pVZjE+flz5VjVWg1mCrFaNKXoXBUXTqWo1WMl49V+TgNHr+P7A2/UJ5bcS3mbz+IUqvBb+rjKDRqNE398Xm6n918VJfqjP2fwF68pqvEq7QTr1FnP16XDi3wnzmC9GVfUZaZa7UteeYHxA6ZicLJkcbvvVgdYfwlDloNJl3l67IMhaaK61hX6ZzQGVBWSn92+mqOD5yN0smRsOXmcTnOoQEA6FMvcurxBSS/9zUBkwbiP862K1B1UzhpbK69y58Vlcv2KmkVjmrLTb/vy2PJ37KbpKdeofxiHkGfLkDh7ITSRYvCUY3PtKfIXLyGlOcX4dSqGX6LbmzZms9P+9futcRr1JdZ0vmMG4QhNYu8TTupTOGown/hRC7MXYUxv6g6QxBC3GDXVFlo2rQp/v7+Vjf+27Zto0ePHlbpTp8+zeTJk3n66afZtGkTjzzyCOPHj+fUqVOWNMuXL2fQoEFs3LiRvLw8Bg4cSMOGDdmwYQPBwcHMnz//mjLepUsXmjZtymOPPcbgwYNZunQpBw4cwNXVlcDAQMB84z1q1ChUKhXR0dHMmzeP1atX891336HX6xk2bBjFxcV8+umnLF26lJ07d/L6669bvuPIkSMEBwezfv16unXrxmeffca3337LokWLWLduHcHBwQwfPpySkpJrynNlqamprF+/nn79+lnWLV68mD/++IMPPviAqKgoTCYTY8aMwWQysWHDBtq3b8+wYcPYsGHDTREDgN+EgXSI/dyyaAJ8bG8YHdUYS3R29zfqDCgr3UgpL6U32qkYXL5ZufJ49bu1R6FQkPPDb1bHydtxhKTXP6fZu5PolBiN/wuDSf94M+WFxX873op8621vFB1t8wbmCk7lm0WFo8qSzqgz2P6hrvSbOXi44dalNRc3bLdKV3Yxn7PjFuE1sBu3nImmefRrZH5ibq0qL7j+OO2pztj/CUx24lVcJV5jqW28So1tvK63tqbpJ6+Q//PvpNmZ3ankZAKFe45xfvK71O/ZCccA+wOLq0vAxAHcGh9lWTSBPig0la/Lq5ebvYqBUmN73RedSCTv1+PE/mcZnr06ogn0JufnQ/wWPpzzi76k+PR5MtftIGnJBhoO61W9Qdph0ultrr2rla1Jbz+tqUSHqcw8q1PeVz+Rt/FnSk/EkTp9CQpHNW7dO0N5OUonDenzV1K0+zAlvx8n9aV3cOt5GypvjxqM0JrRTryWa7dUdw1pVZhKdGiaB+E1vC8XXl5u93t8Jg5Fn3iB/M2/2t0uhPjnuOZHe927d+eXX36hT58+GAwGdu/ezYwZM9i0qeJJ7ocffsjAgQN58EHzDBdBQUEcPXqUqKgoSyXgzjvvpHfv3pZjbt26lfHjxwMwaNAgJk+efE350Wg0rF27lpUrV7J582aWLVvGsmXLaNy4MW+99RatW7dmz549ZGRk8NVXX+Hm5kZoaCizZs3C2dmZXbt2kZaWRnR0NO7u7gDMmjWLsWPH8vzzz1u+Z8yYMbi6ugKwevVqZs6cyW233QbAyy+/zI4dO9i6dasl5j/Tv39/FAoFRqOR0tJSAgMD6d/fPLNPSUkJn332GevWrbN071q0aBGdO3fm4MGDdOjQAbVajVarxdPTk23bttVKDJWlR20le1PFHwSvfrfj3j3CKo3ax92mi8Jl+tSLlu5GFek9MKTnoE/LtuyvS7zUR9/bHKsho+J47t0jyNl20OaJLkDqsm9IW7kJlYcbhsxcAmc+gT4p829Eas2QdhGH+i4o1CpMBvP3qrw9MJbqKcstqJQ2G3WlGwK1t4clBnvbVd7uVjHWvzsCQ2YuRYesu6sAFOz8g2MdRqDy8aAsK496d7XHkJ2HsbhmBohWZ+z/BPbiVV8t3nR7ZelB2RXx1usWSZMPppL3434SJy229Al3qO+K2523kLupot97aax5cgkHz3pQg2M20j79kazv9lg+N+jfBY/u7a3SmK/j3Mq7Aubr+PK1aZ0+B5W7K+53tSXr24rjl8SYp4pVe9ZDl5RJWY719KElsck4+tb8TXRZejYO9V1BrYLL53IDD4w6PeV5tudy5Rt7VQMPyjIvUp6Th0lvQHe2YjIQk74Mw4UM1H7eFB82j8fRn022bNcnXJou18+Hsswbcz2UpWXbv3Z1eqsWSzD/NqpKZary9sCQcZH693fBoZ4LzX98H8AyfjH0x/dJmfEe7v3uQuXjQcvj68zb1SoUDkpaHl/HydaP1HSYQvwlppu8G1Btu+apU++55x527dpFWVkZ+/bto1mzZjRo0MAqTXx8PF9++SXt27e3LJs2bbLq5nL5qT+Yb/j9/PysPuv1+mvOvJubGy+88ALbtm1jy5YtTJ06leLiYsaOHYtOpyMuLo6goCDc3Nws+/Tr148ePXoQHx9PUFCQ5SYbICIigvLyckt+3d3dLTfZRUVFpKWl8eKLL1pii4iIIDU19S9141m+fDnffPMN3377LWvXrqVdu3YMGjSInJwckpKSMBgMPPbYY5bvuO2229DpdCQkJNgcq7ZiqKw8txBdYpplKTx4BudWjS1TQQK4dQyn8FCM3f0LD8Xg1rFiIK+jnxeaAG8KDsVgSM9Bl5yJW6fwimN1Ckefmo3+QkXfYdeIUAr2nqAyz/63EzzvKUxl5RgudfPw6NGB/D3XP9iu+MRZTIYyXCIr8u7aKZziY/E2c+IXHT5jlQ7MXVAu3/gXHYrB9YoY1Y0aoAnwsaoYOEeEUbjfdkYRTVN/mke/Bg5K8w2p0Uj9np0o3Hv8umO8muqM/Z/AXrwuHcMpPm4n3kNncOlQKd7IFpYxCc63hNLkg6nkfr+HxImLrfZ3cHelyfuT0bYOsaxzbtMMU1k5urMpNRGaRVluIaWJaZal4EAMLq0bo3SuuI7rdQqn4KD967jgUCxuna2vY6cAbwoOxqBydyXsg+dxadPEst21XQimsnJK4i/Q6Kne3PLLW1bHc2ndhJL4mh/wXHrKXLbO7SuuP+cOrSg9HmdTtiVHTqONCLda5xwRTskfp6HcSMmxWJxaNrNsUziqcQxoiD45A93JeIw6PU6tmlq2OzYLwmQ02n0vQU0pOZlgjjfiynO5JSXHbOMtPnwGFzvxFv9xhuxP/ktMj3HE9ZlEXJ9JJD41F4DEka+S//N+zg6dTmyv8ZbtOdE/UnIsjrg+k2o+SCFEtbrmloWIiAgcHBw4ePAg27Zto2fPnjZpysvLGTVqFAMGWE/Z6OjoaPl/Bwfr2VmUV5mX+c+sW7cOrVZL377mPq0hISGEhIRwxx130LdvX2JiYlCrr95fW6PR2KwrLzfPoGM0Gm3SXN62ePFimjVrZrXflZWRP+Pn50dwcDAAjRs3pnXr1nTu3JnNmzcTEWF+Gh8VFWVzzMpjG2ozhj+Tv/ckuuRMQt6eQMpb0bj37IBrZChnX7j0BEqtQuXuiiE7H4xGMj79gfCv5lLw+2kKD8UQPGcUudsOoktIBSD90x8InP44+pRMTOVGAqc/TtqHVwz4dFDi1NSPkjO20/uWxqXQ9O2JFOw/RdEfcfhNGIjS2YnM6O02af8qU6me7PXbCZw3lnMvLEXt7YHv6Ac5P20ZYG4ZKC8oxlSqJ+f7PfhNe5KAuaPJ+nQLXo/ei4OrMznfmZ8gZ0Vtofn6eRT+foqiwzEEzn6KvF8OoEtMtXyfNizI7qw4+qR0NE398ZvyOFmf/UC9bpF4DerGmQHTrzvGGxH7P4GpVM/FDdsJmDeW888vReVzKd7pV8SbX4xJpyd386V454wmK2oLXkOt4w16cwKlMUmkvP4pKq+Kgfbl+cXo/8/enYdFVb0BHP8O+y6LGyoggoJrCpZF6k8lK7fUTE3L0szct9zNHdfc19QkM3NLzV0zd9wL1NQUURQEFwJcYNgGZub3BzU6MiIlMIjvp+c+T3PvuTPvcbhwzz3vOSf6LkmHwnCb1oeYEYsxtbfBbXof4r/fhUb531MF/4uHJy+REZtA5fn9uDlrA85N62LvX5lrX2bX+cnr+O73e6mxZSLJv10hOSwCz8nduHfgDOk3snsE7x88i9fMnkQOWYqpgw1eM3txJ3gPamUaDw6dw2P0R3iM7kzc2oPY1fGmfL82RA5fXuD11KZn8HDLAcpO7MvtEXMwK+mEc/f3uTNmIQCmJZ3QJKegzVCR/MsxSg/tRplxvbm/dieOHd7FxM6GpF3ZKbr3gjdTbs5w0i9Fkn4+Apc+H6JJTUN56DTatAwerNtNmTE9USeloE1Nx3ViX5L3nUSdUHi9bNr0DO5vPki5oN7EDpuHWSknSn7ellujFwFgVvLvazdDxcM9xyk7/FPKTexJ4urdOH/4Dib21jzccRRNShrqhzkXk1PF/oUmJQ1Niv7Pq/qhEk26ClX0nRznCGFsmiI+damx5flO3cTEhEaNGnHw4EEOHTqUY7wCgKenJzExMXh4eOi2bdu2sW/fvlXdXWwAACAASURBVHwNGiAiIoJly5blWKPAwSH7j6+zszMVK1YkJiYGpfLRL7QFCxYwcuRIvLy8uHnzJg8ePOpSP3fuHKampri7u+f4PAcHB1xcXIiPj9fVrUKFCsyZM4crV/77E1ITExO0Wi1ZWVm4ublhamrK/fv3dZ/h7OzMtGnTuHUr51PFolKHHDQaIrpNx7xkCWr8MpOSH/yPq91noIrNTv2xq+uD3x/f6WZHUoZFcGPYN5Qf1J7qO6ahTk4lcuBC3dvdWbKNxC1HqfztcCqvGE7CtqPc+Wab7riZkz0m5mZkPcj5hyv1zyhuDPsGt5EfUXP/HCzKOnO5w/h8y5ePnRRM6h9Xqbw+CLdpvbkzfwP3/04hqXVmlW5ufY0yjciuQdj5++K7ew52dX259ukk3R/UlDNXuDliMWUHdMBn6wzUyalED56v91nmJR0N1lGryuL651OxD6hJ1f0LKdnpbSI/n0bahch8qWNB1/1F8U99vdcH4T61N3fnb9ClC9UM06/v9a5B2Pr74rNrDrZ1fYnsml1fqyruWFdxx6aWNzV/X0nNsFW6zbnt/wCIGjCHjGuxeK+bRMWlI0ja/zu3pxlhOmiNhsufzsC8pAO1935N6fb/I7zbTDL+TuGzf9WH1y6swLJ89nWcHBbBtaFLqTC4HbV2TUGdlMrVAYt0bxfRZz5pV29RfeN4fFcM4d6voURNzl4PJS3yNpc/mU6JhrWofXA2HiM7ET35RxK2FE6DMm7qt6Sdj8Djh2mUndSPhEVrSd6d3QCocnKNboE2jTKNmB7jsfGriufWhdj4VSPm8/G6n+XkfSe5O2EJpQZ+jOeORZiXK8XNbmPQ/v37Jm5GMMl7T1Bh8Rjc18wg/UoUd0bOKZQ6Pu7O5GDSzl/Fc80Uyk/uw18L1/NwV/a/ddXfV+vWUdAo04jqPgkbv6p475yHTd2qRHWb+MJdu0KI56PQPmMlCB8fH1auXElAQAD79+9n+PDhuLm5sW1b9s1aly5d8PPzY/DgwZw7d47OnTszfPhwGjduzIkTJwgKCmLJkiU0atRIryzA3LlzOXPmDKtXrwYgJCSEXr16cenSsxdvuXnzJu3ataN27dp88cUXuLq6EhUVxeLFiylTpgzz5s1DrVbTsmVLfHx86N+/P7GxsQwdOpSJEyfy7rvv0rZtW8qUKcOXX37Jw4cPGTNmDK+88gqzZs3i559/Zt68eXqDupcvX87KlSsJCgqicuXKBAcHs2fPHnbu3EmZMmVyjTc2NpbAwEA2bNhA+fLZU+kplUqCg4PZtm0be/bsoUKFCowfP56TJ08yceJESpcuzezZswkPD2f37t1YWVnp/RtqNJpCq8PpcgW3wFdRY26qNnYIooAYaVkYo0lVFcxsWEWRs33BDOYvqrKyCmYNlaKq5o0dzy4kxH/UsLzhRWMLU8itA8YO4an+1dyF9evXR6PRGOxVAKhduzazZs1i0aJFzJo1i/LlyzN16lQaNWqUH7HqcXd3Z/369cyfP5+BAweSlJRE6dKladWqFX369AGyU56WLFnCpEmTaNu2LS4uLvTt21c3wHrx4sUEBQXRsWNHbGxsaNWqFUOGDHnqZ3bv3p20tDQmTpxIUlISVatWJTg4+JkNhcd17NhR9/82NjbUrFmT4OBgKlTInjpw1KhRfP311wwePJiMjAz8/PwIDg42uKaCiYmJUeoghBBCCFFcSBJS7p7ZsyAESM+CKB6kZ6H4kp6F4k16FkRBalAEehaOFpeeBSGEEEIIIYqTor6CsrEVycbC3r17GTly5FOP+/j4sH59zkWMjCUxMfGpqVn/CAkJydcZh4QQQgghhChoRbKxUL9+fbZu3frU449PxVoUODo65hovgK2tbSFFI4QQQgghRP4oko0FW1vbF+rm2tTUVLd2ghBCCCGEeHFIGlLu/tuKaEIIIYQQQohir0j2LAghhBBCCFEYZGLQ3EnPghBCCCGEEMIgaSwIIYQQQgghDJI0JCGEEEII8dKSAc65k54FIYQQQgghhEHSsyCEEEIIIV5aWulZyJX0LAghhBBCCCEMksaCEEIIIYQQwiBJQxJCCCGEEC8tWWchd9KzIIQQQgghhDBIGgtCCCGEEEIIgyQNSQghhBBCvLRknYXcSWNBCCGEEEKIF0x6ejrDhg0jMTERW1tbZsyYgbOzs+54SEgI3377LZA9LiMsLIydO3eSnp5Or169qFixIgCdOnWiefPmT/0chVZGdYg8OF3ufWOHUGjMTdXGDkEUEIXC2BEUrlSVubFDKDTO9qnGDqFQZWWZGjuEQlXzxg5jhyCKsTpl3zR2CJy9e/xfn7Ny5UqUSiX9+/dn165dnD17ljFjxhgsu2LFCpKSkvjyyy/ZuHEjycnJfPbZZ3n6HBmzIIQQQgghxAsmLCyMBg0aANCwYUNOnjxpsNzdu3fZtm0b/fr1A+DixYscPnyYjz76iNGjR6NUKnP9HElDEkIIIYQQwog2bNjAhg0bdK87duxIx44dda83btzIqlWr9M5xcXHB3t4eAFtbW5KTkw2+98qVK+natSsWFhYA1KpVi/bt21OjRg2++eYbFi9ezIgRI54amzQWhBBCCCHES6soDHB+snHwpPbt29O+fXu9ff369SMlJQWAlJQUHBwccpyn0Wg4fPgwgwcP1u1r2rSprmzTpk0JCgrKNTZJQxJCCCGEEOIF4+fnx5EjR4Dswcz+/v45ykRERODp6YmVlZVuX/fu3Tl//jwAJ0+epHr16rl+jvQsCCGEEEKIl5a2CPQs/BedOnVixIgRdOrUCXNzc2bPng3A119/zbvvvkutWrW4ceMGbm5ueudNmDCBoKAgzM3NKVmy5DN7FmQ2JJEnMhuSKA5kNqTiS2ZDKt5kNiRRkGqVfcPYIXD+ruHByUWBpCEJIYQQQgghDJI0JCGEEEII8dLSSJJNrqRnQQghhBBCCGGQ9CwIIYQQQoiX1os6wLmwSGNB5MnLdBm9bINgPfwfGDuEQnPqhKuxQyhUySYvzyDYEup0Y4dQqJzLphg7hELjMrcn6ac2PLtgMWL1+tPn2xeisEkakhBCCCGEEMIg6VkQQgghhBAvLRngnDvpWRBCCCGEEEIYJD0LQgghhBDipSUDnHMnPQtCCCGEEEIIg6SxIIQQQgghhDBI0pCEEEIIIcRLSwY45056FoQQQgghhBAGSc+CEEIIIYR4ackA59xJz4IQQgghhBDCIGksCCGEEEIIIQySNCQhhBBCCPHSkgHOuZOeBSGEEEIIIYRB0lgQQgghhBBCGCRpSEIIIYQQ4qUlsyHlTnoWhBBCCCGEEAZJz4IQQgghhHhpabUaY4dQpEnPghBCCCGEEMIgaSwIIYQQQgghDJI0JCGEEEII8dLSyADnXEnPghBCCCGEEMIg6VkQQgghhBAvLa2s4Jwr6VkQQgghhBBCGCSNBSGEEEIIIYRBkoYkhBBCCCFeWjLAOXfSsyCEEEIIIYQwSHoWhBBCCCHES0sGOOdOehaEEEIIIYQQBkljQQghhBBCCGGQpCGJQmFRvhSVZvXG/lVfVLcSiJ6wkgeHzj61vHOrANxHfoR5WWcehvzB9aHfkJX4MEc533XjSNxxgvi1+3X7zJztqTilB44NX0Gdks6db3dw99udBVKvJykszKgw6QscW7yJNiOTv1Zs46+lWwyWta5aEbepvbGu5kn6tRhiRn1D6vlrOcqV6d8eK68KRA+a+2inqQmugzvh/EETTGwsebDrOLETg9Gmqwqqas9mZo5Nj4FYBDSCTBXp2zeSvnWdwaIWbzbGqsMnmJZ2RX3nFmlrg8kMPQGA85YjBs9JXfcd6T+tKqjo84VVhZLUnP0FTq9VIT02kcvjVxN/8Nwzz/Ma3BY773L80XdxIUT539lUKMmrsz6n5KuVSb2VyNkJa7h78I9nnldtUBvsvV053e8b3fu0+n2+wbKnB3xD1MZj+Rr3sygszHAd34sSzeujUWWSGLyVhOWbDZa18vWk3OS+WFWtSMa1WG6PXUza+at/v5GCan9uwsTKUu+cS690RJOcgqlLCVzH9MCufh3Qakk+9Dt3Jq9Ak5xS0FV8OnNzHL/sj3WT/6HNzES5biPKNRtyPcWiVg2cJn5FXNtOun1lfl6LmWvZHGVTdu7hwZSZ+R72f6XKzGL66l38+vufWJib0eWdN+jWooHBsmeuRPH1mj3cuJOAR1kXBnV4m4Ca3jnK7Tz+BxsP/c6qMZ8XdPiigGgkDSlX+d6z4OPjg4+PDzExMTmOrVu3Dh8fH+bOnWvgzIIXExPDoEGDqFevHrVq1eK9995j7dq1Ronl31i2bBk+Pj7s3bs3xzGlUknnzp2pWbMmc+fOJTExkd27dxshytz5fD+SrPvJXGw2nPiNh6i8YjiWbqUNlrV9xRvvef25NX8Tf7YciamdNd4LBugXUiioOPlzHP9XO8f5VYJHYOXpyuVOk4gctBDXL1pRtnuLgqhWDuW+6oatvy/XOo8jZtQSyvbvgGOrnH+ITKwt8fphPClnIwhv8SXK3y5R6fuxmNha65Vzeq8BroM75Tjf9cvOlPz4XWLGLuNq+6+wrFiOiguGFFi98sLm096Y+VYnefyXpCydjXX7LljUb5KjnFnVWtgO/IqMnZt5OLg7GQd2YzciCFPPygDc79ZWb0tdvRxNchKqg3sKu0r/Wt1VQ8m8n8yJd74i9qcj+AUPxtq9VK7nuLYNoPLQDwopwudTf+WXZNxLZl+zcdz46ShvrhiIrVvu9XNv8wbVh7yvty/tdiLbavXR2659vw9lVBy3fgkryCoYVHbkZ9j4VeVGl6+4/dUiSvXtSImWDXOUU1hb4rFyAqnnwol8bxApoX/isWK87rq1cC+LwsKcK/W7Ef7ax7rtn8aA27xhmJd1IeqTMUR1n4CljwflZwzI8TmFqUS/nljUrE7CgKE8mDEH+24fY/1W46eWN/PyxHnqBBQm+rcP8Z/15k6Ldrrt3oSpaFUqUjZtLegq/CtzNuzl3LUYlg/vytiurfh2ewh7Tl3IUS4xScmAeWtp+lp1Nk3uy9uvVWfwgnXcTnigV+63y9cJ+n57YYUvhFEUSBqSubk5Bw8ezLF///79KBSKgvjIZ0pPT+eTTz7B3t6eVatWsWvXLrp27crMmTNZtapoP63cuXMnHh4ebNmS8wn1pk2biI6OZuvWrXTr1o1Zs2YZ/Lc3Joc3a2BdqRzXh31D2tVYbi/agjI0nFKdAg2WL/tZcxJ3nyL+p0OkXo4mcuACHBvXwbJi9lMr87LOVP1pAk5vv0rWA6XeubY1K+FQrxrX+s4l5XwkSScucnPyD5Tr27bA62libUnJTk25NTGYtAuRPNz3G3FLf6ZU1+Y5yjq2qo82M4tbQd+RcS2WWxODUSen4tSqfnYBUxMqTOmF+6z+ZETfzXF+qa4tuD3zR5L2/Ub6lWiiBs2lxNuvYVmpfEFX0zBLKyybtiQ1eBHq6xFk/nactC3rsGye89/dovE7qE6FkLFvJ5q7t8jYtZmsi2exqJ99g6J9cE+3oVBg3e4jUoMXoEn4q7Br9a+41K+OrVc5Lgz5FmXELa4v3M790AjcOhu+8VKYmlB9Rndqze1FalRcIUf775V+sxr2Xq6EDgsmKeIW4Yt2kPD7VTw7NzJYXmFqgv/0brw65wuUT9RPq9GSHv9Qt1mVdqTSR405PWgZmclphVCbx+K0tsTpw3e4M3k56RcjSd5/moTlm3H+JOcDhhItG6LNUnN3SjAZkbHcDfoWtTKVEn8/mbb0diPzdjyZt+PJSnig2wDMyrpg92Ztbo1eRPrlG6RfuMadSctxaPo6iid6IgqLwsoK29YteDh/MZlXrpJ+9ATKHzdg+0Ebg+Vt2rSk1LKFaO7dz3FM8+Ahmnv30dy7jzYlFYde3Ule+SOZV64WdDXyLDVDxc+Hwxj+UTOqeZajsV9VujZ/k/X7T+coey7iJgDdWzbErYwzn7f6H5bmZpyPfPQgdOmWQ/Sd/SMVSjsVWh1EwdAWgf+KsgJpLNStWzfHDatSqeTs2bNUq1atID7ymU6cOMHDhw+ZNGkSvr6+uLm58f7779O1a1fWrTOcKlEUXL16lYiICPr27cvRo0dJTEzUO65UKnFzc8PLywtHR8ciOaLfzs+HlD9voElJ1+1L/i0ce3+fp5SvQvKpS7rXqtuJZMT8pStvW8OTjOg4Lrw7FHVyqt65lh5lyLqfTHrkbd2+lD+jsCjrjGWF3J+APi/rap4oLMxR/v4o9pTfL2NTqzKY6l9qtn4+KEMvw2PfV0roZWz9sutoamuNlVcFrrw3jJQz4Xrnmjk7YGpvQ0rYo/1ZcffISkzSnV/YzCp6gbk5WZcfPaHLunwBM29fMDHVK5ux+2fSnkwn0mpRmFvkeF/rjl1Rx0ShOrKvQOLOT47+lUm6GIX6sZ/z+6ev4Fi3isHyprZW2FUux4nmY3gQWnRuqJ7Gxd+bBxejyHqsfgm/XaGkf860DAAzWyvsvcuxv8U4EsNyptc9rtaYD4ndHUrC6Sv5GnNeWFXNvm5TH7tuU0MvYV2zSo7r1qa2D6lhl/Su29TQS9j4+QJgWdmdjOu3DH6OJjmVqM8moIp69LsJLShMTVFYGCcj2Lxy9nWb8cej6zbjjwtYVPXJUXcAq1f9uR80HeX6Tbm+r23H90ELyWtzT2cqbBE376LKUlOnsrtuX50qHvx54xZZarVeWUc7G5JT0/n1t4totVoOhl0mJV1FFbdHqVahV6JYOvQT3qprnPsaIQpLgTQWAgMDCQsLIzk5WbfvyJEj1K1bF1tbW72y+/fvp0WLFrzyyiu0bduWkJAQ3bEuXbqwfPlyunXrRq1atWjfvj03b95kzJgx1KlTh3feeYfQ0NA8xaRQKEhLS+PMmTN6+7t168a3336rex0TE0PPnj2pU6cODRs2ZOnSpbpjd+/eZeDAgbz22mvUq1ePSZMmkZGRAcDPP/9Mhw4dGDBgAP7+/mzcuBGtVsuSJUto0KAB/v7+dO/enaioqDz/O0J2r4KXlxfNmjXDwsKCHTt26I6NHDmShQsXcvbsWXx8fBg5ciRbtmxhx44dNGmSnf6RnJzMiBEj8Pf3580332Ts2LEoldlP40+fPk3Dhg2ZNGkS/v7+LFy48F/FllcWZZxQ3dV/EpUZ/wCLci5PLx93L2d51+zyD/aHcX3oErLuJec4NzP+Iab2NpjYWun2WVbITncyc3Z4rno8i1lpJ9QPlGgzMh/Fk/AAE0tzzFxK6JU1L+1E5hN1zEp4gLlrSQDUSSlc+3AM6eHROT4n66ESjSoTi7/LApjYWGHmaFfgdXwahZMLWmUSZD4aM6F5eB+FuQWKEvp1V0dFool9VC9Tt4qY1fIj85J+7rvCyRnLJs1yNiyKKMsyjqQ/8XOeEf8QK1dng+WzklI5/X4QyZduFkZ4z826tCNpcfopGOnxD7F+Sv0yk1I5/MEUHl7OmZL6OKdXPCnbsAZ/zvk532L9N8xLO6N+kIxW9ei6zUq4n33dOuv/7JqVdibryes28QFmZbOvRUtvd0xtrfFcPx2fUz/g8d0ELDyze/s0KWkoD4fqNTRcur5HevgNNEnGGbNg4uKMJikZHqu75t59FBYWmDg65ih/76uJpB95xngSc3PsOnVAuXqd3vsWBQkPkilha42lhblun0sJOzKz1Nx/4sGTn48HH75Vj+FLNuL/2UQGL1jHV5+2pFK5Rw+dVozshr9vxcIKXwijKZDGgpeXF+XLl9e78T9w4ABvvfWWXrnw8HCGDRtGjx492LFjBx06dKBfv35cvnxZV+abb76hffv2/Pzzzzx8+JB27dpRtmxZNm3ahIeHB1OnTs1TTAEBAXh5efHRRx/RsWNH5s+fT2hoKHZ2dri5uQGgUqno3r07ZmZmbNiwgSlTprBixQq2b9+OSqXi008/JTU1lR9++IH58+cTEhLC9OnTdZ/xxx9/4OHhwcaNG2ncuDE//vgj27Zt4+uvv+ann37Cw8ODrl27kpaW9272Xbt20bhxYywsLGjYsKFeKtJXX33FZ599Rq1atTh27BhfffUVzZo145133mHTpuwnP6NHj+b+/fusWbOGZcuWcePGDUaNGqV7j7i4OJRKJVu2bKFt24JJ1TGxttD7QwygUWVi8tgv7Jzls3KWtzRc/nHKsxFk3E7Ac3pPTGytMC/rTIUhHQEK/OmdibUlmifq+U+9n6yribVlzjpmZOUtRrWGB7tP4Dr8YywqlEZhZUGFCdkD6xTmxnlCqbC0gswnbgz+bjgozHL2GOjOK+GI3cjJZF26QOZp/ZsQy8AWaOJuk3kmZ4pAUWRq4PvP7ef8RWO4flmY5uG6zI13l0DuHr5AUoThJ/IFzcTKwLX4dz0VBq7bHNd4xqPv2NK7AiYl7Phr/lpufjEZTYYKz7VTMbG3yfG5Lp+1pkTzN7kzZUV+VudfUVjlvG61f79WmP+379W6yf9QKCB1d84xdsaWpsrE3Ey/p9Pi79eqTP2fgbQMFbfi79PjvYasmdCTQR2a8vWaPZy/lnvjV7yYtFqt0beirMCmTm3SpIkuFSkzM5Njx47pnnb/Izg4mHbt2tGmTRvc3d3p1KkTLVq0YPXq1boyDRs2pHnz5nh7e9OkSRPs7Ozo168fXl5etG/fnuvXr+cpHktLS9atW0ePHj1ISEhgyZIlfPTRRzRr1oyLFy8C2alKf/31F9OnT6dKlSo0aNCAcePGYWNjw9GjR7l79y4zZ87E19eX119/nXHjxrFhwwa9HpSePXtSqVIlSpYsyYoVKxg6dChvvPEGXl5ejB07FlNTU4MDlQ05d+4cMTExukbW22+/TXh4OOHh2ekn9vb22NjYYGZmRqlSpbC3t8fKygoLCwucnZ25efMm+/bt4+uvv8bX15caNWowY8YMfv31V+7cuaP7nM8//xx3d3cqVKiQp7iepVz/drx6dY1us6xQOucfXQtz1GkZBs/XZGTmuGnOrfzjtKosInrMxK52ZV4NX80rB+cRv/4AAOoCzoXWZqhy3Bj+U2/NE7Fr0g3U0dIsR7mniR3/LVkJD6l2bBm1zq9BrUwj9dIN1MrUZ59cALSZKnjy5uLvtCKtKt3AGaBwKYVD0HzQaFDOHKf3xBXAon4TMg4VvRuOf3gNbMPb17/XbdZupXI2CvP4c1sUVR3wHu9fC9Zttm4lDdTPjKy0/z4Dl8JEQfnmrxK18ejzhvufaTJUBn/fAGjS9b87g9e4pbmu3I0OI4hsPYiUk+dJOx9B7MCZKMzMcHjrdb1zXLq3pezo7tyZtJyU48+eTaqgaFU5r9t/GgnajP/2c2v9VmPSDoX85/MLkqW5GZlZ+ulGqr9fWz3R6P1+93Eys7Lo+34gVT1c6daiAW/Vrcby7YZnaxOiOCuwx5CBgYH06dOHrKwsTp06hbe3NyVLltQrExkZSUREBJs3P5qiLjMzk1q1aule//PUH7Jv+MuVK6f3WqXK+x8qe3t7hgwZwpAhQ7h+/TqHDx9m5cqV9OrViwMHDnDt2jXc3d2xt7fXnfPee+8BsHz5ctzd3XF8rGvWz88PtVqtSy1ydHTEzs4OgJSUFO7evcvQoUMxeWzWiIyMjDynIu3cuZNSpUpRu3b2jD//+9//sLCwYMuWLXq9A08TGRmJVqulceOcAyyjoqJ0cZUvn7+DYuNW7yVxx3Hda5f33sSxiZ9eGfPSjmTG5RwkB6C6cw/zJwaMZaftGC7/pNQL1/mjQT/MS5Yg62EKVhXLolWrUd2K/5c1+Xcy797DtIQtCnMztH8/pTIv5YQmXUXWA/2Uqcy4RMxL6dfRrJQTWX/lrY7q+8lEdpmAqYMtWrUGTUoaNc6sQhVrnEHA2sR4FLb2YGYGWdl1N3F0RqvKQJucM13MpIwr9hPnos1IJ3nsILTJSXrHFS6lMPOohPJUSI5zi4qbq/ZxZ9tJ3WvXNm9Qqon+7FyWpUuQ8deDJ099IUT+cICY7Y96ddxav45rk1f0yliVdiQ9j9elIS51K2NmY8nt/c+eXragZMYlYlrCTu+6NSvlhCZDhfrJ6/ZuImZPXrclncj6Kzs16ckeVK0qE1VsHGZlH6Vclh7UmdIDOnN7wlLurd5VEFXKM018Aib2dvrXrYsz2gwVmqSkZ5xtgLk5lv61uTdqfD5Hmj9KOzmQlJJGZlYW5mbZtz8JD5OxMDejxBMz0f154xbe5cvo7atasRybDv1eaPGKwqMp4gOMja3AGgt+fn6YmpoSFhbGgQMHaNq0aY4yarWa7t278/77+tPqWVg8SlswNdXvMjQx+W+dIT/99BPW1ta0atUKgEqVKlGpUiXq169Pq1atiIiIwDyXbldLy5yzVaj/HhCl0WhylPnn2Jw5c/D21h8A+Hhj5GnUajV79uwhISGB6tWr6+3fsWMHw4YNw8ws969PrVZjY2PD1q05p64rVaoUFy5ceGrdnof6gRL1Y7MUKcOuUL7/+9ld+H8/ZbV/tSrKMxEGz1eeicDhVV/d2gkW5VywrFDqqeUfZ1rCFp/vRxHx+UwyE7LXZXB651VSLtxArSzYnoXUP6+jzczC1t8X5ans3irbV6uSejES1Bq9silnrlB2QAe9fbb+vvy1NG952x5zB3F/+1GSDmVPM2lTpwqmdjakhIY/48yCkXXjGmRlYeZTg6w/s2/8zKrWJCsyAjT6T/IUdvbYT5iNNlVJ8vghaJNzrp9hVqUamvuJaG4X3S7/zAcpZD54lGv+IPQqXgPaYGpjiTo1++fcqZ4vD8KK/uBlQ1QPUlA9Vr/E0KtU7f8eptaWut6SUq9VIfFM7oOXc+Pi582Di9FkFfC1mZv0SzfQZmZh41eVlNPZvxNt6lYn7cK1HNdt6rkrlO7bUW+fjX9VEpZtBlMTfI6u5O7UYB7uzG7kmthYYVmxHBmRsUD2GIVS/T7k0xEVygAAIABJREFU1uiF3F9v/F6zzIjs69aiZnVUZ7N7OCxfqYHqSkSOuueFuZcnCgsLVH9czO9Q84WPe1nMzUw5dzWGV6t6AnA24ibVPFwxe+Jeo5SjPRGx+jPR3bgdLzMfiZdSgaUhmZiY0KhRIw4ePMihQ4dyjFcA8PT0JCYmBg8PD922bds29u3L/5lPIiIiWLZsGVlZ+nmJDg7ZA0KdnZ2pWLEiMTExugHAAAsWLGDkyJF4eXlx8+ZNHjx49JTw3LlzmJqa4u7uzpMcHBxwcXEhPj5eV7cKFSowZ84crlx59owfp06dIiEhgblz57J161bdFhQURGJiot54kMc9PjWtp6cnqampqNVqXQwA06ZN06tjQUs6eYmM2Hi85vXHuoob5fq2xc6/Cn+tyf6eFeZmmJdyhL8bgnE//IJL2waU7vwW1r7ueM0bwP0DYaTfuJPbxwCgfpiCibUlHuM/xdKjDM4t36D8oPbcmrexQOsIoE1XcW/TISpM6YXNK5VxaPoaZb5oQ/zK7AXhzEo5orDMbgg/2H0CExsrKkz6AqvKbpQf1x1TOxvub8/bYlRZ95JwHfYx1lUrYlO7ChXnf0n8ql2oHxbe96pHlUHG4V+w7TkY08q+mL8agFXrjmTsyu41VDg6w98PAaw/6oHCvgQpC6aDqSkKR+fszebR5Adm7p6oY6KMUZP/LPHEJdJi4qk1vzd2PhWo1O89HP29ifkxOx1TYW6KRakSYGKc6aOfV/zJy6TGJlBvfk8cqpTHt29LXPy9ifzxEAAm5qZYlSqB4l/Ur4SvGw+vxBZUyHmiTc/gwc8HcZ3UG+taVbAPfI2SPdqSuCp7Mgmzko+u26Q9xzCxscJ1Qi8svd0oO+ZzTO2ssxsHag3Ko2coM6QLNq9Wx7KKBxXmDiXzr3skHziNeblSlBnRlXtrdpO8/zRmJR11G//xIdhz1z0jg9Tde3EcNhDzar5Y1Q/ArnMHUn7Kfmhh4uwElk8fc/Qk80qeqO/EoU03nHpobNaWFrSqX5upP+zkQmQsh8+E88Oe43R6OztNLOFBMul/9w61a1SXsPBoVu46Suxf9/j5SBjbjp3l43cCjFkFIYyiQH9DBQYGsnHjRhwdHfXSif7RtWtXfvnlF77//nuio6NZt24dS5cuNXjz/bw++eQT4uLi6N27N7///juxsbEcO3aMwYMH06xZM8qXL0/9+vUpW7YsY8aMITIykiNHjrB69WoaNmxIQEAAFStWZPjw4YSHh3P69GkmT55M8+bNcXIy/KSha9euzJ8/n/379xMdHc3EiRM5ceIElSpVema8O3fupFKlSjRv3pwqVarotnbt2uHq6mqwtwDAxsaG27dvExcXh5eXFw0aNGD48OH88ccfhIeHM2LECBITEyld2vCCaAVCo+FKt+mYlyxBzV9mUvKD/xHRfQYZsdlpQfZ1ffD/4zss/54dSRkWwfVh31B+UHtq7JiGOjmVyIF5n6npWu85WJR1odaBubiN6Mz1YUu5/2vhdB3HTgom9Y+reK8Pwn1qb+7O38CDHdkNgJphq3TrKGiUaVzvGoStvy8+u+ZgW9eXyK6T0KTk7QnrnVlrSLt8A++fplBpxSju7zrO7Wk/FFi98iL1u8VkXQvHYeJcbHsNIf2nVaiOZ99IOq3cgsWb2WOWLAL+h4mtHSXmBuO0cotus/likO69FI7OaJU505eKNI2WsE9nYVHSgTd/nUr59g0403UOaTHZP+dOr/rw1sVlWJcv+Yw3Kpq0Gi3Hus7BsqQDb++dTMX2DTj22TxSYxMAcKlbhdbnl2D9lFnODLEqVQLVAyM1cB9zZ/IK0s5fpeKaKZSb3Je/FqwnaVf2OArf336kRMvsdRQ0yjSiP5+IjZ8vXjvmY+NfjajPJuiu29sTlpJ8OBS3hSPw+nk2ANHdxoNag31gPUwsLXDp0hLf337U2yzcyhgOrBA8nP8NmZevUHLhbByHDyL5u9WkHTgMgOuuzdgEPn2BtieZODv9t/SlQjS007tU9yxPjxnfM3nVDr5o3Yh369UEIHDgTPaezu4VqelVgfmDOrP39EU+GLOENb+eZFrPdtSr9uy/3+LFY+zBzUV9gLNCm88R+vj4sHLlSgICAkhPT+f111+ne/fu9O/fH8ieDtXPz4/BgwcDsHv3bhYtWsTNmzcpX748vXv3pk2bNgbLzp07lzNnzugGQIeEhNCrVy8uXbpkIJKcIiMjdbMgJSUlUbp0aVq1akWfPn10qTg3btxg0qRJhIWF4eLiwqeffkrXrl0BiI2NJSgoiFOnTmFjY0OrVq0YMmQIlpaW/Pzzz8ybN0/vib9arWbRokVs2rSJpKQkqlatyujRo/XGZBiiUqkICAigV69efP55zuXjlyxZwpIlSzh27BirV6/mxIkTurUizp8/T58+fcjMzOTUqVPcv3+fKVOmcPjwYRQKBQEBAYwdO5ZSpUpx+vRpPvnkE/78889npjSdKvd+rseLE0sz9bMLFSMe/i9mTv1/ceqEq7FDKFTJT6xxUZxVt86ZzlacOZUxzmQGxuAyt6exQyh0Vq93fHYhkW9KOhheD6cwJSQ9O9XaWPK9sSCKJ2ksFF/SWCi+pLFQfEljoXiTxkLhksZC7owzKbsQQgghhBBFgEaem+eqWDQW9u7dy8iRI5963MfHh/Xr1xdiRLlLTEw0OOD7cSEhIXmaNUkIIYQQQoiCUiwaC/Xr13/qgF/Qn4q1KHB0dMw1XgBbW9tcjwshhBBCiOcnGfm5KxaNBVtb2xfq5trU1FQ3jakQQgghhBBFlXEmdxZCCCGEEEIUecWiZ0EIIYQQQoj/QoOkIeVGehaEEEIIIYQQBknPghBCCCGEeGnJAOfcSc+CEEIIIYQQwiBpLAghhBBCCCEMkjQkIYQQQgjx0pIVnHMnPQtCCCGEEEIIg6RnQQghhBBCvLS0MnVqrqRnQQghhBBCCGGQNBaEEEIIIYQQBkkakhBCCCGEeGnJAOfcSc+CEEIIIYQQwiDpWRBCCCGEEC8tWcE5d9KzIIQQQgghhDBIGgtCCCGEEEIIgyQNSQghhBBCvLRknYXcSc+CEEIIIYQQwiBpLAghhBBCCCEMkjQkIYQQQgjx0pLZkHInPQtCCCGEEEIIg6RnQQghhBBCvLSkZyF30rMghBBCCCGEMEgaC0IIIYQQQryg9u3bx5AhQwwe++mnn3j//ffp0KEDhw4dAuDevXt89tlndO7cmUGDBpGWlpbr+0tjQQghhBBCvLS0RWD7ryZPnszs2bPRaDQ5jsXHx7N69WrWr19PcHAwc+bMQaVSsWTJElq2bMnatWupVq0aGzZsyPUzpLEghBBCCCHEC8jPz48JEyYYPHb+/Hnq1KmDhYUF9vb2uLu7Ex4eTlhYGA0aNACgYcOGnDhxItfPkAHOIk9ev/2zsUMQ4rk1N3YAQgghipws1S1jh8CGDRv0nvB37NiRjh076l5v3LiRVatW6Z0zdepUmjdvzunTpw2+p1KpxN7eXvfa1tYWpVKpt9/W1pbk5ORcY5PGghBCCCGEEEb0ZOPgSe3bt6d9+/b/6j3t7OxISUnRvU5JScHe3l6338rKipSUFBwcHHJ9H0lDEkIIIYQQopipVasWYWFhZGRkkJycTGRkJFWqVMHPz48jR44AEBISgr+/f67vIz0LQgghhBBCFBMrV67E3d2dwMBAunTpQufOndFqtQwePBhLS0t69+7NiBEj+Omnn3BycmL27Nm5vp9CKytRCCGEEEIIIQyQNCQhhBBCCCGEQdJYEEIY1bVr14wdghBCCCGeQsYsCCEK3YMHD9i1axdbtmzhzz//5PLly8YOSQghhBAGSGNBCCM4f/48vr6+WFhYAHDw4EFOnDiBk5MTH3zwAWXKlDFyhPlPrVZz5MgRtm7dyqFDh8jMzKROnTrMmDHD2KHlq5flu926dSvNmzfX1VMIIUTxJGlIQhSie/fu8f7779OxY0diYmIACA4Opm/fvpw7d47Q0FDatGlDZGSkkSPNP+Hh4UydOpUGDRro6qlWq1m+fDnr1q3jvffeM3aI+eJl+25HjRr1zIV8iqvw8HDGjBlDly5diIuL48cff9RNQyhefBkZGWzfvp0FCxbw4MEDTp06RXx8vLHDyneLFi3i9u3bxg5DvACkZ0EYna+vLwqFIk9lX/R0lXnz5mFhYcGvv/6Km5sbycnJLFiwgICAAIKDgwGYO3cuc+bMYfHixUaO9vl8//33bNmyhYiICDw8PGjbti1vv/02tWrVokaNGpQrV87YIearl+m7BXhZJ9I7evQo/fv3p1mzZvzxxx+oVCru3bvH9OnTmTZtGq1atTJ2iPkuISGBjRs3EhUVxfDhwzl9+jSVKlXC19fX2KHlu+joaLp164aJiQl3796lbdu2rF+/npMnTxIcHEyNGjWMHWK++f7772ndurWxwxAvAGksCKNbuXKl3mutVkvv3r2ZOHFisUnZ+Mfhw4eZO3cubm5uQPaNR0ZGht6qjU2bNmXdunXGCjHfTJ8+HQ8PD2bOnEnz5s0xMSneHZkv03f7j7w28ouTuXPnMmrUKDp27Mgvv/wCwIABAyhVqhRLliwpdo2FCxcu0LVrV6pXr86ZM2fo168fv/32GyNHjmTJkiXUr1/f2CHmq8mTJ9OkSRO++uor/Pz8AJgzZw4TJkxg6tSprF271sgR5p/WrVuzePFievToQbly5bC0tNQ7Xtx/Z4u8k8aCMLo33ngjxz4TExP8/f11N17Fxf379/WeqJ88eRIzMzMCAgJ0+xwdHVGpVMYIL1/NmjWLXbt2MWrUKCZMmEDDhg156623aNiwobFDKxAv03f7jzfffDNP5V70HsHHXb9+Xe87/cebb77JtGnTjBBRwZo+fTpffPEFPXv2pE6dOgBMmDCBkiVLMnv27GLXWDh79iyjR4/WawibmJjw+eefF7un8Pv37ycuLo5t27YZPF6crlvxfKSxIEQhcnV1JSoqCldXVzQaDSEhIdSpUwc7OztdmdDQ0GKRotOyZUtatmxJUlISv/zyCzt37mTo0KGYmpqi0Wg4efIk7u7uxWaA7Mv03f5j7ty5lChRwthhFKoKFSpw7ty5HA8yDh48WOwebgBcunTJYCOodevWfPvtt0aIqGDZ2NgQHx+Pp6en3v6IiAgcHByMFFXBmDlzprFDEC8IaSwIUYjatGnD1KlT6d+/P6dOnSIuLo6RI0fqjp8/f545c+bQoUMHI0aZvxwcHOjQoQMdOnTgr7/+YteuXezatYspU6awaNEiWrVqxZgxY4wd5nN72b5bhULBq6++iouLi7FDKVSDBg1i+PDhXLhwAbVazebNm4mJiWHv3r3F8ubLxcWFyMhI3N3d9faHhYVRunRpI0VVcD788EPGjRvH0KFDAYiMjOTkyZPMmzePTp06GTm6/PXaa68BEBcXx40bN6hduzZKpZKSJUsaOTJR1EhjQYhC1LNnT5KTkxk7diwKhYIBAwbQrFkzAKZMmcLq1atp3LgxX3zxhZEjLRilS5emW7dudOvWjejoaHbs2MHu3buNHVa+eNm+25d1gPNbb73F2rVr+e6776hcuTKHDx/G09OTNWvW8Morrxg7vHzXo0cPxo4dS48ePdBqtRw/fpw7d+7www8/6G6oi5M+ffpgb2/P5MmTSUtLo1evXri4uNCtWze6d+9u7PDyVUpKCqNGjeLXX3/FxMSEvXv3MnXqVO7fv8/ixYtfugcB4ukU2pf1N74oMjZt2pRjX1BQEH379sXZ2Vlv/wcffFBYYRW6K1euoFarqVatmrFDEfmsOH63o0aN4quvvtJLsxLF08GDBwkODiYyMhK1Wo2npyddu3alefPmxg4t323atIkmTZrg7OxMamoqarUae3t7Y4dVIMaNG8eNGzeYPn06LVu2ZPv27Wg0GkaMGIGrqytz5841doiiiJDGgjC6Jk2a5KmcQqHgwIEDBRyNEEI83ahRowzuVygUmJubU6pUKd5++22qVKlSyJGJ/FC3bl02bdpExYoVjR1Kgatfvz7Lly+nWrVq1KlTh+3bt+Pm5kZ4eDiffPIJv/32m7FDFEWEpCEJozt48KCxQxBCiDyxtbXVpRzVrl0brVbLn3/+SWhoKE2bNuXu3bt8++23zJs3j8aNGxs73Oe2aNEig/v/aRyVLl2aBg0aFJuUlYCAALZs2ULPnj2xsbExdjgFKj09HXNz8xz7VSrVS5tmKAyTxoIQQgiRR9HR0fTu3ZsBAwbo7V+2bBlnz55l2bJlbNy4kfnz5xeLxsKNGzfYvXs3ZcuWpUaNGmi1Wi5fvszt27fx8/Pj4cOHTJ48mRUrVlC7dm1jh/vc4uLi+PXXX1m+fDmOjo451h44fPiwcQIrAIGBgcyePZuvv/5aty8qKoqgoCAaNWpkvMBEkSNpSEIIIUQe1a5dm61bt+ZIU4mKiuK9997j/Pnz3L59W7fC84tuyJAh2NjYMGHCBExNTQHQaDRMnToVpVLJ9OnTWbp0KYcPH2b9+vVGjvb5bdmyJdfjbdu2LaRICp5SqWTUqFHs378frVaLjY0NaWlp1K9fn6+//honJydjhyiKCOlZEEIIIfLIzc2NPXv20Lt3b739e/fuxdXVFchuODw5OcOL6uDBg/z888+6hgJkL1L28ccf07ZtW6ZPn06LFi1YunSpEaPMP/80BpRKJdHR0ajVajw8PIrleiJ2dnYsXLiQmzdvcv36dbKysvD09MTLy8vYoYkiRhoLQgghRB6NGDGCPn36EBISQs2aNdFqtVy8eJGLFy+ycOFCLl++zODBg/nss8+MHWq+KFmyJL/99luORcp+//13HB0dAUhISCg2s2KpVCpmzJjBhg0bUKvVaLVazMzMaNGiBUFBQcVmEUnITkPavHkz7u7ueutoxMXF0aZNG06ePGnE6ERRImlIQgghxL8QExPDpk2buHLlCmZmZnh7e9OxY0dcXFyIjo7m5s2bBAYGGjvMfLF9+3ZGjx7Nu+++q2sc/fnnn/zyyy+MHz8ef39/vvjiC5o0afLUmaJeJEFBQYSEhDBu3Djq1KmDRqPh7NmzTJkyhcDAQEaMGGHsEJ/L7t27deMutm/fzjvvvJNjXMbt27eJjo7m6NGjRohQFEXSWBBCCCHy6K+//mLp0qVcvXoVjUajmzVGpVJx48YNwsLCjBxh/gsNDWXdunVERERgamqKt7c3H3/8MbVr1+b8+fOcO3eOjz76SC9V6UX1+uuvs2DBAt3qxv84ffo0X375JcePHzdSZPnj3r17upXGt2zZQrNmzbCystIrY2trS+vWralZs6YxQhRFkKQhCSGEEHk0evRoYmNjefvtt/nuu+/o1q0bMTEx/Prrr4wePdrY4RWIunXrUrduXYPHatWqRa1atQo5ooKj1WoNDux1dHQkNTXVCBHlr9WrVzNq1CgcHBwAZGFFkSfSsyCEEELkUZ06dfjuu++oU6cO7dq1Y/To0fj7+7N8+XJOnz5NcHCwsUPMVykpKaxfv55r166hVquB7BtqlUpFeHg4e/fuNXKE+WvgwIGkp6cza9Ys3crNSUlJDBs2DIVC8cIP5H7llVfYtm0bFStWpGrVqhw/frzYDMYXBUd6FoQQQog80mq1lClTBgBvb28uXbqEv78/zZo1K3YNBYAxY8Zw6tQpAgIC+OWXX2jWrBnR0dFcuHCBfv36GTu8fDdq1Cg+/fRTGjZsqBv0e/PmTSpWrMjixYuNHN3zq169Oh9//DHu7u5otVp69+6NmZnhW8E1a9YUcnSiqJLGghBCCJFH1atXZ+vWrfTp04eqVaty7NgxunTpQkxMjLFDKxBHjx5lwYIFBAQEcPXqVbp27UqNGjWYPn06ERERxg4vXxw9epR69ephYWFB2bJl2blzJyEhIVy/fh1LS0u8vLwICAhAoVAYO9Tntnz5cjZv3kxycjJnzpzBz88PW1tbY4clijhpLAghhBB5NHToUHr16oW1tTVt2rRhxYoVNGvWjLi4OFq3bm3s8PKdSqXSLUBXuXJlLly4QI0aNfjwww/p3LmzcYPLJwMGDGD37t24urrqphMNDAwsNjNaPc7Ozo5PP/0UgFu3btG3b1+DYxaSkpIKOzRRhEljQQghhMijOnXqcPDgQdLS0nBycmLz5s3s378fR0dHmjVrZuzw8p23tzfHjx+nffv2VK5cmdDQUDp16kRSUhIqlcrY4eULZ2dnxo4dS40aNbh16xZLly7F2traYNmBAwcWcnQFZ9q0aXqvNRoNISEhbNmyhUOHDnH+/HkjRSaKGmksCCGEEP+Cra2tLnWjTJkyfPTRR0aOqOD079+fAQMGoNFoaN26Nc2bN+fzzz/n6tWrNGjQwNjh5YvFixezcOFCzpw5g0Kh4Ny5c5ibm+coVxzSkAy5cuUKW7duZceOHSQmJlK2bNkcK5SLl5vMhiSEEEKIp4qNjUWtVuPh4UF4eDjbtm3DycmJLl26PPUJ/IuqSZMmbN682eD0qcXJvXv32LFjB1u3biU8PBxzc3MyMzOZNGkS7dq1w8TExNghiiJEGgtCCCGEyJVGo8HExIT4+HhCQ0OpWrWqbixDcRQWFkZUVBTvvPMOt2/fpmLFilhYWBg7rOe2b98+tmzZQkhICBYWFjRs2JC3336bhg0bUq9ePbZt24a3t7exwxRFjKQhCSGEEMKgc+fOMXDgQGbMmIG3tzft2rVDqVSiUqmYO3cuTZs2NXaI+erevXt88cUXXLt2DZVKxWuvvcbcuXOJiIjgu+++w8PDw9ghPpf+/fvj4eHBtGnTaNas2VOnTRXicdLPJIQQQgiDpk+fTmBgIDVr1mTTpk2YmZlx8uRJxo8fz/z5840dXr6bNGkS5cuX59SpU1haWgIwc+ZMfHx8mDx5spGje36DBw/G0tKS4cOH06ZNG+bMmSMDmcUzSWNBCCGEEAZdunSJ7t27Y2try8GDBwkMDMTS0pKAgABu3rxp7PDy3cmTJ+nfvz9WVla6fXZ2dgwZMoQzZ84YMbL80bNnT7Zv38727dsJDAxkz549dOzYkQYNGqDRaIiMjDR2iKIIksaCEEIIIQxydHTkzp07xMbGcuHCBRo1agTAxYsXKVWqlHGDKwAmJiakpaXl2B8fH6/raSgOKleuzODBg9m3bx/r1q3jnXfewcXFhUGDBtGiRQtWrlxp7BBFESKNBSGEEEIY1K5dO/r27cuHH36Ij48Pb7zxBmvWrGHEiBF06dLF2OHlu5YtWzJ58mTCw8NRKBQolUqOHz/O+PHjad68ubHDKxC1a9dmzJgxhISEsGLFCmrVqsU333yjO65UKo0YnSgKZDYkIYQQQjzVvn37uHXrFu+99x7Ozs4cOXIEjUZD48aNjR1avlOpVMyZM4c1a9aQmZkJgKmpKe3bt2fUqFHFqnchNyqVSjf7k5+fH9u2bcPNzc3IUQljkcaCEEIIIXIVGRlJZGQkarUaT09PfH19jR1SgUpPTycmJga1Wo2bm5tuEb6XUZ06ddi+fbs0Fl5iMmeWEEIIIQx6+PAhI0aM4MiRIzg4OKBWq0lJSaFu3bosWbIEe3t7Y4dYIKysrChdujS7du0iNDSUwMBAXF1djR2WEEYhYxaEEEIIYVBQUBDx8fHs3r2b06dPExoayo4dO0hLS2PatGnGDi9faDQali1bRuvWrWndujXfffcdCQkJtG7dmlmzZjF79mzeffddQkNDjR2qEEYhjQUhhBBCGHTo0CEmTpyIp6enbp+3tzfjxo3jwIEDRows/8yaNYt169YRGBhI+/bt2bZtG507d8bf35/Tp0/z+++/06pVKxYtWmTsUIUwCklDEkIIIYRBj6838DiFQoFarS7kaArGzp07mTlzJvXq1QMgMDCQxo0b8/XXX2Nubg5A9+7dadeunTHDFMJopGdBCCGEEAY1adKESZMmcePGDd2+69evExQUVGxmQ0pISKBixYq6166urlhZWeHs7KzbZ29vb3D9hZeBQqEwdgjCyKRnQQghhBAGDRs2jL59+9K8eXPdjEBKpZJGjRoxduxYI0eXPzQaDWZm+rdDCoVCbpL/JpNmCmksCCGEEMKghw8f8tlnn1GvXj0qVKhAcnIyDRo00HsSXxzcunWL1NRUvX137tzR/f+9e/cKO6RCkZGRwd69e4mKiuKTTz4hPDwcLy8vvdW5Dxw4gKOjoxGjFMYm6ywIIYQQQs/JkyeZNm0aV69e1XuyrFAoqF69OiNHjqRu3bpGjDD/+Pr65uhF+KfO/+zXarUoFAouX75c6PEVlOjoaLp164aJiQl3795lz549zJ49m5MnTxIcHEyNGjWMHaIoIqSxIIQQQgidY8eO0bNnT1q0aEGHDh3w9vbG3t4epVJJeHg4mzdvZs+ePfzwww/UqVPH2OE+t1u3buW5bPny5YHsVCw7O7uCCqlQ9OjRAw8PD7766iv8/PzYvn075cuXZ8KECVy7do21a9caO0RRREhjQQghhBA6nTp1ws/Pj2HDhj21zJQpU4iOjmb58uWFGFnR4efnx7Zt217oVY3r1q3Lxo0b8fT01Ful+ebNm7Ru3ZqzZ88aO0RRRMhsSEIIIYTQCQ8Pp23btrmWad++PZcuXSqkiIqe4vCc1cbGhvj4+Bz7IyIicHBwMEJEoqiSxoIQQgghdNLT0ylRokSuZZycnEhMTCykiERB+PDDDxk3bhz79+8HIDIykp9++olx48bxwQcfGDk6UZTIbEhCCCGE0NFqtZiY5P4sUaYVffH16dMHe3t7Jk+eTFpaGr169cLFxYVu3brRvXt3Y4cnihBpLAghhBBCz86dO3XrKhiiVCoLMRpREDZt2kSLFi3o0qULqampqNVq7O3tjR2WKIKksSCEEEIInXLlyrFq1apnlnN1dS2EaERBmT59OnXr1sXZ2RkbGxtjhyOKMGksCCGEEELn4MGDxg6hyCsRWsHNAAAO3UlEQVQOaVgBAQFs2bKFnj17SmNB5EoaC0IIIYQQ/0JxmA0p7v/t3X1M1eX/x/EXiKBBCVRThMNtburaHCopBzEGtJRs84ZEVwFq6rIlTs2buTDHmKlo2iaEd2uonRmgG9oUDTBQ7EZBpk7MQFGaemJs4Q0JHfj9ofH9EfatvoKfo+f5+I/39fnjxcYfvr2u93XduKHDhw9ry5Yt8vT0lJubW6f1o0ePGhMMdod3FgAAAO67e/euCgsLdfnyZSUmJqq6ulohISF6/vnnO75pbGyUp6fn3w6C27N9+/b91/W/uz4XjoNmAQAAQFJdXZ1mzJghZ2dnXb9+XQcPHtT69et14sQJbd++XS+++KLREbvdrVu3VFdXJ5vNpoCAgL+9NheOh2YBAABA0uzZsxUQEKAVK1Zo+PDhKigokK+vrz766CP99NNP+uKLL4yO2G1aWlq0Zs0a7dmzRzabTe3t7XJxcdFrr72mtLQ0ubq6Gh0RduLx3T8DAADoRpWVlXrzzTc7DTA7OzvrnXfe0fnz5w1M1v3WrFmj0tJSZWVl6YcfftD333+vzZs3q7KyUp988onR8WBHaBYAAAAkPfXUU/rll1+61H/88Uc988wzBiTqOV999ZXS09MVGRkpDw8PPfPMM3r55ZeVlpamgoICo+PBjtAsAAAASJo2bZpSU1P19ddfS5Jqamr05ZdfKjU1VfHx8Qan617t7e3y8vLqUvf09NSdO3cMSAR7xcwCAADAfTt37tT27dt1/fp1SdKzzz6r5ORkzZo167G+/ejPUlJS9NtvvykjI6Pj5eampiZ98MEHcnJy0meffWZwQtgLmgUAAABJeXl5io6Olre3t+7cuSObzdbxD+knzY0bN5SYmCir1Sp/f39J926DCgwMVGZmpgYOHGhwQtgLmgUAAABJI0eOVF5engIDA42O8ki0traqrKxMNTU1cnNzU0hIiMxm8xPxQjW6z5OznwYAAPAQzGaz9u3b5zBn9gsLC9Xe3q7Zs2crMTFRBw4cUGFhodGxYGdcjA4AAABgD27cuKHDhw9ry5Yt8vT0lJubW6f1o0ePGhOsB2RnZ2vbtm1auXJlR83Hx0epqamyWq1KTEw0MB3sCceQAAAAJO3bt++/rk+aNOkRJel5UVFRSk9PV0RERKf6N998o1WrVqm4uNigZLA37CwAAADoP83ArVu3VFdXJ5vNpoCAAPXr18/gZN2vqalJAwYM6FL38/NTY2OjAYlgr5hZAAAAkNTS0qK0tDSNHj1a8fHxmjp1qiIiIrR06VK1tLQYHa9bhYWFadOmTbp9+3ZH7fbt29q8ebNGjBhhYDLYG44hAQAASEpLS1NpaalSU1MVGhqqtrY2VVZWKj09XTExMVq6dKnREbtNfX29Zs2aJavVqoCAAEnSlStX5OPjo8zMzI4aQLMAAAAgafTo0fr000/10ksvdap/9913WrhwoY4fP25Qsp7R0tKi8vJy1dTUqHfv3goICFBkZOQT9fgcHh4zCwAAAJLa29vl5eXVpe7p6flEXqfq6uqqqKgomc1mXbx4Ud7e3jQK6IK/CAAAAN3bWcjIyNDNmzc7ak1NTdqwYYNGjRplYLLus3PnTsXFxam+vl6SdO7cOcXGxio+Pl7R0dFatGjREzefgYfDMSQAAADde2chMTFRVqtV/v7+kqS6ujoFBgYqMzNTAwcONDjhw7FYLFq7dq1mzJihmTNnyt3dXePHj1dzc7O2bdump59+WgsXLtSoUaOUkpJidFzYCZoFAACA+1pbW1VWVqaamhq5ubkpJCREZrNZTk5ORkd7aBMnTlRycrImTpwoSTp58qTeeustLVu2TMnJyZKkY8eOaeXKlSoqKjIwKewJMwsAAAD3FRYWqm/fvpo9e7Ykafny5bp586bGjRtncLKHd+nSJY0cObLj5/Lycjk5OSkqKqqjFhQUJKvVakA62CtmFgAAACRlZ2dr1apVam5u7qj5+PgoNTVVOTk5BibrHn369Ok0qF1eXi4/Pz8FBgZ21K5du/ZEPkKH/x3NAgAAgO6d6d+4caMmTJjQUZs/f77WrVunzz//3Lhg3cRsNmv37t2SpIqKClVVVSkuLq5jva2tTVu3bu20+wBwDAkAAED3bj4aMGBAl7qfn58aGxsNSNS9Fi5cqKSkJI0cOVLNzc164YUXOo5bHThwQNnZ2bJarbJYLAYnhT1hwBkAAEDS3Llz5ebmptWrV8vd3V2SdPv2bX344Yf69ddftX37doMTPry7d+92zCqYzWa5urpKknJzc3XhwgUlJSXJZDIZnBL2hGYBAABAUn19vWbNmiWr1aqAgABJ0pUrV+Tj46PMzMyOmiOZPn26Nm7cqP79+xsdBQbhGBIAAIDuHTfav3+/ysvLVVNTo969eysgIECRkZEO+7JxdXU1j7Q5OJoFAACA+1xdXRUVFSWz2ayLFy/K29vbYRsFQOI2JAAA4OB27typuLg41dfXS5LOnTun2NhYxcfHKzo6WosWLeJ/1+GwaBYAAIDDslgs2rBhg8aNGydPT0+1t7dr0aJFcnJyUkFBgUpKSnTt2jVlZWUZHRUwBM0CAABwWHv27NHKlSs1f/58eXh46NSpU7p8+bJmzJihQYMGacCAAZo3b54KCgqMjgoYgmYBAAA4rEuXLnV6hOyPa0WjoqI6akFBQbJarQakA4xHswAAABxWnz59dOfOnY6fy8vL5efnp8DAwI7atWvX1K9fPwPSAcajWQAAAA7LbDZr9+7dkqSKigpVVVUpLi6uY72trU1bt27ttPvgSNasWaPnnnvO6BgwEI+yAQAAh3X16lUlJSWpqalJzc3NCg4OlsVikYeHhw4cOKDs7GxZrVZZLBYFBwcbHbfbVFRUKD09XbW1tWptbe2yfvbsWQNSwR7RLAAAAId29+7djlkFs9ksV1dXSVJubq4uXLigpKQkmUwmg1N2r1dffVWBgYF644035Obm1mU9MjLSgFSwRzQLAAAA/8L06dO1ceNG9e/f3+go/7PQ0FDl5+c/Ubsl6BnMLAAAAPwL1dXVj/0jbRMmTNChQ4eMjoHHgIvRAQAAAPBozZkzR1OmTFFeXp58fX3l5OTUaT0nJ8egZLA3NAsAAAAOZvHixfLy8lJ0dPQDZxaAP9AsAAAAOJjq6mrt3btXISEhRkeBnWNmAQAAwMGMGDFCFy9eNDoGHgPsLAAAADiY8PBwrVixQoWFhTKZTOrVq1en9ZSUFIOSwd7QLAAAADiY0tJSDR06VA0NDWpoaOi09udhZzg23lkAAAD4Fw4fPqzIyEj17dvX6ChAj6NZAAAAkFRRUaH09HTV1taqtbW1y/rZs2cNSNVzqqurtWvXLtXV1SkjI0NHjhyRv7+/xo4da3Q02BGOIQEAAEhavny5AgMD9e677z7x14mWlZXp/fff1/jx41VVVaWWlhY1Njbq448/1urVq/X6668bHRF2gp0FAAAASaGhocrPz1dwcLDRUXrc5MmTlZCQoISEBIWGhqqgoEAmk0kWi0U5OTk6ePCg0RFhJ7g6FQAAQNKECRN06NAho2M8ErW1tTKbzV3qERER+vnnnw1IBHvFMSQAAABJc+bM0ZQpU5SXlydfX98utwLl5OQYlKz7+fn56fTp0zKZTJ3qxcXFXWpwbDQLAAAAkhYvXiwvLy9FR0c/8TMLCxYs0JIlS3TmzBnZbDbl5+fr6tWrKiws1Lp164yOBzvCzAIAAICkYcOGae/evQoJCTE6yiNRXV2tHTt2qKamRjabTUFBQUpOTtawYcOMjgY7QrMAAAAgaebMmZo6darGjRtndBTAbnAMCQAAQFJ4eLhWrFihwsJCmUwm9erVq9N6SkqKQcm6x6ZNm/7xt4/774ruQ7MAAAAgqbS0VEOHDlVDQ4MaGho6rf152PlxlJWVJWdnZw0ZMkTu7u76q8MlT8Lviu7DMSQAAAAHYLFYVFRUpMrKSoWFhSkmJkYxMTHy9vY2OhrsGM0CAADAfdXV1dq1a5fq6uqUkZGhI0eOyN/fX2PHjjU6Wre5deuWSktLVVRUpGPHjmnQoEGKjY1VbGys/Pz8jI4HO8OjbAAAAJLKyso0bdo02Ww2VVVVqaWlRY2NjZo3b572799vdLxu4+Hhobi4OK1fv17Hjx/X3LlzVVdXp7fffluTJk3S5s2bjY4IO8LOAgAAgKTJkycrISFBCQkJCg0NVUFBgUwmkywWi3JycnTw4EGjI/aItrY2nTp1SkVFRcrNzVVbW5sqKyuNjgU7wYAzAACApNraWpnN5i71iIgIrV692oBEPeePo0glJSUqLS2Vi4uLoqKitHbtWo0ZM8boeLAjNAsAAACS/Pz8dPr0aZlMpk714uLiLrXHUX19vUpKSlRcXKyTJ0/K19dX0dHRyszM1PDhw7kFCQ9EswAAACBpwYIFWrJkic6cOSObzab8/HxdvXpVhYWFWrdundHxHtorr7wiFxcXhYWFadmyZQoODpYktbS06Ntvv+30bXh4uBERYYeYWQAAALivurpaO3bsUE1NjWw2m4KCgpScnKxhw4YZHe2hDR48+B995+TkpPPnz/dwGjwuaBYAAAAAPBDHkAAAgMPatGnTP/42JSWlB5MA9olmAQAAOKysrCw5OztryJAhcnd3118duGD4F46KY0gAAMBhWSwWFRUVqbKyUmFhYYqJiVFMTIy8vb2NjgbYBZoFAADg8P54d6CoqEjHjh3ToEGDFBsbq9jYWPn5+RkdDzAMzQIAAMD/8/vvv+vEiRMqLi7W0aNH5enpqdjYWL333ntGRwMeOZoFAACAP2lra9OpU6dUVFSk3NxctbW1qbKy0uhYwCNHswAAAKD/HEUqKSlRaWmpXFxcFBUVpejoaI0ZM0Zubm5GRwQeOZoFAADgsOrr61VSUqLi4mKdPHlSvr6+io6OVkxMjIYPH84tSHB4NAsAAMBhDRkyRC4uLh03IQUHB//lt+Hh4Y8wGWAfaBYAAIDDGjx48D/6zsnJSefPn+/hNID9oVkAAAAA8EDORgcAAAAAYJ9oFgAAAAA8EM0CAAAAgAeiWQAAAADwQDQLAAAAAB7o/wCh5az6MevT/wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x864 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now we find correlation using heat map\n",
    "# we know that the uppertrange of the correlation table is the same as lower traiange so we get rid of them using mask\n",
    "\n",
    "mask=np.zeros_like(filtered_all_data.corr())\n",
    "traingle_indices=np.triu_indices_from(mask)\n",
    "mask[traingle_indices]= True\n",
    "\n",
    "#one our mask is all set\n",
    "#we find the correlation and plot it into a head map\n",
    "plt.figure(figsize=[12,12])\n",
    "sns.heatmap(filtered_all_data.corr(), mask=mask, annot=True, annot_kws= {\"size\":14})\n",
    "sns.set_style('white')\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skew Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# skew of mean score before distribution\n",
    "# we got a positve skew and most of the score was distributed around 45-50\n",
    "plt.figure(figsize=[12,8])\n",
    "plt.title(f\" Distribution of mean score before drug. skew= ({new_org_data['Mem_Score_Before'].skew()})\", fontsize=16)\n",
    "sns.distplot(new_org_data['Mem_Score_Before'], hist=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# skew of age\n",
    "plt.figure(figsize=[12,8])\n",
    "plt.title(f\" Distribution of mean score before drug. skew= ({new_org_data['age'].skew()})\", fontsize=16)\n",
    "sns.distplot(new_org_data['age'], hist=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Dividing trainining and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a important phase where our data is divided into a training and testing data\n",
    "# It is divided in a ration of 8:2 where 8 is training data and 2 is testing data\n",
    "# we are using the training data to pass in our neural network\n",
    "# the test data will be used to comapre the prediction that we got form our neural network\n",
    "x_train, x_test, y_train, y_test= train_test_split(features,target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparign our training dataset to a array as our neural network doesn't take dataframe\n",
    "x_train=np.array(x_train)\n",
    "y_train=np.array(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.96271972,  0.96271972, -0.6704784 ,  1.36847626, -0.72057669,\n",
       "         0.02358517, -1.23073865,  0.31697849],\n",
       "       [-0.96271972,  0.96271972, -0.6704784 , -0.73073975,  1.38777733,\n",
       "        -0.14579926,  0.0078391 , -0.95376454],\n",
       "       [ 1.03872391, -1.03872391, -0.6704784 ,  1.36847626, -0.72057669,\n",
       "        -0.823337  , -1.23073865,  0.08070968],\n",
       "       [-0.96271972,  0.96271972,  1.49147236, -0.73073975, -0.72057669,\n",
       "         0.36235405, -1.23073865,  1.01939925],\n",
       "       [-0.96271972,  0.96271972, -0.6704784 , -0.73073975,  1.38777733,\n",
       "         1.97150618,  1.24641685,  0.56601858]])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature scaling\n",
    "# Feature scaling is really important as it helps to minimize the gap between data\n",
    "# it also enhances our calculation \n",
    "sc=StandardScaler()\n",
    "x_train=sc.fit_transform(x_train)\n",
    "x_test=sc.fit_transform(x_test)\n",
    "x_train[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Using ANN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Initializing ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiying our model\n",
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aayus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=8, units=128)`\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\aayus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=64)`\n",
      "  import sys\n",
      "C:\\Users\\aayus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=32)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\aayus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"linear\", units=1)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Adding different layers in our ANN\n",
    "# init= where we want to start and it should be uniform. Usually start between 0-1\n",
    "#our first layer\n",
    "model.add(Dense(output_dim=128,activation='relu', input_dim=8))\n",
    "\n",
    "#our second layer\n",
    "model.add(Dense(output_dim=64, activation='relu'))\n",
    "\n",
    "#our third layer\n",
    "model.add(Dense(output_dim=32, activation='relu'))\n",
    "\n",
    "\n",
    "#our output layer\n",
    "model.add(Dense(output_dim=1, activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Compiling and Predicting the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_17 (Dense)             (None, 128)               1152      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 11,521\n",
      "Trainable params: 11,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='mse', metrics=['mae','mse'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Using Tensor Board\n",
    "### Helps us visualize the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensorboard(model_name):\n",
    "    folder_name=f'{model_name} at {strftime(\"%H %M\")}'\n",
    "    \n",
    "    dir_path=os.path.join(LOG_DIR,folder_name)\n",
    "    try:\n",
    "        os.makedirs(dir_path)\n",
    "    except osError as err:\n",
    "        print(err.stererror)\n",
    "    else:\n",
    "        print(\" Successfully created directiory\")\n",
    "    return TensorBoard(log_dir=dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Training the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Successfully created directiory\n",
      "Train on 126 samples, validate on 32 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aayus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.9679 - mae: 0.6440 - mse: 0.9679 - val_loss: 243.8004 - val_mae: 12.1794 - val_mse: 243.8004\n",
      "Epoch 2/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.6896 - mae: 0.5797 - mse: 0.6896 - val_loss: 241.7766 - val_mae: 12.0701 - val_mse: 241.7766\n",
      "Epoch 3/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.5031 - mae: 0.4719 - mse: 0.5031 - val_loss: 248.5820 - val_mae: 12.1685 - val_mse: 248.5820\n",
      "Epoch 4/3500\n",
      "126/126 [==============================] - 0s 166us/step - loss: 0.6888 - mae: 0.4986 - mse: 0.6888 - val_loss: 243.3202 - val_mae: 12.1184 - val_mse: 243.3202\n",
      "Epoch 5/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.6624 - mae: 0.5148 - mse: 0.6624 - val_loss: 243.8953 - val_mae: 12.1468 - val_mse: 243.8953\n",
      "Epoch 6/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.6069 - mae: 0.4872 - mse: 0.6069 - val_loss: 246.3013 - val_mae: 12.1466 - val_mse: 246.3013\n",
      "Epoch 7/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6510 - mae: 0.5549 - mse: 0.6510 - val_loss: 245.6817 - val_mae: 12.1540 - val_mse: 245.6817\n",
      "Epoch 8/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6833 - mae: 0.5839 - mse: 0.6833 - val_loss: 244.0781 - val_mae: 12.1061 - val_mse: 244.0781\n",
      "Epoch 9/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.9172 - mae: 0.6537 - mse: 0.9172 - val_loss: 249.9198 - val_mae: 12.2366 - val_mse: 249.9198\n",
      "Epoch 10/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.9786 - mae: 0.7730 - mse: 0.9786 - val_loss: 252.5419 - val_mae: 12.3473 - val_mse: 252.5419\n",
      "Epoch 11/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.7039 - mae: 0.6154 - mse: 0.7039 - val_loss: 239.8061 - val_mae: 12.0504 - val_mse: 239.8061\n",
      "Epoch 12/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.7942 - mae: 0.6206 - mse: 0.7942 - val_loss: 250.7420 - val_mae: 12.1981 - val_mse: 250.7420\n",
      "Epoch 13/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6400 - mae: 0.5426 - mse: 0.6400 - val_loss: 243.9768 - val_mae: 12.0907 - val_mse: 243.9768\n",
      "Epoch 14/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.7538 - mae: 0.6170 - mse: 0.7538 - val_loss: 246.4448 - val_mae: 12.1139 - val_mse: 246.4448\n",
      "Epoch 15/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 1.0063 - mae: 0.7042 - mse: 1.0063 - val_loss: 252.1624 - val_mae: 12.2838 - val_mse: 252.1624\n",
      "Epoch 16/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.8881 - mae: 0.7162 - mse: 0.8881 - val_loss: 246.4211 - val_mae: 12.2332 - val_mse: 246.4211\n",
      "Epoch 17/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.8442 - mae: 0.6301 - mse: 0.8442 - val_loss: 251.2633 - val_mae: 12.3583 - val_mse: 251.2633\n",
      "Epoch 18/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.7973 - mae: 0.6385 - mse: 0.7973 - val_loss: 241.8053 - val_mae: 12.1114 - val_mse: 241.8053\n",
      "Epoch 19/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.7585 - mae: 0.5639 - mse: 0.7585 - val_loss: 245.5983 - val_mae: 12.1517 - val_mse: 245.5983\n",
      "Epoch 20/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.6730 - mae: 0.5736 - mse: 0.6730 - val_loss: 244.1682 - val_mae: 12.0251 - val_mse: 244.1682\n",
      "Epoch 21/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5724 - mae: 0.4850 - mse: 0.5724 - val_loss: 242.3851 - val_mae: 12.0717 - val_mse: 242.3851\n",
      "Epoch 22/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.6422 - mae: 0.5616 - mse: 0.6422 - val_loss: 248.9394 - val_mae: 12.2153 - val_mse: 248.9394\n",
      "Epoch 23/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.8180 - mae: 0.6857 - mse: 0.8180 - val_loss: 244.5710 - val_mae: 12.1030 - val_mse: 244.5710\n",
      "Epoch 24/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7634 - mae: 0.6480 - mse: 0.7634 - val_loss: 251.3762 - val_mae: 12.2505 - val_mse: 251.3762\n",
      "Epoch 25/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.6440 - mae: 0.5880 - mse: 0.6440 - val_loss: 243.4640 - val_mae: 12.1270 - val_mse: 243.4640\n",
      "Epoch 26/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.6313 - mae: 0.5432 - mse: 0.6313 - val_loss: 252.8064 - val_mae: 12.1940 - val_mse: 252.8064\n",
      "Epoch 27/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6660 - mae: 0.5039 - mse: 0.6660 - val_loss: 246.0906 - val_mae: 12.1909 - val_mse: 246.0906\n",
      "Epoch 28/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6935 - mae: 0.5298 - mse: 0.6935 - val_loss: 249.1850 - val_mae: 12.1350 - val_mse: 249.1850\n",
      "Epoch 29/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6537 - mae: 0.5168 - mse: 0.6537 - val_loss: 245.4342 - val_mae: 12.1146 - val_mse: 245.4342\n",
      "Epoch 30/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.5393 - mae: 0.5131 - mse: 0.5393 - val_loss: 240.5399 - val_mae: 12.0553 - val_mse: 240.5399\n",
      "Epoch 31/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.6108 - mae: 0.5135 - mse: 0.6108 - val_loss: 247.0170 - val_mae: 12.1267 - val_mse: 247.0170\n",
      "Epoch 32/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.4688 - mae: 0.4225 - mse: 0.4688 - val_loss: 244.7847 - val_mae: 12.1271 - val_mse: 244.7847\n",
      "Epoch 33/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.5104 - mae: 0.4597 - mse: 0.5104 - val_loss: 244.1994 - val_mae: 12.0540 - val_mse: 244.1994\n",
      "Epoch 34/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5984 - mae: 0.4746 - mse: 0.5984 - val_loss: 245.8479 - val_mae: 12.0876 - val_mse: 245.8479\n",
      "Epoch 35/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4698 - mae: 0.4135 - mse: 0.4698 - val_loss: 246.5652 - val_mae: 12.1321 - val_mse: 246.5652\n",
      "Epoch 36/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5308 - mae: 0.4800 - mse: 0.5308 - val_loss: 244.4824 - val_mae: 12.0860 - val_mse: 244.4824\n",
      "Epoch 37/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4342 - mae: 0.3920 - mse: 0.4342 - val_loss: 247.6319 - val_mae: 12.1841 - val_mse: 247.6319\n",
      "Epoch 38/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4673 - mae: 0.3910 - mse: 0.4673 - val_loss: 249.7944 - val_mae: 12.2298 - val_mse: 249.7944\n",
      "Epoch 39/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.4406 - mae: 0.3694 - mse: 0.4406 - val_loss: 243.8149 - val_mae: 12.1074 - val_mse: 243.8149\n",
      "Epoch 40/3500\n",
      "126/126 [==============================] - 0s 198us/step - loss: 0.4061 - mae: 0.3712 - mse: 0.4061 - val_loss: 245.3517 - val_mae: 12.1145 - val_mse: 245.3517\n",
      "Epoch 41/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.5343 - mae: 0.4569 - mse: 0.5343 - val_loss: 250.3440 - val_mae: 12.1845 - val_mse: 250.3440\n",
      "Epoch 42/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.6796 - mae: 0.5838 - mse: 0.6796 - val_loss: 242.0114 - val_mae: 12.0921 - val_mse: 242.0114\n",
      "Epoch 43/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.6817 - mae: 0.5606 - mse: 0.6817 - val_loss: 246.7760 - val_mae: 12.1455 - val_mse: 246.7760\n",
      "Epoch 44/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4740 - mae: 0.4366 - mse: 0.4740 - val_loss: 242.9807 - val_mae: 12.0556 - val_mse: 242.9807\n",
      "Epoch 45/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4598 - mae: 0.4609 - mse: 0.4598 - val_loss: 243.9070 - val_mae: 12.0892 - val_mse: 243.9070\n",
      "Epoch 46/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5815 - mae: 0.4817 - mse: 0.5815 - val_loss: 245.9298 - val_mae: 12.1274 - val_mse: 245.9298\n",
      "Epoch 47/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5072 - mae: 0.4065 - mse: 0.5072 - val_loss: 250.9705 - val_mae: 12.2118 - val_mse: 250.9705\n",
      "Epoch 48/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4681 - mae: 0.3707 - mse: 0.4681 - val_loss: 242.1834 - val_mae: 12.1133 - val_mse: 242.1834\n",
      "Epoch 49/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5193 - mae: 0.4290 - mse: 0.5193 - val_loss: 245.2307 - val_mae: 12.0695 - val_mse: 245.2307\n",
      "Epoch 50/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4100 - mae: 0.3747 - mse: 0.4100 - val_loss: 244.0150 - val_mae: 12.0904 - val_mse: 244.0150\n",
      "Epoch 51/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.4585 - mae: 0.3861 - mse: 0.4585 - val_loss: 243.5274 - val_mae: 12.0691 - val_mse: 243.5274\n",
      "Epoch 52/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3920 - mae: 0.3299 - mse: 0.3920 - val_loss: 250.1024 - val_mae: 12.1640 - val_mse: 250.1024\n",
      "Epoch 53/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.3893 - mae: 0.3362 - mse: 0.3893 - val_loss: 243.7531 - val_mae: 12.1151 - val_mse: 243.7531\n",
      "Epoch 54/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3288 - mae: 0.2786 - mse: 0.3288 - val_loss: 247.2611 - val_mae: 12.1552 - val_mse: 247.2611\n",
      "Epoch 55/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.0719 - mae: 0.2180 - mse: 0.071 - 0s 63us/step - loss: 0.3976 - mae: 0.3307 - mse: 0.3976 - val_loss: 245.8512 - val_mae: 12.1258 - val_mse: 245.8512\n",
      "Epoch 56/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3728 - mae: 0.3137 - mse: 0.3728 - val_loss: 244.1647 - val_mae: 12.1144 - val_mse: 244.1647\n",
      "Epoch 57/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3739 - mae: 0.3155 - mse: 0.3739 - val_loss: 250.7138 - val_mae: 12.2105 - val_mse: 250.7138\n",
      "Epoch 58/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4650 - mae: 0.4009 - mse: 0.4650 - val_loss: 244.2379 - val_mae: 12.1153 - val_mse: 244.2379\n",
      "Epoch 59/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3794 - mae: 0.3121 - mse: 0.3794 - val_loss: 244.8202 - val_mae: 12.1209 - val_mse: 244.8202\n",
      "Epoch 60/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3802 - mae: 0.2899 - mse: 0.3802 - val_loss: 247.5766 - val_mae: 12.1474 - val_mse: 247.5766\n",
      "Epoch 61/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3678 - mae: 0.3307 - mse: 0.3678 - val_loss: 246.2465 - val_mae: 12.1502 - val_mse: 246.2465\n",
      "Epoch 62/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.3966 - mae: 0.3700 - mse: 0.3966 - val_loss: 248.1776 - val_mae: 12.1503 - val_mse: 248.1776\n",
      "Epoch 63/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.3249 - mae: 0.2799 - mse: 0.3249 - val_loss: 244.3476 - val_mae: 12.0818 - val_mse: 244.3476\n",
      "Epoch 64/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.3463 - mae: 0.2861 - mse: 0.3463 - val_loss: 248.3913 - val_mae: 12.1699 - val_mse: 248.3913\n",
      "Epoch 65/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3768 - mae: 0.3219 - mse: 0.3768 - val_loss: 243.5607 - val_mae: 12.0852 - val_mse: 243.5607\n",
      "Epoch 66/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.4270 - mae: 0.3723 - mse: 0.4270 - val_loss: 247.5557 - val_mae: 12.1159 - val_mse: 247.5557\n",
      "Epoch 67/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4335 - mae: 0.3789 - mse: 0.4335 - val_loss: 244.6002 - val_mae: 12.1067 - val_mse: 244.6002\n",
      "Epoch 68/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4560 - mae: 0.3782 - mse: 0.4560 - val_loss: 249.4502 - val_mae: 12.1536 - val_mse: 249.4502\n",
      "Epoch 69/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4068 - mae: 0.3471 - mse: 0.4068 - val_loss: 245.8523 - val_mae: 12.1577 - val_mse: 245.8523\n",
      "Epoch 70/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4278 - mae: 0.3790 - mse: 0.4278 - val_loss: 251.4443 - val_mae: 12.2221 - val_mse: 251.4443\n",
      "Epoch 71/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3989 - mae: 0.3535 - mse: 0.3989 - val_loss: 247.7379 - val_mae: 12.1400 - val_mse: 247.7379\n",
      "Epoch 72/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3192 - mae: 0.2867 - mse: 0.3192 - val_loss: 242.5914 - val_mae: 12.0505 - val_mse: 242.5914\n",
      "Epoch 73/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3374 - mae: 0.2810 - mse: 0.3374 - val_loss: 248.0233 - val_mae: 12.1588 - val_mse: 248.0233\n",
      "Epoch 74/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3149 - mae: 0.2504 - mse: 0.3149 - val_loss: 246.7804 - val_mae: 12.1291 - val_mse: 246.7804\n",
      "Epoch 75/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3393 - mae: 0.2724 - mse: 0.3393 - val_loss: 245.4154 - val_mae: 12.0950 - val_mse: 245.4154\n",
      "Epoch 76/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4319 - mae: 0.3805 - mse: 0.4319 - val_loss: 242.4682 - val_mae: 12.0158 - val_mse: 242.4682\n",
      "Epoch 77/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4032 - mae: 0.3466 - mse: 0.4032 - val_loss: 247.6322 - val_mae: 12.1065 - val_mse: 247.6322\n",
      "Epoch 78/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4178 - mae: 0.3655 - mse: 0.4178 - val_loss: 248.4164 - val_mae: 12.1922 - val_mse: 248.4164\n",
      "Epoch 79/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3920 - mae: 0.3081 - mse: 0.3920 - val_loss: 246.9129 - val_mae: 12.1341 - val_mse: 246.9129\n",
      "Epoch 80/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4208 - mae: 0.3604 - mse: 0.4208 - val_loss: 247.0209 - val_mae: 12.1692 - val_mse: 247.0209\n",
      "Epoch 81/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4252 - mae: 0.3767 - mse: 0.4252 - val_loss: 249.0369 - val_mae: 12.1946 - val_mse: 249.0369\n",
      "Epoch 82/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4808 - mae: 0.4143 - mse: 0.4808 - val_loss: 243.1533 - val_mae: 12.0919 - val_mse: 243.1533\n",
      "Epoch 83/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7281 - mae: 0.5432 - mse: 0.7281 - val_loss: 248.2256 - val_mae: 12.1218 - val_mse: 248.2256\n",
      "Epoch 84/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5623 - mae: 0.4685 - mse: 0.5623 - val_loss: 246.5441 - val_mae: 12.1280 - val_mse: 246.5441\n",
      "Epoch 85/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6060 - mae: 0.4975 - mse: 0.6060 - val_loss: 243.7971 - val_mae: 12.0995 - val_mse: 243.7971\n",
      "Epoch 86/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6513 - mae: 0.5311 - mse: 0.6513 - val_loss: 255.5759 - val_mae: 12.2119 - val_mse: 255.5759\n",
      "Epoch 87/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.0715 - mae: 0.7523 - mse: 1.0715 - val_loss: 241.4963 - val_mae: 12.0181 - val_mse: 241.4963\n",
      "Epoch 88/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.3186 - mae: 0.8243 - mse: 1.3186 - val_loss: 246.4393 - val_mae: 12.1643 - val_mse: 246.4393\n",
      "Epoch 89/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.7711 - mae: 0.9996 - mse: 1.7711 - val_loss: 258.7513 - val_mae: 12.3151 - val_mse: 258.7513\n",
      "Epoch 90/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.5054 - mae: 0.9590 - mse: 1.5054 - val_loss: 243.5414 - val_mae: 12.1327 - val_mse: 243.5414\n",
      "Epoch 91/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.1212 - mae: 0.7915 - mse: 1.1212 - val_loss: 258.2275 - val_mae: 12.2957 - val_mse: 258.2275\n",
      "Epoch 92/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.0965 - mae: 0.7644 - mse: 1.0965 - val_loss: 248.2474 - val_mae: 12.1397 - val_mse: 248.2474\n",
      "Epoch 93/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.1986 - mae: 0.7730 - mse: 1.1986 - val_loss: 249.9564 - val_mae: 12.2786 - val_mse: 249.9564\n",
      "Epoch 94/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.5826 - mae: 0.9809 - mse: 1.5826 - val_loss: 250.1188 - val_mae: 12.0661 - val_mse: 250.1188\n",
      "Epoch 95/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 1.7592 - mae: 1.0322 - mse: 1.7592 - val_loss: 232.4477 - val_mae: 11.9261 - val_mse: 232.4477\n",
      "Epoch 96/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.9375 - mae: 1.1011 - mse: 1.9375 - val_loss: 264.3324 - val_mae: 12.5301 - val_mse: 264.3324\n",
      "Epoch 97/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.5774 - mae: 0.9499 - mse: 1.5774 - val_loss: 239.5943 - val_mae: 11.9630 - val_mse: 239.5943\n",
      "Epoch 98/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.8106 - mae: 1.0868 - mse: 1.8106 - val_loss: 245.6462 - val_mae: 12.3582 - val_mse: 245.6462\n",
      "Epoch 99/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 2.2026 - mae: 1.1905 - mse: 2.2026 - val_loss: 254.3727 - val_mae: 12.2582 - val_mse: 254.3727\n",
      "Epoch 100/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 2.1499 - mae: 1.1543 - mse: 2.1499 - val_loss: 246.4870 - val_mae: 12.1619 - val_mse: 246.4870\n",
      "Epoch 101/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.4217 - mae: 0.9438 - mse: 1.4217 - val_loss: 250.4177 - val_mae: 12.3662 - val_mse: 250.4177\n",
      "Epoch 102/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.2266 - mae: 0.8552 - mse: 1.2266 - val_loss: 244.6205 - val_mae: 11.9397 - val_mse: 244.6205\n",
      "Epoch 103/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.4670 - mae: 0.9337 - mse: 1.4670 - val_loss: 237.8543 - val_mae: 11.9608 - val_mse: 237.8543\n",
      "Epoch 104/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.6490 - mae: 0.9842 - mse: 1.6490 - val_loss: 247.1611 - val_mae: 12.0947 - val_mse: 247.1611\n",
      "Epoch 105/3500\n",
      "126/126 [==============================] - 0s 142us/step - loss: 1.5627 - mae: 0.8661 - mse: 1.5627 - val_loss: 246.5647 - val_mae: 12.0839 - val_mse: 246.5647\n",
      "Epoch 106/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.4791 - mae: 0.8936 - mse: 1.4791 - val_loss: 240.9354 - val_mae: 12.2003 - val_mse: 240.9354\n",
      "Epoch 107/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.4080 - mae: 0.9374 - mse: 1.4080 - val_loss: 256.2592 - val_mae: 12.3847 - val_mse: 256.2592\n",
      "Epoch 108/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 2.3279 - mae: 1.2385 - mse: 2.3279 - val_loss: 230.7769 - val_mae: 12.0457 - val_mse: 230.7769\n",
      "Epoch 109/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 2.7581 - mae: 1.3251 - mse: 2.7581 - val_loss: 272.6205 - val_mae: 12.6581 - val_mse: 272.6205\n",
      "Epoch 110/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 2.8027 - mae: 1.2427 - mse: 2.802 - 0s 95us/step - loss: 3.4613 - mae: 1.3175 - mse: 3.4613 - val_loss: 241.4645 - val_mae: 12.0623 - val_mse: 241.4645\n",
      "Epoch 111/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 3.7455 - mae: 1.4616 - mse: 3.7455 - val_loss: 243.7532 - val_mae: 11.9996 - val_mse: 243.7532\n",
      "Epoch 112/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 4.9394 - mae: 1.6625 - mse: 4.9394 - val_loss: 263.6933 - val_mae: 12.5938 - val_mse: 263.6933\n",
      "Epoch 113/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 5.0450 - mae: 1.8581 - mse: 5.0450 - val_loss: 232.5842 - val_mae: 12.1386 - val_mse: 232.5842\n",
      "Epoch 114/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 5.0763 - mae: 1.7891 - mse: 5.0763 - val_loss: 286.9641 - val_mae: 12.7820 - val_mse: 286.9641\n",
      "Epoch 115/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 6.5773 - mae: 1.9895 - mse: 6.5773 - val_loss: 220.4368 - val_mae: 11.9368 - val_mse: 220.4368\n",
      "Epoch 116/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 7.9822 - mae: 2.1937 - mse: 7.9822 - val_loss: 261.5175 - val_mae: 12.3824 - val_mse: 261.5175\n",
      "Epoch 117/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 5.0825 - mae: 1.7991 - mse: 5.0825 - val_loss: 266.2032 - val_mae: 12.3602 - val_mse: 266.2032\n",
      "Epoch 118/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 5.4615 - mae: 1.6505 - mse: 5.4615 - val_loss: 250.1930 - val_mae: 11.9698 - val_mse: 250.1930\n",
      "Epoch 119/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 7.2825 - mae: 2.1465 - mse: 7.2825 - val_loss: 246.4092 - val_mae: 12.0257 - val_mse: 246.4092\n",
      "Epoch 120/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 6.2453 - mae: 2.0326 - mse: 6.2453 - val_loss: 268.0964 - val_mae: 12.6079 - val_mse: 268.0964\n",
      "Epoch 121/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 4.7170 - mae: 1.7789 - mse: 4.7170 - val_loss: 219.1486 - val_mae: 11.8179 - val_mse: 219.1486\n",
      "Epoch 122/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 4.2642 - mae: 1.5852 - mse: 4.2642 - val_loss: 243.6475 - val_mae: 12.0830 - val_mse: 243.6475\n",
      "Epoch 123/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 5.0071 - mae: 1.8308 - mse: 5.0071 - val_loss: 223.7294 - val_mae: 11.7991 - val_mse: 223.7294\n",
      "Epoch 124/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 6.0439 - mae: 2.0004 - mse: 6.0439 - val_loss: 260.6295 - val_mae: 12.5013 - val_mse: 260.6295\n",
      "Epoch 125/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 6.0788 - mae: 1.8344 - mse: 6.0788 - val_loss: 255.9516 - val_mae: 12.3442 - val_mse: 255.9516\n",
      "Epoch 126/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 5.2421 - mae: 1.7487 - mse: 5.2421 - val_loss: 245.9814 - val_mae: 11.9644 - val_mse: 245.9814\n",
      "Epoch 127/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 7.0859 - mae: 2.0720 - mse: 7.0859 - val_loss: 239.5761 - val_mae: 12.1378 - val_mse: 239.5761\n",
      "Epoch 128/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 5.6925 - mae: 1.8932 - mse: 5.6925 - val_loss: 291.0645 - val_mae: 12.6819 - val_mse: 291.0645\n",
      "Epoch 129/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 6.6380 - mae: 1.9241 - mse: 6.6380 - val_loss: 216.5239 - val_mae: 11.6661 - val_mse: 216.5239\n",
      "Epoch 130/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 6.5459 - mae: 2.1078 - mse: 6.5459 - val_loss: 240.9462 - val_mae: 12.1627 - val_mse: 240.9462\n",
      "Epoch 131/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 6.3624 - mae: 1.9958 - mse: 6.3624 - val_loss: 253.4037 - val_mae: 12.3511 - val_mse: 253.4037\n",
      "Epoch 132/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 4.2303 - mae: 1.6343 - mse: 4.2303 - val_loss: 258.1915 - val_mae: 12.3967 - val_mse: 258.1915\n",
      "Epoch 133/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 3.5024 - mae: 1.4151 - mse: 3.5024 - val_loss: 230.6266 - val_mae: 11.8984 - val_mse: 230.6266\n",
      "Epoch 134/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 3.9451 - mae: 1.3350 - mse: 3.9451 - val_loss: 248.9881 - val_mae: 12.1761 - val_mse: 248.9881\n",
      "Epoch 135/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 3.0762 - mae: 1.4043 - mse: 3.0762 - val_loss: 236.6186 - val_mae: 11.9832 - val_mse: 236.6186\n",
      "Epoch 136/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 2.6758 - mae: 1.2897 - mse: 2.6758 - val_loss: 248.5493 - val_mae: 12.2448 - val_mse: 248.5493\n",
      "Epoch 137/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 2.0351 - mae: 1.1254 - mse: 2.0351 - val_loss: 239.8496 - val_mae: 12.0622 - val_mse: 239.8496\n",
      "Epoch 138/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.2640 - mae: 0.8321 - mse: 1.2640 - val_loss: 241.4285 - val_mae: 11.8913 - val_mse: 241.4285\n",
      "Epoch 139/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.9372 - mae: 0.6296 - mse: 0.9372 - val_loss: 237.5085 - val_mae: 11.8847 - val_mse: 237.5085\n",
      "Epoch 140/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.6590 - mae: 0.5416 - mse: 0.6590 - val_loss: 238.1843 - val_mae: 11.9015 - val_mse: 238.1843\n",
      "Epoch 141/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4952 - mae: 0.4364 - mse: 0.4952 - val_loss: 237.7843 - val_mae: 11.7973 - val_mse: 237.7843\n",
      "Epoch 142/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4912 - mae: 0.4137 - mse: 0.4912 - val_loss: 245.0048 - val_mae: 11.8944 - val_mse: 245.0048\n",
      "Epoch 143/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5039 - mae: 0.4446 - mse: 0.5039 - val_loss: 239.7693 - val_mae: 11.8775 - val_mse: 239.7693\n",
      "Epoch 144/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5043 - mae: 0.4577 - mse: 0.5043 - val_loss: 237.9429 - val_mae: 11.8396 - val_mse: 237.9429\n",
      "Epoch 145/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4369 - mae: 0.3927 - mse: 0.4369 - val_loss: 240.2065 - val_mae: 11.8668 - val_mse: 240.2065\n",
      "Epoch 146/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4150 - mae: 0.3604 - mse: 0.4150 - val_loss: 238.9602 - val_mae: 11.8731 - val_mse: 238.9602\n",
      "Epoch 147/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3963 - mae: 0.3420 - mse: 0.3963 - val_loss: 236.6146 - val_mae: 11.8482 - val_mse: 236.6146\n",
      "Epoch 148/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3971 - mae: 0.3250 - mse: 0.3971 - val_loss: 238.6920 - val_mae: 11.8454 - val_mse: 238.6920\n",
      "Epoch 149/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3565 - mae: 0.3106 - mse: 0.3565 - val_loss: 239.4160 - val_mae: 11.8759 - val_mse: 239.4160\n",
      "Epoch 150/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3559 - mae: 0.2940 - mse: 0.3559 - val_loss: 238.7475 - val_mae: 11.8803 - val_mse: 238.7475\n",
      "Epoch 151/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4597 - mae: 0.3521 - mse: 0.4597 - val_loss: 237.9984 - val_mae: 11.8926 - val_mse: 237.9984\n",
      "Epoch 152/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4250 - mae: 0.3836 - mse: 0.4250 - val_loss: 242.4207 - val_mae: 11.9059 - val_mse: 242.4207\n",
      "Epoch 153/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4720 - mae: 0.4279 - mse: 0.4720 - val_loss: 237.5246 - val_mae: 11.8867 - val_mse: 237.5246\n",
      "Epoch 154/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4786 - mae: 0.4173 - mse: 0.4786 - val_loss: 236.9482 - val_mae: 11.8384 - val_mse: 236.9482\n",
      "Epoch 155/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4201 - mae: 0.3985 - mse: 0.4201 - val_loss: 241.0858 - val_mae: 11.8886 - val_mse: 241.0858\n",
      "Epoch 156/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4116 - mae: 0.3548 - mse: 0.4116 - val_loss: 241.2952 - val_mae: 11.9423 - val_mse: 241.2952\n",
      "Epoch 157/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.3666 - mae: 0.3187 - mse: 0.3666 - val_loss: 239.6301 - val_mae: 11.9126 - val_mse: 239.6301\n",
      "Epoch 158/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3868 - mae: 0.3154 - mse: 0.3868 - val_loss: 238.2659 - val_mae: 11.8995 - val_mse: 238.2659\n",
      "Epoch 159/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4561 - mae: 0.3912 - mse: 0.4561 - val_loss: 240.1541 - val_mae: 11.9109 - val_mse: 240.1541\n",
      "Epoch 160/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3784 - mae: 0.2924 - mse: 0.3784 - val_loss: 238.7641 - val_mae: 11.9040 - val_mse: 238.7641\n",
      "Epoch 161/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3532 - mae: 0.2805 - mse: 0.3532 - val_loss: 239.7710 - val_mae: 11.9034 - val_mse: 239.7710\n",
      "Epoch 162/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4209 - mae: 0.3249 - mse: 0.4209 - val_loss: 240.8798 - val_mae: 11.8964 - val_mse: 240.8798\n",
      "Epoch 163/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3751 - mae: 0.3444 - mse: 0.3751 - val_loss: 238.4051 - val_mae: 11.9066 - val_mse: 238.4051\n",
      "Epoch 164/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4164 - mae: 0.3714 - mse: 0.4164 - val_loss: 240.2837 - val_mae: 11.9070 - val_mse: 240.2837\n",
      "Epoch 165/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4191 - mae: 0.3636 - mse: 0.4191 - val_loss: 241.3759 - val_mae: 11.9404 - val_mse: 241.3759\n",
      "Epoch 166/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4869 - mae: 0.4255 - mse: 0.4869 - val_loss: 239.3064 - val_mae: 11.8923 - val_mse: 239.3064\n",
      "Epoch 167/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.6608 - mae: 0.5097 - mse: 0.6608 - val_loss: 238.3513 - val_mae: 11.8820 - val_mse: 238.3513\n",
      "Epoch 168/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.5007 - mae: 0.4550 - mse: 0.5007 - val_loss: 242.6700 - val_mae: 11.9340 - val_mse: 242.6700\n",
      "Epoch 169/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.4215 - mae: 0.3601 - mse: 0.4215 - val_loss: 239.8619 - val_mae: 11.9421 - val_mse: 239.8619\n",
      "Epoch 170/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3635 - mae: 0.3084 - mse: 0.3635 - val_loss: 238.6266 - val_mae: 11.9149 - val_mse: 238.6266\n",
      "Epoch 171/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3978 - mae: 0.3659 - mse: 0.3978 - val_loss: 240.6492 - val_mae: 11.9171 - val_mse: 240.6492\n",
      "Epoch 172/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3588 - mae: 0.3226 - mse: 0.3588 - val_loss: 241.2792 - val_mae: 11.9449 - val_mse: 241.2792\n",
      "Epoch 173/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3455 - mae: 0.2786 - mse: 0.3455 - val_loss: 239.8561 - val_mae: 11.9090 - val_mse: 239.8561\n",
      "Epoch 174/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4319 - mae: 0.3412 - mse: 0.4319 - val_loss: 238.4875 - val_mae: 11.8863 - val_mse: 238.4875\n",
      "Epoch 175/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3632 - mae: 0.3015 - mse: 0.3632 - val_loss: 241.4115 - val_mae: 11.9727 - val_mse: 241.4115\n",
      "Epoch 176/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3604 - mae: 0.2929 - mse: 0.3604 - val_loss: 242.0403 - val_mae: 11.9582 - val_mse: 242.0403\n",
      "Epoch 177/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3330 - mae: 0.2650 - mse: 0.3330 - val_loss: 240.0322 - val_mae: 11.9211 - val_mse: 240.0322\n",
      "Epoch 178/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.3791 - mae: 0.3263 - mse: 0.3791 - val_loss: 238.8123 - val_mae: 11.9115 - val_mse: 238.8123\n",
      "Epoch 179/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.3941 - mae: 0.3305 - mse: 0.3941 - val_loss: 241.9087 - val_mae: 11.9422 - val_mse: 241.9087\n",
      "Epoch 180/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.3964 - mae: 0.3291 - mse: 0.3964 - val_loss: 238.8988 - val_mae: 11.9335 - val_mse: 238.8988\n",
      "Epoch 181/3500\n",
      "126/126 [==============================] - 0s 158us/step - loss: 0.3864 - mae: 0.3105 - mse: 0.3864 - val_loss: 242.6433 - val_mae: 11.9707 - val_mse: 242.6433\n",
      "Epoch 182/3500\n",
      "126/126 [==============================] - 0s 158us/step - loss: 0.3833 - mae: 0.3345 - mse: 0.3833 - val_loss: 240.9906 - val_mae: 11.9181 - val_mse: 240.9906\n",
      "Epoch 183/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.3780 - mae: 0.3205 - mse: 0.3780 - val_loss: 241.3831 - val_mae: 11.9518 - val_mse: 241.3831\n",
      "Epoch 184/3500\n",
      "126/126 [==============================] - 0s 150us/step - loss: 0.4122 - mae: 0.3103 - mse: 0.4122 - val_loss: 241.4990 - val_mae: 11.9569 - val_mse: 241.4990\n",
      "Epoch 185/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.3652 - mae: 0.2709 - mse: 0.3652 - val_loss: 241.1145 - val_mae: 11.9493 - val_mse: 241.1145\n",
      "Epoch 186/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3714 - mae: 0.2899 - mse: 0.3714 - val_loss: 239.7477 - val_mae: 11.9159 - val_mse: 239.7477\n",
      "Epoch 187/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.5418 - mae: 0.3819 - mse: 0.5418 - val_loss: 239.9579 - val_mae: 11.9147 - val_mse: 239.9579\n",
      "Epoch 188/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4633 - mae: 0.3567 - mse: 0.4633 - val_loss: 242.5602 - val_mae: 11.9605 - val_mse: 242.5602\n",
      "Epoch 189/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4330 - mae: 0.3970 - mse: 0.4330 - val_loss: 238.8027 - val_mae: 11.9341 - val_mse: 238.8027\n",
      "Epoch 190/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4265 - mae: 0.3976 - mse: 0.4265 - val_loss: 241.1231 - val_mae: 11.9363 - val_mse: 241.1231\n",
      "Epoch 191/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.4500 - mae: 0.3569 - mse: 0.4500 - val_loss: 241.9697 - val_mae: 11.9561 - val_mse: 241.9697\n",
      "Epoch 192/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3765 - mae: 0.3416 - mse: 0.3765 - val_loss: 238.4194 - val_mae: 11.9125 - val_mse: 238.4194\n",
      "Epoch 193/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.3802 - mae: 0.3285 - mse: 0.3802 - val_loss: 238.7658 - val_mae: 11.8741 - val_mse: 238.7658\n",
      "Epoch 194/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3836 - mae: 0.2986 - mse: 0.3836 - val_loss: 241.2181 - val_mae: 11.9484 - val_mse: 241.2181\n",
      "Epoch 195/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3428 - mae: 0.3169 - mse: 0.3428 - val_loss: 239.9106 - val_mae: 11.9380 - val_mse: 239.9106\n",
      "Epoch 196/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4362 - mae: 0.3442 - mse: 0.4362 - val_loss: 239.0286 - val_mae: 11.9181 - val_mse: 239.0286\n",
      "Epoch 197/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3796 - mae: 0.3151 - mse: 0.3796 - val_loss: 243.3227 - val_mae: 11.9488 - val_mse: 243.3227\n",
      "Epoch 198/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3572 - mae: 0.3145 - mse: 0.3572 - val_loss: 239.4485 - val_mae: 11.9204 - val_mse: 239.4485\n",
      "Epoch 199/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3663 - mae: 0.3061 - mse: 0.3663 - val_loss: 242.1016 - val_mae: 11.9534 - val_mse: 242.1016\n",
      "Epoch 200/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3980 - mae: 0.3548 - mse: 0.3980 - val_loss: 239.1113 - val_mae: 11.9377 - val_mse: 239.1113\n",
      "Epoch 201/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3796 - mae: 0.3288 - mse: 0.3796 - val_loss: 241.7814 - val_mae: 11.9303 - val_mse: 241.7814\n",
      "Epoch 202/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3977 - mae: 0.3429 - mse: 0.3977 - val_loss: 239.5111 - val_mae: 11.9519 - val_mse: 239.5111\n",
      "Epoch 203/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4069 - mae: 0.3546 - mse: 0.4069 - val_loss: 240.6695 - val_mae: 11.9437 - val_mse: 240.6695\n",
      "Epoch 204/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4016 - mae: 0.3487 - mse: 0.4016 - val_loss: 240.8167 - val_mae: 11.8919 - val_mse: 240.8167\n",
      "Epoch 205/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.4231 - mae: 0.3371 - mse: 0.4231 - val_loss: 239.6717 - val_mae: 11.9076 - val_mse: 239.6717\n",
      "Epoch 206/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.3781 - mae: 0.3126 - mse: 0.3781 - val_loss: 242.3021 - val_mae: 11.9268 - val_mse: 242.3021\n",
      "Epoch 207/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.3391 - mae: 0.2905 - mse: 0.3391 - val_loss: 240.1146 - val_mae: 11.9201 - val_mse: 240.1146\n",
      "Epoch 208/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.3690 - mae: 0.3248 - mse: 0.3690 - val_loss: 238.4715 - val_mae: 11.9117 - val_mse: 238.4715\n",
      "Epoch 209/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.3518 - mae: 0.2758 - mse: 0.3518 - val_loss: 243.2584 - val_mae: 11.9540 - val_mse: 243.2584\n",
      "Epoch 210/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3314 - mae: 0.2635 - mse: 0.3314 - val_loss: 240.3929 - val_mae: 11.9589 - val_mse: 240.3929\n",
      "Epoch 211/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.3142 - mae: 0.2552 - mse: 0.3142 - val_loss: 241.9367 - val_mae: 11.9392 - val_mse: 241.9367\n",
      "Epoch 212/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.3339 - mae: 0.2813 - mse: 0.3339 - val_loss: 240.7200 - val_mae: 11.9522 - val_mse: 240.7200\n",
      "Epoch 213/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3322 - mae: 0.2471 - mse: 0.3322 - val_loss: 241.6417 - val_mae: 11.9339 - val_mse: 241.6417\n",
      "Epoch 214/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.3449 - mae: 0.2338 - mse: 0.3449 - val_loss: 240.6071 - val_mae: 11.9450 - val_mse: 240.6071\n",
      "Epoch 215/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.3383 - mae: 0.2512 - mse: 0.3383 - val_loss: 241.0676 - val_mae: 11.9432 - val_mse: 241.0676\n",
      "Epoch 216/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3355 - mae: 0.2428 - mse: 0.3355 - val_loss: 243.3960 - val_mae: 11.9800 - val_mse: 243.3960\n",
      "Epoch 217/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3714 - mae: 0.2779 - mse: 0.3714 - val_loss: 241.8207 - val_mae: 11.9805 - val_mse: 241.8207\n",
      "Epoch 218/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3172 - mae: 0.2285 - mse: 0.3172 - val_loss: 241.6451 - val_mae: 11.9450 - val_mse: 241.6451\n",
      "Epoch 219/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3367 - mae: 0.2467 - mse: 0.3367 - val_loss: 240.3606 - val_mae: 11.9411 - val_mse: 240.3606\n",
      "Epoch 220/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3391 - mae: 0.2676 - mse: 0.3391 - val_loss: 243.3937 - val_mae: 11.9538 - val_mse: 243.3937\n",
      "Epoch 221/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3498 - mae: 0.2903 - mse: 0.3498 - val_loss: 241.2823 - val_mae: 11.9901 - val_mse: 241.2823\n",
      "Epoch 222/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3659 - mae: 0.3242 - mse: 0.3659 - val_loss: 240.7934 - val_mae: 11.9141 - val_mse: 240.7934\n",
      "Epoch 223/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3921 - mae: 0.3174 - mse: 0.3921 - val_loss: 239.9873 - val_mae: 11.9161 - val_mse: 239.9873\n",
      "Epoch 224/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3162 - mae: 0.2701 - mse: 0.3162 - val_loss: 242.2938 - val_mae: 11.9675 - val_mse: 242.2938\n",
      "Epoch 225/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3850 - mae: 0.3443 - mse: 0.3850 - val_loss: 239.7450 - val_mae: 11.9154 - val_mse: 239.7450\n",
      "Epoch 226/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4153 - mae: 0.3847 - mse: 0.4153 - val_loss: 245.2805 - val_mae: 12.0002 - val_mse: 245.2805\n",
      "Epoch 227/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3884 - mae: 0.3671 - mse: 0.3884 - val_loss: 242.5367 - val_mae: 12.0208 - val_mse: 242.5367\n",
      "Epoch 228/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4129 - mae: 0.3395 - mse: 0.4129 - val_loss: 243.1869 - val_mae: 11.9629 - val_mse: 243.1869\n",
      "Epoch 229/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3783 - mae: 0.2918 - mse: 0.3783 - val_loss: 242.4873 - val_mae: 11.9871 - val_mse: 242.4873\n",
      "Epoch 230/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.3552 - mae: 0.2966 - mse: 0.3552 - val_loss: 242.5751 - val_mae: 11.9570 - val_mse: 242.5751\n",
      "Epoch 231/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3927 - mae: 0.3476 - mse: 0.3927 - val_loss: 241.6342 - val_mae: 11.9844 - val_mse: 241.6342\n",
      "Epoch 232/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3536 - mae: 0.2924 - mse: 0.3536 - val_loss: 243.0851 - val_mae: 11.9819 - val_mse: 243.0851\n",
      "Epoch 233/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3547 - mae: 0.2759 - mse: 0.3547 - val_loss: 241.2868 - val_mae: 11.9510 - val_mse: 241.2868\n",
      "Epoch 234/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3299 - mae: 0.2858 - mse: 0.3299 - val_loss: 243.8272 - val_mae: 11.9956 - val_mse: 243.8272\n",
      "Epoch 235/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4073 - mae: 0.3607 - mse: 0.4073 - val_loss: 240.8255 - val_mae: 11.9554 - val_mse: 240.8255\n",
      "Epoch 236/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4011 - mae: 0.3460 - mse: 0.4011 - val_loss: 241.6492 - val_mae: 11.9185 - val_mse: 241.6492\n",
      "Epoch 237/3500\n",
      "126/126 [==============================] - 0s 309us/step - loss: 0.3423 - mae: 0.2938 - mse: 0.3423 - val_loss: 240.3770 - val_mae: 11.9503 - val_mse: 240.3770\n",
      "Epoch 238/3500\n",
      "126/126 [==============================] - 0s 150us/step - loss: 0.4185 - mae: 0.3652 - mse: 0.4185 - val_loss: 241.7333 - val_mae: 11.9201 - val_mse: 241.7333\n",
      "Epoch 239/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.3761 - mae: 0.3489 - mse: 0.3761 - val_loss: 238.4849 - val_mae: 11.9543 - val_mse: 238.4849\n",
      "Epoch 240/3500\n",
      "126/126 [==============================] - 0s 150us/step - loss: 0.4579 - mae: 0.3952 - mse: 0.4579 - val_loss: 244.2239 - val_mae: 11.9719 - val_mse: 244.2239\n",
      "Epoch 241/3500\n",
      "126/126 [==============================] - 0s 253us/step - loss: 0.4707 - mae: 0.4183 - mse: 0.4707 - val_loss: 243.2887 - val_mae: 11.9620 - val_mse: 243.2887\n",
      "Epoch 242/3500\n",
      "126/126 [==============================] - 0s 166us/step - loss: 0.5803 - mae: 0.4657 - mse: 0.5803 - val_loss: 240.8082 - val_mae: 11.9595 - val_mse: 240.8082\n",
      "Epoch 243/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.5785 - mae: 0.4806 - mse: 0.5785 - val_loss: 241.1633 - val_mae: 11.8855 - val_mse: 241.1633\n",
      "Epoch 244/3500\n",
      "126/126 [==============================] - 0s 222us/step - loss: 0.5458 - mae: 0.4536 - mse: 0.5458 - val_loss: 239.9527 - val_mae: 11.9625 - val_mse: 239.9527\n",
      "Epoch 245/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4958 - mae: 0.4388 - mse: 0.4958 - val_loss: 243.2468 - val_mae: 11.9565 - val_mse: 243.2468\n",
      "Epoch 246/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5886 - mae: 0.4882 - mse: 0.5886 - val_loss: 237.3423 - val_mae: 11.8820 - val_mse: 237.3423\n",
      "Epoch 247/3500\n",
      "126/126 [==============================] - 0s 325us/step - loss: 0.4742 - mae: 0.4397 - mse: 0.4742 - val_loss: 243.3170 - val_mae: 11.9889 - val_mse: 243.3170\n",
      "Epoch 248/3500\n",
      "126/126 [==============================] - 0s 285us/step - loss: 0.4863 - mae: 0.4018 - mse: 0.4863 - val_loss: 244.2277 - val_mae: 11.9634 - val_mse: 244.2277\n",
      "Epoch 249/3500\n",
      "126/126 [==============================] - 0s 245us/step - loss: 0.4347 - mae: 0.4155 - mse: 0.4347 - val_loss: 239.5804 - val_mae: 11.9479 - val_mse: 239.5804\n",
      "Epoch 250/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.3791 - mae: 0.3064 - mse: 0.3791 - val_loss: 244.9773 - val_mae: 12.0138 - val_mse: 244.9773\n",
      "Epoch 251/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.3699 - mae: 0.2979 - mse: 0.3699 - val_loss: 242.1535 - val_mae: 11.9623 - val_mse: 242.1535\n",
      "Epoch 252/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.4205 - mae: 0.3526 - mse: 0.4205 - val_loss: 241.9762 - val_mae: 11.9323 - val_mse: 241.9762\n",
      "Epoch 253/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4406 - mae: 0.3906 - mse: 0.4406 - val_loss: 239.0917 - val_mae: 11.9253 - val_mse: 239.0917\n",
      "Epoch 254/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.4821 - mae: 0.4196 - mse: 0.4821 - val_loss: 246.1301 - val_mae: 12.0061 - val_mse: 246.1301\n",
      "Epoch 255/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4045 - mae: 0.3925 - mse: 0.4045 - val_loss: 245.5456 - val_mae: 12.0825 - val_mse: 245.5456\n",
      "Epoch 256/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4425 - mae: 0.3706 - mse: 0.4425 - val_loss: 244.7060 - val_mae: 12.0112 - val_mse: 244.7060\n",
      "Epoch 257/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.4700 - mae: 0.3800 - mse: 0.4700 - val_loss: 240.9725 - val_mae: 11.9532 - val_mse: 240.9725\n",
      "Epoch 258/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4887 - mae: 0.4126 - mse: 0.4887 - val_loss: 245.1460 - val_mae: 12.0396 - val_mse: 245.1460\n",
      "Epoch 259/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4009 - mae: 0.3593 - mse: 0.4009 - val_loss: 241.9784 - val_mae: 11.9622 - val_mse: 241.9784\n",
      "Epoch 260/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.3701 - mae: 0.3426 - mse: 0.3701 - val_loss: 243.1831 - val_mae: 11.9813 - val_mse: 243.1831\n",
      "Epoch 261/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3588 - mae: 0.3145 - mse: 0.3588 - val_loss: 244.3360 - val_mae: 12.0227 - val_mse: 244.3360\n",
      "Epoch 262/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3593 - mae: 0.3252 - mse: 0.3593 - val_loss: 241.7803 - val_mae: 11.9363 - val_mse: 241.7803\n",
      "Epoch 263/3500\n",
      "126/126 [==============================] - 0s 317us/step - loss: 0.3463 - mae: 0.2623 - mse: 0.3463 - val_loss: 240.2619 - val_mae: 11.9135 - val_mse: 240.2619\n",
      "Epoch 264/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.3515 - mae: 0.2804 - mse: 0.3515 - val_loss: 241.9758 - val_mae: 11.9623 - val_mse: 241.9758\n",
      "Epoch 265/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3568 - mae: 0.3228 - mse: 0.3568 - val_loss: 241.4520 - val_mae: 11.9478 - val_mse: 241.4520\n",
      "Epoch 266/3500\n",
      "126/126 [==============================] - 0s 190us/step - loss: 0.4333 - mae: 0.3698 - mse: 0.4333 - val_loss: 244.3417 - val_mae: 11.9894 - val_mse: 244.3417\n",
      "Epoch 267/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4145 - mae: 0.3307 - mse: 0.4145 - val_loss: 237.9016 - val_mae: 11.9052 - val_mse: 237.9016\n",
      "Epoch 268/3500\n",
      "126/126 [==============================] - 0s 150us/step - loss: 0.3292 - mae: 0.3023 - mse: 0.3292 - val_loss: 244.0176 - val_mae: 11.9523 - val_mse: 244.0176\n",
      "Epoch 269/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3676 - mae: 0.3084 - mse: 0.3676 - val_loss: 246.8769 - val_mae: 12.0497 - val_mse: 246.8769\n",
      "Epoch 270/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.3424 - mae: 0.2898 - mse: 0.3424 - val_loss: 242.5392 - val_mae: 11.9833 - val_mse: 242.5392\n",
      "Epoch 271/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4019 - mae: 0.3351 - mse: 0.4019 - val_loss: 240.1276 - val_mae: 11.8978 - val_mse: 240.1276\n",
      "Epoch 272/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.3703 - mae: 0.3021 - mse: 0.3703 - val_loss: 243.8936 - val_mae: 11.9689 - val_mse: 243.8936\n",
      "Epoch 273/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3507 - mae: 0.2866 - mse: 0.3507 - val_loss: 241.8872 - val_mae: 11.9632 - val_mse: 241.8872\n",
      "Epoch 274/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.3941 - mae: 0.3427 - mse: 0.3941 - val_loss: 242.6434 - val_mae: 11.9757 - val_mse: 242.6434\n",
      "Epoch 275/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.1398 - mae: 0.3304 - mse: 0.139 - 0s 135us/step - loss: 0.3909 - mae: 0.3811 - mse: 0.3909 - val_loss: 243.3879 - val_mae: 12.0169 - val_mse: 243.3879\n",
      "Epoch 276/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4664 - mae: 0.4415 - mse: 0.4664 - val_loss: 244.2230 - val_mae: 11.9576 - val_mse: 244.2230\n",
      "Epoch 277/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.4456 - mae: 0.4316 - mse: 0.4456 - val_loss: 236.4115 - val_mae: 11.8944 - val_mse: 236.4115\n",
      "Epoch 278/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4717 - mae: 0.4223 - mse: 0.4717 - val_loss: 243.2535 - val_mae: 11.9177 - val_mse: 243.2535\n",
      "Epoch 279/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.5282 - mae: 0.4628 - mse: 0.5282 - val_loss: 239.5328 - val_mae: 11.9053 - val_mse: 239.5328\n",
      "Epoch 280/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.5161 - mae: 0.4370 - mse: 0.5161 - val_loss: 244.0512 - val_mae: 11.9636 - val_mse: 244.0512\n",
      "Epoch 281/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4665 - mae: 0.4713 - mse: 0.4665 - val_loss: 238.4555 - val_mae: 11.8854 - val_mse: 238.4555\n",
      "Epoch 282/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.6019 - mae: 0.5005 - mse: 0.6019 - val_loss: 242.2061 - val_mae: 11.8971 - val_mse: 242.2061\n",
      "Epoch 283/3500\n",
      "126/126 [==============================] - 0s 158us/step - loss: 0.4075 - mae: 0.3691 - mse: 0.4075 - val_loss: 238.9167 - val_mae: 11.9176 - val_mse: 238.9167\n",
      "Epoch 284/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4433 - mae: 0.3822 - mse: 0.4433 - val_loss: 244.7798 - val_mae: 11.9891 - val_mse: 244.7798\n",
      "Epoch 285/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.4194 - mae: 0.3495 - mse: 0.4194 - val_loss: 244.5743 - val_mae: 11.9733 - val_mse: 244.5743\n",
      "Epoch 286/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.3835 - mae: 0.3324 - mse: 0.3835 - val_loss: 240.2266 - val_mae: 11.9312 - val_mse: 240.2266\n",
      "Epoch 287/3500\n",
      "126/126 [==============================] - 0s 142us/step - loss: 0.3850 - mae: 0.3364 - mse: 0.3850 - val_loss: 244.2821 - val_mae: 11.9337 - val_mse: 244.2821\n",
      "Epoch 288/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.4508 - mae: 0.3258 - mse: 0.4508 - val_loss: 241.1751 - val_mae: 11.9159 - val_mse: 241.1751\n",
      "Epoch 289/3500\n",
      "126/126 [==============================] - 0s 158us/step - loss: 0.3526 - mae: 0.3255 - mse: 0.3526 - val_loss: 242.1696 - val_mae: 11.9772 - val_mse: 242.1696\n",
      "Epoch 290/3500\n",
      "126/126 [==============================] - 0s 142us/step - loss: 0.4105 - mae: 0.3598 - mse: 0.4105 - val_loss: 244.2178 - val_mae: 11.9676 - val_mse: 244.2178\n",
      "Epoch 291/3500\n",
      "126/126 [==============================] - 0s 143us/step - loss: 0.3503 - mae: 0.3257 - mse: 0.3503 - val_loss: 243.8157 - val_mae: 12.0108 - val_mse: 243.8157\n",
      "Epoch 292/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4208 - mae: 0.3751 - mse: 0.4208 - val_loss: 245.0827 - val_mae: 11.9825 - val_mse: 245.0827\n",
      "Epoch 293/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.3947 - mae: 0.3563 - mse: 0.3947 - val_loss: 240.7165 - val_mae: 11.9427 - val_mse: 240.7165\n",
      "Epoch 294/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.3680 - mae: 0.3072 - mse: 0.3680 - val_loss: 248.9383 - val_mae: 12.0676 - val_mse: 248.9383\n",
      "Epoch 295/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4521 - mae: 0.3888 - mse: 0.4521 - val_loss: 242.5725 - val_mae: 11.9930 - val_mse: 242.5725\n",
      "Epoch 296/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4573 - mae: 0.4129 - mse: 0.4573 - val_loss: 246.4905 - val_mae: 11.9951 - val_mse: 246.4905\n",
      "Epoch 297/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.5247 - mae: 0.5020 - mse: 0.5247 - val_loss: 243.2692 - val_mae: 12.0509 - val_mse: 243.2692\n",
      "Epoch 298/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5177 - mae: 0.4500 - mse: 0.5177 - val_loss: 247.3425 - val_mae: 12.0404 - val_mse: 247.3425\n",
      "Epoch 299/3500\n",
      "126/126 [==============================] - 0s 150us/step - loss: 0.4698 - mae: 0.4539 - mse: 0.4698 - val_loss: 241.6971 - val_mae: 11.9514 - val_mse: 241.6971\n",
      "Epoch 300/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3585 - mae: 0.3553 - mse: 0.3585 - val_loss: 244.8068 - val_mae: 11.9896 - val_mse: 244.8068\n",
      "Epoch 301/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4874 - mae: 0.4434 - mse: 0.4874 - val_loss: 243.7781 - val_mae: 12.0110 - val_mse: 243.7781\n",
      "Epoch 302/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4089 - mae: 0.3724 - mse: 0.4089 - val_loss: 248.8711 - val_mae: 12.0501 - val_mse: 248.8711\n",
      "Epoch 303/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5236 - mae: 0.4668 - mse: 0.5236 - val_loss: 245.3481 - val_mae: 12.0837 - val_mse: 245.3481\n",
      "Epoch 304/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5507 - mae: 0.5154 - mse: 0.5507 - val_loss: 246.1815 - val_mae: 12.0212 - val_mse: 246.1815\n",
      "Epoch 305/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4351 - mae: 0.4463 - mse: 0.4351 - val_loss: 240.0591 - val_mae: 11.9836 - val_mse: 240.0591\n",
      "Epoch 306/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5824 - mae: 0.5388 - mse: 0.5824 - val_loss: 251.5081 - val_mae: 12.0650 - val_mse: 251.5081\n",
      "Epoch 307/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4890 - mae: 0.4480 - mse: 0.4890 - val_loss: 236.2728 - val_mae: 11.9033 - val_mse: 236.2728\n",
      "Epoch 308/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.5817 - mae: 0.5020 - mse: 0.5817 - val_loss: 243.9833 - val_mae: 11.9575 - val_mse: 243.9833\n",
      "Epoch 309/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4183 - mae: 0.3700 - mse: 0.4183 - val_loss: 244.5539 - val_mae: 11.9614 - val_mse: 244.5539\n",
      "Epoch 310/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4092 - mae: 0.3472 - mse: 0.4092 - val_loss: 241.6982 - val_mae: 11.9547 - val_mse: 241.6982\n",
      "Epoch 311/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4647 - mae: 0.3952 - mse: 0.4647 - val_loss: 245.9087 - val_mae: 12.0169 - val_mse: 245.9087\n",
      "Epoch 312/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.5382 - mae: 0.4433 - mse: 0.5382 - val_loss: 242.0673 - val_mae: 12.0282 - val_mse: 242.0673\n",
      "Epoch 313/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4124 - mae: 0.3664 - mse: 0.4124 - val_loss: 247.1779 - val_mae: 12.0408 - val_mse: 247.1779\n",
      "Epoch 314/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4328 - mae: 0.4156 - mse: 0.4328 - val_loss: 242.4853 - val_mae: 12.0258 - val_mse: 242.4853\n",
      "Epoch 315/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.5169 - mae: 0.4751 - mse: 0.5169 - val_loss: 247.6759 - val_mae: 12.0251 - val_mse: 247.6759\n",
      "Epoch 316/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4865 - mae: 0.4429 - mse: 0.4865 - val_loss: 239.2756 - val_mae: 11.9515 - val_mse: 239.2756\n",
      "Epoch 317/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4920 - mae: 0.4565 - mse: 0.4920 - val_loss: 242.6632 - val_mae: 11.9374 - val_mse: 242.6632\n",
      "Epoch 318/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.4381 - mae: 0.4504 - mse: 0.4381 - val_loss: 243.0749 - val_mae: 12.0035 - val_mse: 243.0749\n",
      "Epoch 319/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5775 - mae: 0.4843 - mse: 0.5775 - val_loss: 247.4953 - val_mae: 12.0773 - val_mse: 247.4953\n",
      "Epoch 320/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4893 - mae: 0.4582 - mse: 0.4893 - val_loss: 246.7424 - val_mae: 12.0561 - val_mse: 246.7424\n",
      "Epoch 321/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.5800 - mae: 0.4988 - mse: 0.5800 - val_loss: 242.2915 - val_mae: 11.9328 - val_mse: 242.2915\n",
      "Epoch 322/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.4660 - mae: 0.3554 - mse: 0.4660 - val_loss: 239.3253 - val_mae: 11.9057 - val_mse: 239.3253\n",
      "Epoch 323/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5693 - mae: 0.5059 - mse: 0.5693 - val_loss: 242.0554 - val_mae: 11.9964 - val_mse: 242.0554\n",
      "Epoch 324/3500\n",
      "126/126 [==============================] - 0s 293us/step - loss: 0.5773 - mae: 0.4760 - mse: 0.5773 - val_loss: 247.1455 - val_mae: 12.0201 - val_mse: 247.1455\n",
      "Epoch 325/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.5208 - mae: 0.4709 - mse: 0.5208 - val_loss: 241.3609 - val_mae: 11.9980 - val_mse: 241.3609\n",
      "Epoch 326/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5993 - mae: 0.5101 - mse: 0.5993 - val_loss: 249.9178 - val_mae: 12.0784 - val_mse: 249.9178\n",
      "Epoch 327/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.4951 - mae: 0.4584 - mse: 0.4951 - val_loss: 243.8859 - val_mae: 11.9594 - val_mse: 243.8859\n",
      "Epoch 328/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.4533 - mae: 0.4118 - mse: 0.4533 - val_loss: 241.8526 - val_mae: 11.9780 - val_mse: 241.8526\n",
      "Epoch 329/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4616 - mae: 0.4391 - mse: 0.4616 - val_loss: 248.2524 - val_mae: 12.1052 - val_mse: 248.2524\n",
      "Epoch 330/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.5106 - mae: 0.3998 - mse: 0.5106 - val_loss: 246.9520 - val_mae: 12.0213 - val_mse: 246.9520\n",
      "Epoch 331/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5235 - mae: 0.4068 - mse: 0.5235 - val_loss: 241.4126 - val_mae: 11.9872 - val_mse: 241.4126\n",
      "Epoch 332/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4313 - mae: 0.3806 - mse: 0.4313 - val_loss: 246.7940 - val_mae: 12.0318 - val_mse: 246.7940\n",
      "Epoch 333/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.3954 - mae: 0.3403 - mse: 0.3954 - val_loss: 242.7892 - val_mae: 11.9848 - val_mse: 242.7892\n",
      "Epoch 334/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.3853 - mae: 0.3358 - mse: 0.3853 - val_loss: 243.4336 - val_mae: 11.9902 - val_mse: 243.4336\n",
      "Epoch 335/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.3942 - mae: 0.3322 - mse: 0.3942 - val_loss: 243.9848 - val_mae: 11.9923 - val_mse: 243.9848\n",
      "Epoch 336/3500\n",
      "126/126 [==============================] - 0s 253us/step - loss: 0.3467 - mae: 0.2793 - mse: 0.3467 - val_loss: 241.9899 - val_mae: 11.9465 - val_mse: 241.9899\n",
      "Epoch 337/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.3343 - mae: 0.2910 - mse: 0.3343 - val_loss: 245.1947 - val_mae: 12.0080 - val_mse: 245.1947\n",
      "Epoch 338/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.3333 - mae: 0.2839 - mse: 0.3333 - val_loss: 242.4473 - val_mae: 11.9310 - val_mse: 242.4473\n",
      "Epoch 339/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3136 - mae: 0.2482 - mse: 0.3136 - val_loss: 243.1551 - val_mae: 11.9793 - val_mse: 243.1551\n",
      "Epoch 340/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3374 - mae: 0.2320 - mse: 0.3374 - val_loss: 245.0655 - val_mae: 12.0269 - val_mse: 245.0655\n",
      "Epoch 341/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3410 - mae: 0.2758 - mse: 0.3410 - val_loss: 243.9178 - val_mae: 11.9539 - val_mse: 243.9178\n",
      "Epoch 342/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3624 - mae: 0.2924 - mse: 0.3624 - val_loss: 242.1217 - val_mae: 12.0000 - val_mse: 242.1217\n",
      "Epoch 343/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3946 - mae: 0.3749 - mse: 0.3946 - val_loss: 244.8340 - val_mae: 11.9770 - val_mse: 244.8340\n",
      "Epoch 344/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3999 - mae: 0.3547 - mse: 0.3999 - val_loss: 240.3692 - val_mae: 11.9316 - val_mse: 240.3692\n",
      "Epoch 345/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4041 - mae: 0.3541 - mse: 0.4041 - val_loss: 246.0743 - val_mae: 12.0286 - val_mse: 246.0743\n",
      "Epoch 346/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4093 - mae: 0.3663 - mse: 0.4093 - val_loss: 247.5314 - val_mae: 12.0617 - val_mse: 247.5314\n",
      "Epoch 347/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3287 - mae: 0.2977 - mse: 0.3287 - val_loss: 246.3058 - val_mae: 12.0480 - val_mse: 246.3058\n",
      "Epoch 348/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4220 - mae: 0.3675 - mse: 0.4220 - val_loss: 240.7263 - val_mae: 11.9315 - val_mse: 240.7263\n",
      "Epoch 349/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.3850 - mae: 0.3547 - mse: 0.3850 - val_loss: 248.3173 - val_mae: 12.0321 - val_mse: 248.3173\n",
      "Epoch 350/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3653 - mae: 0.3026 - mse: 0.3653 - val_loss: 241.4095 - val_mae: 11.9698 - val_mse: 241.4095\n",
      "Epoch 351/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.3973 - mae: 0.3351 - mse: 0.3973 - val_loss: 242.8087 - val_mae: 11.9349 - val_mse: 242.8087\n",
      "Epoch 352/3500\n",
      "126/126 [==============================] - 0s 158us/step - loss: 0.4379 - mae: 0.3625 - mse: 0.4379 - val_loss: 244.8641 - val_mae: 12.0437 - val_mse: 244.8641\n",
      "Epoch 353/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4911 - mae: 0.4529 - mse: 0.4911 - val_loss: 241.0989 - val_mae: 11.9588 - val_mse: 241.0989\n",
      "Epoch 354/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.4549 - mae: 0.4042 - mse: 0.4549 - val_loss: 248.0537 - val_mae: 11.9998 - val_mse: 248.0537\n",
      "Epoch 355/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3595 - mae: 0.3266 - mse: 0.3595 - val_loss: 242.1298 - val_mae: 12.0180 - val_mse: 242.1298\n",
      "Epoch 356/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.4314 - mae: 0.3972 - mse: 0.4314 - val_loss: 248.0003 - val_mae: 12.0084 - val_mse: 248.0003\n",
      "Epoch 357/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4341 - mae: 0.4067 - mse: 0.4341 - val_loss: 244.1646 - val_mae: 11.9753 - val_mse: 244.1646\n",
      "Epoch 358/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3523 - mae: 0.3366 - mse: 0.3523 - val_loss: 243.5153 - val_mae: 12.0050 - val_mse: 243.5153\n",
      "Epoch 359/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3328 - mae: 0.2782 - mse: 0.3328 - val_loss: 245.6800 - val_mae: 12.0299 - val_mse: 245.6800\n",
      "Epoch 360/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4599 - mae: 0.3533 - mse: 0.4599 - val_loss: 245.4001 - val_mae: 12.0460 - val_mse: 245.4001\n",
      "Epoch 361/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3243 - mae: 0.3084 - mse: 0.3243 - val_loss: 244.1473 - val_mae: 12.0221 - val_mse: 244.1473\n",
      "Epoch 362/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.3727 - mae: 0.3328 - mse: 0.3727 - val_loss: 244.3985 - val_mae: 12.0034 - val_mse: 244.3985\n",
      "Epoch 363/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.3711 - mae: 0.3432 - mse: 0.3711 - val_loss: 245.3157 - val_mae: 12.0169 - val_mse: 245.3157\n",
      "Epoch 364/3500\n",
      "126/126 [==============================] - 0s 214us/step - loss: 0.3104 - mae: 0.2473 - mse: 0.3104 - val_loss: 244.6185 - val_mae: 11.9950 - val_mse: 244.6185\n",
      "Epoch 365/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3738 - mae: 0.2862 - mse: 0.3738 - val_loss: 245.9957 - val_mae: 12.0312 - val_mse: 245.9957\n",
      "Epoch 366/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4043 - mae: 0.3609 - mse: 0.4043 - val_loss: 244.5535 - val_mae: 12.0408 - val_mse: 244.5535\n",
      "Epoch 367/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4053 - mae: 0.3370 - mse: 0.4053 - val_loss: 243.8875 - val_mae: 11.9719 - val_mse: 243.8875\n",
      "Epoch 368/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3689 - mae: 0.3301 - mse: 0.3689 - val_loss: 243.3285 - val_mae: 12.0035 - val_mse: 243.3285\n",
      "Epoch 369/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3982 - mae: 0.3552 - mse: 0.3982 - val_loss: 247.2517 - val_mae: 12.0295 - val_mse: 247.2517\n",
      "Epoch 370/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4786 - mae: 0.3932 - mse: 0.4786 - val_loss: 243.8596 - val_mae: 11.9475 - val_mse: 243.8596\n",
      "Epoch 371/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4210 - mae: 0.3948 - mse: 0.4210 - val_loss: 240.9637 - val_mae: 11.9655 - val_mse: 240.9637\n",
      "Epoch 372/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4001 - mae: 0.3776 - mse: 0.4001 - val_loss: 245.4318 - val_mae: 12.0053 - val_mse: 245.4318\n",
      "Epoch 373/3500\n",
      "126/126 [==============================] - 0s 222us/step - loss: 0.4187 - mae: 0.3600 - mse: 0.4187 - val_loss: 243.5627 - val_mae: 12.0143 - val_mse: 243.5627\n",
      "Epoch 374/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4787 - mae: 0.3511 - mse: 0.4787 - val_loss: 243.5435 - val_mae: 12.0296 - val_mse: 243.5435\n",
      "Epoch 375/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.3989 - mae: 0.3530 - mse: 0.3989 - val_loss: 245.9603 - val_mae: 12.0130 - val_mse: 245.9603\n",
      "Epoch 376/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.4202 - mae: 0.4146 - mse: 0.4202 - val_loss: 243.4525 - val_mae: 11.9653 - val_mse: 243.4525\n",
      "Epoch 377/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5578 - mae: 0.4538 - mse: 0.5578 - val_loss: 241.5421 - val_mae: 11.9160 - val_mse: 241.5421\n",
      "Epoch 378/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.6985 - mae: 0.5303 - mse: 0.6985 - val_loss: 242.9749 - val_mae: 11.9710 - val_mse: 242.9749\n",
      "Epoch 379/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.5813 - mae: 0.5100 - mse: 0.5813 - val_loss: 241.6328 - val_mae: 11.9562 - val_mse: 241.6328\n",
      "Epoch 380/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.7388 - mae: 0.5807 - mse: 0.7388 - val_loss: 247.8028 - val_mae: 12.1001 - val_mse: 247.8028\n",
      "Epoch 381/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.5335 - mae: 0.5225 - mse: 0.5335 - val_loss: 243.4124 - val_mae: 11.9952 - val_mse: 243.4124\n",
      "Epoch 382/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.6099 - mae: 0.5010 - mse: 0.6099 - val_loss: 241.7348 - val_mae: 11.9604 - val_mse: 241.7348\n",
      "Epoch 383/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6407 - mae: 0.4905 - mse: 0.6407 - val_loss: 252.6172 - val_mae: 12.1348 - val_mse: 252.6172\n",
      "Epoch 384/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.4886 - mae: 0.4678 - mse: 0.4886 - val_loss: 240.5659 - val_mae: 11.9504 - val_mse: 240.5659\n",
      "Epoch 385/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.5862 - mae: 0.5061 - mse: 0.5862 - val_loss: 247.2674 - val_mae: 12.0921 - val_mse: 247.2674\n",
      "Epoch 386/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.6073 - mae: 0.4963 - mse: 0.6073 - val_loss: 249.0851 - val_mae: 12.0081 - val_mse: 249.0851\n",
      "Epoch 387/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4300 - mae: 0.3982 - mse: 0.4300 - val_loss: 236.8756 - val_mae: 11.8853 - val_mse: 236.8756\n",
      "Epoch 388/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4813 - mae: 0.4396 - mse: 0.4813 - val_loss: 250.2597 - val_mae: 12.0691 - val_mse: 250.2597\n",
      "Epoch 389/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5356 - mae: 0.4352 - mse: 0.5356 - val_loss: 240.3650 - val_mae: 11.9653 - val_mse: 240.3650\n",
      "Epoch 390/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.6269 - mae: 0.5387 - mse: 0.6269 - val_loss: 247.1933 - val_mae: 12.0915 - val_mse: 247.1933\n",
      "Epoch 391/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5101 - mae: 0.4845 - mse: 0.5101 - val_loss: 248.2443 - val_mae: 12.0318 - val_mse: 248.2443\n",
      "Epoch 392/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4858 - mae: 0.4372 - mse: 0.4858 - val_loss: 239.8655 - val_mae: 11.8518 - val_mse: 239.8655\n",
      "Epoch 393/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.8151 - mae: 0.5389 - mse: 0.8151 - val_loss: 250.3988 - val_mae: 12.0672 - val_mse: 250.3988\n",
      "Epoch 394/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.9794 - mae: 0.6826 - mse: 0.9794 - val_loss: 240.4041 - val_mae: 11.9155 - val_mse: 240.4041\n",
      "Epoch 395/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6360 - mae: 0.5688 - mse: 0.6360 - val_loss: 241.7615 - val_mae: 11.9783 - val_mse: 241.7615\n",
      "Epoch 396/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5446 - mae: 0.5032 - mse: 0.5446 - val_loss: 245.3114 - val_mae: 12.0815 - val_mse: 245.3114\n",
      "Epoch 397/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6073 - mae: 0.4751 - mse: 0.6073 - val_loss: 244.3799 - val_mae: 11.9145 - val_mse: 244.3799\n",
      "Epoch 398/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.4717 - mae: 0.4153 - mse: 0.4717 - val_loss: 244.1139 - val_mae: 12.0412 - val_mse: 244.1139\n",
      "Epoch 399/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.5830 - mae: 0.4732 - mse: 0.5830 - val_loss: 252.2282 - val_mae: 12.1051 - val_mse: 252.2282\n",
      "Epoch 400/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.5226 - mae: 0.4529 - mse: 0.5226 - val_loss: 246.2948 - val_mae: 12.0559 - val_mse: 246.2948\n",
      "Epoch 401/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.4146 - mae: 0.3719 - mse: 0.4146 - val_loss: 244.5380 - val_mae: 11.9913 - val_mse: 244.5380\n",
      "Epoch 402/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.5014 - mae: 0.4781 - mse: 0.5014 - val_loss: 240.9803 - val_mae: 11.9243 - val_mse: 240.9803\n",
      "Epoch 403/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.5793 - mae: 0.5330 - mse: 0.5793 - val_loss: 249.0421 - val_mae: 12.0628 - val_mse: 249.0421\n",
      "Epoch 404/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.6558 - mae: 0.5433 - mse: 0.6558 - val_loss: 242.6827 - val_mae: 11.9926 - val_mse: 242.6827\n",
      "Epoch 405/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5866 - mae: 0.5203 - mse: 0.5866 - val_loss: 248.3722 - val_mae: 11.9645 - val_mse: 248.3722\n",
      "Epoch 406/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.6900 - mae: 0.5655 - mse: 0.6900 - val_loss: 245.6483 - val_mae: 12.0486 - val_mse: 245.6483\n",
      "Epoch 407/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.7746 - mae: 0.6022 - mse: 0.7746 - val_loss: 244.2586 - val_mae: 11.9845 - val_mse: 244.2586\n",
      "Epoch 408/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.9190 - mae: 0.6556 - mse: 0.9190 - val_loss: 247.1447 - val_mae: 11.9386 - val_mse: 247.1447\n",
      "Epoch 409/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 1.0812 - mae: 0.7752 - mse: 1.0812 - val_loss: 240.1195 - val_mae: 12.0336 - val_mse: 240.1195\n",
      "Epoch 410/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.9521 - mae: 0.7228 - mse: 0.9521 - val_loss: 252.1297 - val_mae: 12.0277 - val_mse: 252.1297\n",
      "Epoch 411/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 1.2308 - mae: 0.8285 - mse: 1.2308 - val_loss: 249.0598 - val_mae: 12.1194 - val_mse: 249.0598\n",
      "Epoch 412/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.9889 - mae: 0.7411 - mse: 0.9889 - val_loss: 243.4055 - val_mae: 12.0462 - val_mse: 243.4055\n",
      "Epoch 413/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.0425 - mae: 0.7538 - mse: 1.0425 - val_loss: 245.8236 - val_mae: 12.0224 - val_mse: 245.8236\n",
      "Epoch 414/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 1.1348 - mae: 0.8128 - mse: 1.1348 - val_loss: 240.3577 - val_mae: 11.9765 - val_mse: 240.3577\n",
      "Epoch 415/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.7319 - mae: 0.5681 - mse: 0.731 - 0s 182us/step - loss: 1.3008 - mae: 0.8212 - mse: 1.3008 - val_loss: 251.9756 - val_mae: 12.0867 - val_mse: 251.9756\n",
      "Epoch 416/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 1.0612 - mae: 0.7602 - mse: 1.0612 - val_loss: 242.0034 - val_mae: 11.8993 - val_mse: 242.0034\n",
      "Epoch 417/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.2201 - mae: 0.8124 - mse: 1.2201 - val_loss: 241.2366 - val_mae: 11.9177 - val_mse: 241.2366\n",
      "Epoch 418/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.9245 - mae: 0.7315 - mse: 0.9245 - val_loss: 250.6894 - val_mae: 12.1324 - val_mse: 250.6894\n",
      "Epoch 419/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.9717 - mae: 0.7711 - mse: 0.9717 - val_loss: 241.7904 - val_mae: 11.9671 - val_mse: 241.7904\n",
      "Epoch 420/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 1.0511 - mae: 0.7486 - mse: 1.0511 - val_loss: 243.7029 - val_mae: 12.0285 - val_mse: 243.7029\n",
      "Epoch 421/3500\n",
      "126/126 [==============================] - 0s 190us/step - loss: 0.8761 - mae: 0.6507 - mse: 0.8761 - val_loss: 244.4123 - val_mae: 11.8824 - val_mse: 244.4123\n",
      "Epoch 422/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.8574 - mae: 0.6494 - mse: 0.8574 - val_loss: 240.1974 - val_mae: 11.9429 - val_mse: 240.1974\n",
      "Epoch 423/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.6580 - mae: 0.5958 - mse: 0.6580 - val_loss: 254.1701 - val_mae: 12.1381 - val_mse: 254.1701\n",
      "Epoch 424/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.6502 - mae: 0.5615 - mse: 0.6502 - val_loss: 246.5376 - val_mae: 12.1269 - val_mse: 246.5376\n",
      "Epoch 425/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.6649 - mae: 0.5530 - mse: 0.6649 - val_loss: 246.1538 - val_mae: 12.0614 - val_mse: 246.1538\n",
      "Epoch 426/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.5469 - mae: 0.4950 - mse: 0.5469 - val_loss: 239.4990 - val_mae: 11.9396 - val_mse: 239.4990\n",
      "Epoch 427/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4945 - mae: 0.4701 - mse: 0.4945 - val_loss: 249.9075 - val_mae: 12.0908 - val_mse: 249.9075\n",
      "Epoch 428/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4961 - mae: 0.4258 - mse: 0.4961 - val_loss: 240.2973 - val_mae: 11.9853 - val_mse: 240.2973\n",
      "Epoch 429/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4260 - mae: 0.4061 - mse: 0.4260 - val_loss: 245.3687 - val_mae: 11.9982 - val_mse: 245.3687\n",
      "Epoch 430/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4320 - mae: 0.4311 - mse: 0.4320 - val_loss: 244.1711 - val_mae: 11.9991 - val_mse: 244.1711\n",
      "Epoch 431/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.4703 - mae: 0.4229 - mse: 0.4703 - val_loss: 241.8055 - val_mae: 12.0205 - val_mse: 241.8055\n",
      "Epoch 432/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.4744 - mae: 0.4503 - mse: 0.4744 - val_loss: 248.0633 - val_mae: 12.0833 - val_mse: 248.0633\n",
      "Epoch 433/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.4420 - mae: 0.4018 - mse: 0.4420 - val_loss: 244.3145 - val_mae: 12.0278 - val_mse: 244.3145\n",
      "Epoch 434/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4286 - mae: 0.4224 - mse: 0.4286 - val_loss: 247.1093 - val_mae: 12.0380 - val_mse: 247.1093\n",
      "Epoch 435/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.6434 - mae: 0.5605 - mse: 0.6434 - val_loss: 240.4917 - val_mae: 11.8683 - val_mse: 240.4917\n",
      "Epoch 436/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.7011 - mae: 0.6203 - mse: 0.7011 - val_loss: 245.8239 - val_mae: 12.0172 - val_mse: 245.8239\n",
      "Epoch 437/3500\n",
      "126/126 [==============================] - 0s 142us/step - loss: 0.6911 - mae: 0.5823 - mse: 0.6911 - val_loss: 249.4412 - val_mae: 12.1727 - val_mse: 249.4412\n",
      "Epoch 438/3500\n",
      "126/126 [==============================] - 0s 435us/step - loss: 0.5313 - mae: 0.4664 - mse: 0.5313 - val_loss: 245.3632 - val_mae: 12.0196 - val_mse: 245.3632\n",
      "Epoch 439/3500\n",
      "126/126 [==============================] - 0s 222us/step - loss: 0.5585 - mae: 0.4723 - mse: 0.5585 - val_loss: 242.4663 - val_mae: 12.0005 - val_mse: 242.4663\n",
      "Epoch 440/3500\n",
      "126/126 [==============================] - 0s 150us/step - loss: 0.5709 - mae: 0.5087 - mse: 0.5709 - val_loss: 247.9653 - val_mae: 12.0707 - val_mse: 247.9653\n",
      "Epoch 441/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.5031 - mae: 0.4730 - mse: 0.5031 - val_loss: 243.3835 - val_mae: 12.0072 - val_mse: 243.3835\n",
      "Epoch 442/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.4269 - mae: 0.3774 - mse: 0.4269 - val_loss: 249.4351 - val_mae: 12.1057 - val_mse: 249.4351\n",
      "Epoch 443/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4176 - mae: 0.3771 - mse: 0.4176 - val_loss: 244.9467 - val_mae: 12.0173 - val_mse: 244.9467\n",
      "Epoch 444/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4547 - mae: 0.4057 - mse: 0.4547 - val_loss: 243.9934 - val_mae: 12.0075 - val_mse: 243.9934\n",
      "Epoch 445/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.3712 - mae: 0.3569 - mse: 0.3712 - val_loss: 241.7188 - val_mae: 11.9749 - val_mse: 241.7188\n",
      "Epoch 446/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.1858 - mae: 0.2852 - mse: 0.185 - 0s 111us/step - loss: 0.4306 - mae: 0.3914 - mse: 0.4306 - val_loss: 245.5742 - val_mae: 12.0853 - val_mse: 245.5742\n",
      "Epoch 447/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3980 - mae: 0.3548 - mse: 0.3980 - val_loss: 248.1668 - val_mae: 12.0643 - val_mse: 248.1668\n",
      "Epoch 448/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3593 - mae: 0.3223 - mse: 0.3593 - val_loss: 241.3402 - val_mae: 11.9424 - val_mse: 241.3402\n",
      "Epoch 449/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3466 - mae: 0.3189 - mse: 0.3466 - val_loss: 246.0661 - val_mae: 12.0173 - val_mse: 246.0661\n",
      "Epoch 450/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3993 - mae: 0.3665 - mse: 0.3993 - val_loss: 247.4597 - val_mae: 12.0906 - val_mse: 247.4597\n",
      "Epoch 451/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4160 - mae: 0.3859 - mse: 0.4160 - val_loss: 244.2144 - val_mae: 11.9955 - val_mse: 244.2144\n",
      "Epoch 452/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4417 - mae: 0.3859 - mse: 0.4417 - val_loss: 244.5648 - val_mae: 11.9761 - val_mse: 244.5648\n",
      "Epoch 453/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4026 - mae: 0.3701 - mse: 0.4026 - val_loss: 244.4186 - val_mae: 11.9908 - val_mse: 244.4186\n",
      "Epoch 454/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4485 - mae: 0.3553 - mse: 0.4485 - val_loss: 244.8817 - val_mae: 12.0127 - val_mse: 244.8817\n",
      "Epoch 455/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.3811 - mae: 0.3271 - mse: 0.3811 - val_loss: 244.6803 - val_mae: 12.0474 - val_mse: 244.6803\n",
      "Epoch 456/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4488 - mae: 0.3730 - mse: 0.4488 - val_loss: 243.7478 - val_mae: 11.9600 - val_mse: 243.7478\n",
      "Epoch 457/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4722 - mae: 0.4507 - mse: 0.4722 - val_loss: 243.2285 - val_mae: 11.9928 - val_mse: 243.2285\n",
      "Epoch 458/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4399 - mae: 0.4153 - mse: 0.4399 - val_loss: 247.7326 - val_mae: 12.0490 - val_mse: 247.7326\n",
      "Epoch 459/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3916 - mae: 0.3277 - mse: 0.3916 - val_loss: 239.7207 - val_mae: 11.8903 - val_mse: 239.7207\n",
      "Epoch 460/3500\n",
      "126/126 [==============================] - 0s 190us/step - loss: 0.4939 - mae: 0.3748 - mse: 0.4939 - val_loss: 245.7484 - val_mae: 12.0097 - val_mse: 245.7484\n",
      "Epoch 461/3500\n",
      "126/126 [==============================] - 0s 214us/step - loss: 0.6564 - mae: 0.4891 - mse: 0.6564 - val_loss: 245.3616 - val_mae: 11.9764 - val_mse: 245.3616\n",
      "Epoch 462/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.8696 - mae: 0.6586 - mse: 0.8696 - val_loss: 243.2701 - val_mae: 12.0245 - val_mse: 243.2701\n",
      "Epoch 463/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 1.0825 - mae: 0.7151 - mse: 1.0825 - val_loss: 251.4382 - val_mae: 12.1156 - val_mse: 251.4382\n",
      "Epoch 464/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.9961 - mae: 0.7575 - mse: 0.9961 - val_loss: 249.4704 - val_mae: 12.0574 - val_mse: 249.4704\n",
      "Epoch 465/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.8149 - mae: 0.6661 - mse: 0.8149 - val_loss: 244.5974 - val_mae: 12.0259 - val_mse: 244.5974\n",
      "Epoch 466/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.7558 - mae: 0.6119 - mse: 0.7558 - val_loss: 242.3786 - val_mae: 11.9540 - val_mse: 242.3786\n",
      "Epoch 467/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.6868 - mae: 0.5951 - mse: 0.6868 - val_loss: 247.9202 - val_mae: 12.0278 - val_mse: 247.9202\n",
      "Epoch 468/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.5976 - mae: 0.5336 - mse: 0.5976 - val_loss: 247.3521 - val_mae: 12.0985 - val_mse: 247.3521\n",
      "Epoch 469/3500\n",
      "126/126 [==============================] - 0s 150us/step - loss: 0.7427 - mae: 0.5962 - mse: 0.7427 - val_loss: 244.9666 - val_mae: 11.9901 - val_mse: 244.9666\n",
      "Epoch 470/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.7545 - mae: 0.6201 - mse: 0.7545 - val_loss: 242.3597 - val_mae: 11.9289 - val_mse: 242.3597\n",
      "Epoch 471/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 1.0313 - mae: 0.7469 - mse: 1.0313 - val_loss: 239.9988 - val_mae: 11.9597 - val_mse: 239.9988\n",
      "Epoch 472/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 1.0877 - mae: 0.7636 - mse: 1.0877 - val_loss: 250.8468 - val_mae: 12.0230 - val_mse: 250.8468\n",
      "Epoch 473/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.8840 - mae: 0.7089 - mse: 0.8840 - val_loss: 238.8908 - val_mae: 11.9786 - val_mse: 238.8908\n",
      "Epoch 474/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.8628 - mae: 0.7100 - mse: 0.8628 - val_loss: 244.8978 - val_mae: 11.9547 - val_mse: 244.8978\n",
      "Epoch 475/3500\n",
      "126/126 [==============================] - 0s 356us/step - loss: 0.6831 - mae: 0.5688 - mse: 0.6831 - val_loss: 241.2902 - val_mae: 11.9471 - val_mse: 241.2902\n",
      "Epoch 476/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.7316 - mae: 0.5248 - mse: 0.731 - 0s 285us/step - loss: 0.6789 - mae: 0.5549 - mse: 0.6789 - val_loss: 244.7278 - val_mae: 12.0405 - val_mse: 244.7278\n",
      "Epoch 477/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.6399 - mae: 0.5156 - mse: 0.6399 - val_loss: 251.8396 - val_mae: 12.1099 - val_mse: 251.8396\n",
      "Epoch 478/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.6047 - mae: 0.5162 - mse: 0.6047 - val_loss: 241.0190 - val_mae: 11.9965 - val_mse: 241.0190\n",
      "Epoch 479/3500\n",
      "126/126 [==============================] - 0s 190us/step - loss: 0.6799 - mae: 0.5795 - mse: 0.6799 - val_loss: 249.4037 - val_mae: 12.1020 - val_mse: 249.4037\n",
      "Epoch 480/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.7973 - mae: 0.6024 - mse: 0.7973 - val_loss: 247.6803 - val_mae: 12.0992 - val_mse: 247.6803\n",
      "Epoch 481/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.8400 - mae: 0.6167 - mse: 0.8400 - val_loss: 237.4223 - val_mae: 11.9244 - val_mse: 237.4223\n",
      "Epoch 482/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.5969 - mae: 0.5143 - mse: 0.5969 - val_loss: 249.6483 - val_mae: 12.0847 - val_mse: 249.6483\n",
      "Epoch 483/3500\n",
      "126/126 [==============================] - 0s 142us/step - loss: 0.5240 - mae: 0.4675 - mse: 0.5240 - val_loss: 240.3680 - val_mae: 11.9730 - val_mse: 240.3680\n",
      "Epoch 484/3500\n",
      "126/126 [==============================] - 0s 198us/step - loss: 0.4621 - mae: 0.4338 - mse: 0.4621 - val_loss: 247.4191 - val_mae: 12.0258 - val_mse: 247.4191\n",
      "Epoch 485/3500\n",
      "126/126 [==============================] - 0s 166us/step - loss: 0.5074 - mae: 0.4732 - mse: 0.5074 - val_loss: 245.6010 - val_mae: 12.0517 - val_mse: 245.6010\n",
      "Epoch 486/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.6027 - mae: 0.4944 - mse: 0.6027 - val_loss: 245.7189 - val_mae: 12.0868 - val_mse: 245.7189\n",
      "Epoch 487/3500\n",
      "126/126 [==============================] - 0s 245us/step - loss: 0.4276 - mae: 0.4004 - mse: 0.4276 - val_loss: 246.8155 - val_mae: 12.0718 - val_mse: 246.8155\n",
      "Epoch 488/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4458 - mae: 0.4124 - mse: 0.4458 - val_loss: 243.9101 - val_mae: 12.0033 - val_mse: 243.9101\n",
      "Epoch 489/3500\n",
      "126/126 [==============================] - 0s 158us/step - loss: 0.4709 - mae: 0.4326 - mse: 0.4709 - val_loss: 252.1718 - val_mae: 12.1029 - val_mse: 252.1718\n",
      "Epoch 490/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.6660 - mae: 0.5202 - mse: 0.666 - 0s 127us/step - loss: 0.5379 - mae: 0.4768 - mse: 0.5379 - val_loss: 238.9606 - val_mae: 11.9381 - val_mse: 238.9606\n",
      "Epoch 491/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6645 - mae: 0.5690 - mse: 0.6645 - val_loss: 243.8526 - val_mae: 11.9932 - val_mse: 243.8526\n",
      "Epoch 492/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.5618 - mae: 0.4806 - mse: 0.5618 - val_loss: 244.7864 - val_mae: 11.9875 - val_mse: 244.7864\n",
      "Epoch 493/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.5877 - mae: 0.5063 - mse: 0.5877 - val_loss: 240.5229 - val_mae: 12.0071 - val_mse: 240.5229\n",
      "Epoch 494/3500\n",
      "126/126 [==============================] - 0s 182us/step - loss: 0.5099 - mae: 0.4663 - mse: 0.5099 - val_loss: 248.5281 - val_mae: 12.0161 - val_mse: 248.5281\n",
      "Epoch 495/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6306 - mae: 0.5443 - mse: 0.6306 - val_loss: 243.5240 - val_mae: 12.0175 - val_mse: 243.5240\n",
      "Epoch 496/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.6641 - mae: 0.6256 - mse: 0.6641 - val_loss: 248.5101 - val_mae: 12.0564 - val_mse: 248.5101\n",
      "Epoch 497/3500\n",
      "126/126 [==============================] - 0s 142us/step - loss: 0.9629 - mae: 0.6871 - mse: 0.9629 - val_loss: 246.0567 - val_mae: 12.1154 - val_mse: 246.0567\n",
      "Epoch 498/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.7148 - mae: 0.6043 - mse: 0.7148 - val_loss: 245.1985 - val_mae: 12.0302 - val_mse: 245.1985\n",
      "Epoch 499/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.5645 - mae: 0.5029 - mse: 0.5645 - val_loss: 251.4399 - val_mae: 12.2234 - val_mse: 251.4399\n",
      "Epoch 500/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.7394 - mae: 0.6040 - mse: 0.7394 - val_loss: 239.3602 - val_mae: 11.9782 - val_mse: 239.3602\n",
      "Epoch 501/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5964 - mae: 0.5457 - mse: 0.5964 - val_loss: 245.6772 - val_mae: 11.9975 - val_mse: 245.6772\n",
      "Epoch 502/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0052 - mae: 0.5740 - mse: 1.005 - 0s 79us/step - loss: 0.6180 - mae: 0.4834 - mse: 0.6180 - val_loss: 249.4040 - val_mae: 12.1419 - val_mse: 249.4040\n",
      "Epoch 503/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.7522 - mae: 0.5894 - mse: 0.7522 - val_loss: 241.2540 - val_mae: 12.0010 - val_mse: 241.2540\n",
      "Epoch 504/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.8116 - mae: 0.6121 - mse: 0.8116 - val_loss: 248.7121 - val_mae: 12.0412 - val_mse: 248.7121\n",
      "Epoch 505/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.7365 - mae: 0.6034 - mse: 0.7365 - val_loss: 236.3789 - val_mae: 11.8905 - val_mse: 236.3789\n",
      "Epoch 506/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.7549 - mae: 0.6015 - mse: 0.7549 - val_loss: 252.2394 - val_mae: 12.1059 - val_mse: 252.2394\n",
      "Epoch 507/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 1.2057 - mae: 0.7919 - mse: 1.2057 - val_loss: 245.5590 - val_mae: 12.0765 - val_mse: 245.5590\n",
      "Epoch 508/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.0509 - mae: 0.7939 - mse: 1.0509 - val_loss: 239.6224 - val_mae: 11.9433 - val_mse: 239.6224\n",
      "Epoch 509/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.9275 - mae: 0.7404 - mse: 0.9275 - val_loss: 247.0082 - val_mae: 12.0190 - val_mse: 247.0082\n",
      "Epoch 510/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.9793 - mae: 0.6876 - mse: 0.9793 - val_loss: 237.5094 - val_mae: 11.8992 - val_mse: 237.5094\n",
      "Epoch 511/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7737 - mae: 0.5953 - mse: 0.7737 - val_loss: 253.0801 - val_mae: 12.1626 - val_mse: 253.0801\n",
      "Epoch 512/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.7851 - mae: 0.6096 - mse: 0.7851 - val_loss: 243.6880 - val_mae: 12.0206 - val_mse: 243.6880\n",
      "Epoch 513/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.8875 - mae: 0.6270 - mse: 0.8875 - val_loss: 238.1953 - val_mae: 11.8578 - val_mse: 238.1953\n",
      "Epoch 514/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.1199 - mae: 0.7819 - mse: 1.1199 - val_loss: 241.7722 - val_mae: 11.8273 - val_mse: 241.7722\n",
      "Epoch 515/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.9681 - mae: 0.6735 - mse: 0.9681 - val_loss: 238.5835 - val_mae: 11.8661 - val_mse: 238.5835\n",
      "Epoch 516/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.9071 - mae: 0.6687 - mse: 0.9071 - val_loss: 248.7264 - val_mae: 12.1547 - val_mse: 248.7264\n",
      "Epoch 517/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.7533 - mae: 0.6148 - mse: 0.7533 - val_loss: 245.4705 - val_mae: 11.9815 - val_mse: 245.4705\n",
      "Epoch 518/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.7002 - mae: 0.6088 - mse: 0.7002 - val_loss: 246.5768 - val_mae: 12.1347 - val_mse: 246.5768\n",
      "Epoch 519/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6831 - mae: 0.6111 - mse: 0.6831 - val_loss: 246.0464 - val_mae: 12.0245 - val_mse: 246.0464\n",
      "Epoch 520/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.5426 - mae: 0.4951 - mse: 0.5426 - val_loss: 239.0219 - val_mae: 11.9649 - val_mse: 239.0219\n",
      "Epoch 521/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6294 - mae: 0.5171 - mse: 0.6294 - val_loss: 249.8822 - val_mae: 12.0952 - val_mse: 249.8822\n",
      "Epoch 522/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5649 - mae: 0.4935 - mse: 0.5649 - val_loss: 235.9156 - val_mae: 11.8347 - val_mse: 235.9156\n",
      "Epoch 523/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.7016 - mae: 0.6097 - mse: 0.7016 - val_loss: 244.7983 - val_mae: 12.0544 - val_mse: 244.7983\n",
      "Epoch 524/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6887 - mae: 0.6002 - mse: 0.6887 - val_loss: 249.7630 - val_mae: 12.1223 - val_mse: 249.7630\n",
      "Epoch 525/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6590 - mae: 0.5832 - mse: 0.6590 - val_loss: 240.9232 - val_mae: 11.9763 - val_mse: 240.9232\n",
      "Epoch 526/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5574 - mae: 0.5297 - mse: 0.5574 - val_loss: 248.7444 - val_mae: 12.0483 - val_mse: 248.7444\n",
      "Epoch 527/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4617 - mae: 0.4279 - mse: 0.4617 - val_loss: 238.7076 - val_mae: 11.8846 - val_mse: 238.7076\n",
      "Epoch 528/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.5460 - mae: 0.4987 - mse: 0.5460 - val_loss: 242.4895 - val_mae: 11.9475 - val_mse: 242.4895\n",
      "Epoch 529/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6566 - mae: 0.5548 - mse: 0.6566 - val_loss: 248.3988 - val_mae: 12.0250 - val_mse: 248.3988\n",
      "Epoch 530/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.6846 - mae: 0.6063 - mse: 0.6846 - val_loss: 241.2887 - val_mae: 11.9601 - val_mse: 241.2887\n",
      "Epoch 531/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.9173 - mae: 0.7014 - mse: 0.9173 - val_loss: 244.3953 - val_mae: 11.9474 - val_mse: 244.3953\n",
      "Epoch 532/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.7094 - mae: 0.5856 - mse: 0.7094 - val_loss: 243.3759 - val_mae: 11.9612 - val_mse: 243.3759\n",
      "Epoch 533/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6304 - mae: 0.5594 - mse: 0.6304 - val_loss: 239.9569 - val_mae: 12.0065 - val_mse: 239.9569\n",
      "Epoch 534/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.7471 - mae: 0.6192 - mse: 0.7471 - val_loss: 244.6187 - val_mae: 11.9499 - val_mse: 244.6187\n",
      "Epoch 535/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7310 - mae: 0.6437 - mse: 0.7310 - val_loss: 232.9974 - val_mae: 11.7581 - val_mse: 232.9974\n",
      "Epoch 536/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.7756 - mae: 0.6329 - mse: 0.7756 - val_loss: 252.3307 - val_mae: 12.1024 - val_mse: 252.3307\n",
      "Epoch 537/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.8042 - mae: 0.6737 - mse: 0.8042 - val_loss: 245.6942 - val_mae: 12.0666 - val_mse: 245.6942\n",
      "Epoch 538/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5972 - mae: 0.5447 - mse: 0.5972 - val_loss: 244.2075 - val_mae: 11.9769 - val_mse: 244.2075\n",
      "Epoch 539/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4486 - mae: 0.4259 - mse: 0.4486 - val_loss: 241.1418 - val_mae: 11.8218 - val_mse: 241.1418\n",
      "Epoch 540/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4919 - mae: 0.4206 - mse: 0.4919 - val_loss: 238.8539 - val_mae: 11.8745 - val_mse: 238.8539\n",
      "Epoch 541/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.4809 - mae: 0.4427 - mse: 0.4809 - val_loss: 243.1838 - val_mae: 11.9586 - val_mse: 243.1838\n",
      "Epoch 542/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.3869 - mae: 0.3612 - mse: 0.3869 - val_loss: 240.5571 - val_mae: 11.9639 - val_mse: 240.5571\n",
      "Epoch 543/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4175 - mae: 0.3592 - mse: 0.4175 - val_loss: 245.1998 - val_mae: 12.0384 - val_mse: 245.1998\n",
      "Epoch 544/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4099 - mae: 0.3673 - mse: 0.4099 - val_loss: 243.0658 - val_mae: 11.9187 - val_mse: 243.0658\n",
      "Epoch 545/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4638 - mae: 0.3893 - mse: 0.4638 - val_loss: 239.6668 - val_mae: 11.9302 - val_mse: 239.6668\n",
      "Epoch 546/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3747 - mae: 0.3468 - mse: 0.3747 - val_loss: 247.5714 - val_mae: 12.0726 - val_mse: 247.5714\n",
      "Epoch 547/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3909 - mae: 0.3370 - mse: 0.3909 - val_loss: 243.4289 - val_mae: 11.9681 - val_mse: 243.4289\n",
      "Epoch 548/3500\n",
      "126/126 [==============================] - 0s 166us/step - loss: 0.4679 - mae: 0.3931 - mse: 0.4679 - val_loss: 242.8842 - val_mae: 11.9678 - val_mse: 242.8842\n",
      "Epoch 549/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5159 - mae: 0.3900 - mse: 0.5159 - val_loss: 244.2414 - val_mae: 11.9823 - val_mse: 244.2414\n",
      "Epoch 550/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5173 - mae: 0.4419 - mse: 0.5173 - val_loss: 242.9394 - val_mae: 11.9370 - val_mse: 242.9394\n",
      "Epoch 551/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4673 - mae: 0.4111 - mse: 0.4673 - val_loss: 241.1464 - val_mae: 11.9559 - val_mse: 241.1464\n",
      "Epoch 552/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6680 - mae: 0.5092 - mse: 0.6680 - val_loss: 239.6725 - val_mae: 11.8404 - val_mse: 239.6725\n",
      "Epoch 553/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6266 - mae: 0.5085 - mse: 0.6266 - val_loss: 246.9302 - val_mae: 12.0161 - val_mse: 246.9302\n",
      "Epoch 554/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.4453 - mae: 0.4220 - mse: 0.4453 - val_loss: 241.7107 - val_mae: 11.9987 - val_mse: 241.7107\n",
      "Epoch 555/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4561 - mae: 0.4235 - mse: 0.4561 - val_loss: 242.3986 - val_mae: 11.8945 - val_mse: 242.3986\n",
      "Epoch 556/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4531 - mae: 0.4082 - mse: 0.4531 - val_loss: 242.8987 - val_mae: 12.0200 - val_mse: 242.8987\n",
      "Epoch 557/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.4663 - mae: 0.4031 - mse: 0.4663 - val_loss: 240.1308 - val_mae: 11.8859 - val_mse: 240.1308\n",
      "Epoch 558/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3812 - mae: 0.3104 - mse: 0.3812 - val_loss: 241.7664 - val_mae: 11.9420 - val_mse: 241.7664\n",
      "Epoch 559/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3669 - mae: 0.3304 - mse: 0.3669 - val_loss: 243.9568 - val_mae: 11.9441 - val_mse: 243.9568\n",
      "Epoch 560/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3943 - mae: 0.3730 - mse: 0.3943 - val_loss: 242.8396 - val_mae: 11.9607 - val_mse: 242.8396\n",
      "Epoch 561/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4565 - mae: 0.4089 - mse: 0.4565 - val_loss: 242.9857 - val_mae: 11.9270 - val_mse: 242.9857\n",
      "Epoch 562/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4998 - mae: 0.4292 - mse: 0.4998 - val_loss: 242.7966 - val_mae: 11.9173 - val_mse: 242.7966\n",
      "Epoch 563/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4792 - mae: 0.4274 - mse: 0.4792 - val_loss: 243.4429 - val_mae: 12.0283 - val_mse: 243.4429\n",
      "Epoch 564/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5346 - mae: 0.4229 - mse: 0.5346 - val_loss: 242.6583 - val_mae: 11.8993 - val_mse: 242.6583\n",
      "Epoch 565/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4662 - mae: 0.4172 - mse: 0.4662 - val_loss: 241.6378 - val_mae: 11.9535 - val_mse: 241.6378\n",
      "Epoch 566/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5519 - mae: 0.4939 - mse: 0.5519 - val_loss: 243.8263 - val_mae: 11.9534 - val_mse: 243.8263\n",
      "Epoch 567/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6075 - mae: 0.5220 - mse: 0.6075 - val_loss: 247.0055 - val_mae: 11.9922 - val_mse: 247.0055\n",
      "Epoch 568/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5304 - mae: 0.4641 - mse: 0.5304 - val_loss: 241.6365 - val_mae: 11.9049 - val_mse: 241.6365\n",
      "Epoch 569/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5910 - mae: 0.4984 - mse: 0.5910 - val_loss: 238.6911 - val_mae: 11.8659 - val_mse: 238.6911\n",
      "Epoch 570/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5012 - mae: 0.4845 - mse: 0.5012 - val_loss: 247.9745 - val_mae: 12.0275 - val_mse: 247.9745\n",
      "Epoch 571/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5266 - mae: 0.4498 - mse: 0.5266 - val_loss: 241.3716 - val_mae: 11.9762 - val_mse: 241.3716\n",
      "Epoch 572/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.4325 - mae: 0.4263 - mse: 0.4325 - val_loss: 244.8036 - val_mae: 11.9799 - val_mse: 244.8036\n",
      "Epoch 573/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4064 - mae: 0.4224 - mse: 0.4064 - val_loss: 241.4747 - val_mae: 11.9311 - val_mse: 241.4747\n",
      "Epoch 574/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4009 - mae: 0.4001 - mse: 0.4009 - val_loss: 246.6457 - val_mae: 11.9993 - val_mse: 246.6457\n",
      "Epoch 575/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4783 - mae: 0.4197 - mse: 0.4783 - val_loss: 247.1421 - val_mae: 12.0836 - val_mse: 247.1421\n",
      "Epoch 576/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4471 - mae: 0.4251 - mse: 0.4471 - val_loss: 245.4460 - val_mae: 12.0266 - val_mse: 245.4460\n",
      "Epoch 577/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4595 - mae: 0.4275 - mse: 0.4595 - val_loss: 242.6753 - val_mae: 11.9212 - val_mse: 242.6753\n",
      "Epoch 578/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4464 - mae: 0.4587 - mse: 0.4464 - val_loss: 241.1854 - val_mae: 11.9398 - val_mse: 241.1854\n",
      "Epoch 579/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5084 - mae: 0.4464 - mse: 0.5084 - val_loss: 247.0523 - val_mae: 12.0431 - val_mse: 247.0523\n",
      "Epoch 580/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5509 - mae: 0.4542 - mse: 0.5509 - val_loss: 243.8948 - val_mae: 12.0356 - val_mse: 243.8948\n",
      "Epoch 581/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5527 - mae: 0.4895 - mse: 0.5527 - val_loss: 244.7797 - val_mae: 11.9984 - val_mse: 244.7797\n",
      "Epoch 582/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5844 - mae: 0.5275 - mse: 0.5844 - val_loss: 245.8906 - val_mae: 11.9578 - val_mse: 245.8906\n",
      "Epoch 583/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4616 - mae: 0.4764 - mse: 0.4616 - val_loss: 239.8566 - val_mae: 11.9416 - val_mse: 239.8566\n",
      "Epoch 584/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4653 - mae: 0.4287 - mse: 0.4653 - val_loss: 246.9388 - val_mae: 12.0585 - val_mse: 246.9388\n",
      "Epoch 585/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4107 - mae: 0.3940 - mse: 0.4107 - val_loss: 244.7887 - val_mae: 11.9874 - val_mse: 244.7887\n",
      "Epoch 586/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4597 - mae: 0.3989 - mse: 0.4597 - val_loss: 241.9805 - val_mae: 11.8826 - val_mse: 241.9805\n",
      "Epoch 587/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5797 - mae: 0.4690 - mse: 0.5797 - val_loss: 243.2023 - val_mae: 11.9313 - val_mse: 243.2023\n",
      "Epoch 588/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4091 - mae: 0.3471 - mse: 0.4091 - val_loss: 247.4498 - val_mae: 12.0555 - val_mse: 247.4498\n",
      "Epoch 589/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4007 - mae: 0.3463 - mse: 0.4007 - val_loss: 237.8342 - val_mae: 11.8858 - val_mse: 237.8342\n",
      "Epoch 590/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4412 - mae: 0.3822 - mse: 0.4412 - val_loss: 245.9814 - val_mae: 11.9596 - val_mse: 245.9814\n",
      "Epoch 591/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.5755 - mae: 0.4807 - mse: 0.5755 - val_loss: 244.4543 - val_mae: 12.0014 - val_mse: 244.4543\n",
      "Epoch 592/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.6533 - mae: 0.5365 - mse: 0.6533 - val_loss: 244.2323 - val_mae: 11.9888 - val_mse: 244.2323\n",
      "Epoch 593/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6604 - mae: 0.5487 - mse: 0.6604 - val_loss: 247.7894 - val_mae: 12.0089 - val_mse: 247.7894\n",
      "Epoch 594/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6222 - mae: 0.5253 - mse: 0.6222 - val_loss: 235.4544 - val_mae: 11.8465 - val_mse: 235.4544\n",
      "Epoch 595/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6327 - mae: 0.5212 - mse: 0.6327 - val_loss: 246.4651 - val_mae: 11.9927 - val_mse: 246.4651\n",
      "Epoch 596/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5964 - mae: 0.5216 - mse: 0.5964 - val_loss: 241.3871 - val_mae: 11.9525 - val_mse: 241.3871\n",
      "Epoch 597/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.1433 - mae: 0.2911 - mse: 0.143 - 0s 71us/step - loss: 0.6669 - mae: 0.5595 - mse: 0.6669 - val_loss: 241.4781 - val_mae: 11.9612 - val_mse: 241.4781\n",
      "Epoch 598/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.9321 - mae: 0.7021 - mse: 0.9321 - val_loss: 253.6402 - val_mae: 12.0764 - val_mse: 253.6402\n",
      "Epoch 599/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.9223 - mae: 0.7103 - mse: 0.9223 - val_loss: 240.3559 - val_mae: 11.9432 - val_mse: 240.3559\n",
      "Epoch 600/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7855 - mae: 0.6317 - mse: 0.7855 - val_loss: 245.8176 - val_mae: 11.9697 - val_mse: 245.8176\n",
      "Epoch 601/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6384 - mae: 0.5691 - mse: 0.6384 - val_loss: 241.9388 - val_mae: 11.9227 - val_mse: 241.9388\n",
      "Epoch 602/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.6183 - mae: 0.5390 - mse: 0.6183 - val_loss: 246.1687 - val_mae: 12.0602 - val_mse: 246.1687\n",
      "Epoch 603/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4881 - mae: 0.4402 - mse: 0.4881 - val_loss: 245.2464 - val_mae: 11.9894 - val_mse: 245.2464\n",
      "Epoch 604/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5625 - mae: 0.5249 - mse: 0.5625 - val_loss: 240.5951 - val_mae: 11.9314 - val_mse: 240.5951\n",
      "Epoch 605/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4612 - mae: 0.4203 - mse: 0.4612 - val_loss: 247.2613 - val_mae: 12.0155 - val_mse: 247.2613\n",
      "Epoch 606/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4782 - mae: 0.4227 - mse: 0.4782 - val_loss: 240.8242 - val_mae: 11.9351 - val_mse: 240.8242\n",
      "Epoch 607/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4401 - mae: 0.3947 - mse: 0.4401 - val_loss: 248.8404 - val_mae: 12.0314 - val_mse: 248.8404\n",
      "Epoch 608/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4826 - mae: 0.4422 - mse: 0.4826 - val_loss: 242.4868 - val_mae: 11.9590 - val_mse: 242.4868\n",
      "Epoch 609/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5057 - mae: 0.4456 - mse: 0.5057 - val_loss: 245.2729 - val_mae: 12.0115 - val_mse: 245.2729\n",
      "Epoch 610/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3975 - mae: 0.3580 - mse: 0.3975 - val_loss: 247.7188 - val_mae: 12.0436 - val_mse: 247.7188\n",
      "Epoch 611/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4877 - mae: 0.4384 - mse: 0.4877 - val_loss: 239.6414 - val_mae: 11.9353 - val_mse: 239.6414\n",
      "Epoch 612/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4377 - mae: 0.4067 - mse: 0.4377 - val_loss: 246.4149 - val_mae: 12.0175 - val_mse: 246.4149\n",
      "Epoch 613/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4237 - mae: 0.4105 - mse: 0.4237 - val_loss: 240.8263 - val_mae: 11.9255 - val_mse: 240.8263\n",
      "Epoch 614/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4814 - mae: 0.3929 - mse: 0.4814 - val_loss: 247.4625 - val_mae: 12.0219 - val_mse: 247.4625\n",
      "Epoch 615/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4157 - mae: 0.3836 - mse: 0.4157 - val_loss: 244.0285 - val_mae: 11.9901 - val_mse: 244.0285\n",
      "Epoch 616/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4916 - mae: 0.4579 - mse: 0.4916 - val_loss: 240.2918 - val_mae: 11.8914 - val_mse: 240.2918\n",
      "Epoch 617/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.5457 - mae: 0.4901 - mse: 0.5457 - val_loss: 248.5635 - val_mae: 12.0598 - val_mse: 248.5635\n",
      "Epoch 618/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.5244 - mae: 0.4968 - mse: 0.5244 - val_loss: 240.5747 - val_mae: 11.8939 - val_mse: 240.5747\n",
      "Epoch 619/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.5005 - mae: 0.4653 - mse: 0.5005 - val_loss: 243.5961 - val_mae: 11.9494 - val_mse: 243.5961\n",
      "Epoch 620/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5346 - mae: 0.4890 - mse: 0.5346 - val_loss: 248.3971 - val_mae: 12.0692 - val_mse: 248.3971\n",
      "Epoch 621/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5009 - mae: 0.4713 - mse: 0.5009 - val_loss: 241.2219 - val_mae: 11.9479 - val_mse: 241.2219\n",
      "Epoch 622/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5217 - mae: 0.4750 - mse: 0.5217 - val_loss: 242.5934 - val_mae: 11.8919 - val_mse: 242.5934\n",
      "Epoch 623/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4075 - mae: 0.3630 - mse: 0.4075 - val_loss: 243.6012 - val_mae: 11.9694 - val_mse: 243.6012\n",
      "Epoch 624/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3683 - mae: 0.3482 - mse: 0.3683 - val_loss: 245.3025 - val_mae: 12.0274 - val_mse: 245.3025\n",
      "Epoch 625/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4512 - mae: 0.4316 - mse: 0.4512 - val_loss: 243.3979 - val_mae: 11.9334 - val_mse: 243.3979\n",
      "Epoch 626/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3941 - mae: 0.3613 - mse: 0.3941 - val_loss: 240.0081 - val_mae: 11.9085 - val_mse: 240.0081\n",
      "Epoch 627/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3258 - mae: 0.3030 - mse: 0.3258 - val_loss: 243.0370 - val_mae: 11.9079 - val_mse: 243.0370\n",
      "Epoch 628/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3901 - mae: 0.3602 - mse: 0.3901 - val_loss: 244.6327 - val_mae: 12.0032 - val_mse: 244.6327\n",
      "Epoch 629/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3701 - mae: 0.3228 - mse: 0.3701 - val_loss: 246.2638 - val_mae: 12.0115 - val_mse: 246.2638\n",
      "Epoch 630/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3631 - mae: 0.3284 - mse: 0.3631 - val_loss: 242.9178 - val_mae: 11.9401 - val_mse: 242.9178\n",
      "Epoch 631/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4312 - mae: 0.3805 - mse: 0.4312 - val_loss: 245.4811 - val_mae: 12.0059 - val_mse: 245.4811\n",
      "Epoch 632/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4954 - mae: 0.4647 - mse: 0.4954 - val_loss: 244.5422 - val_mae: 11.9967 - val_mse: 244.5422\n",
      "Epoch 633/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5530 - mae: 0.4240 - mse: 0.5530 - val_loss: 247.1285 - val_mae: 12.1269 - val_mse: 247.1285\n",
      "Epoch 634/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5836 - mae: 0.4582 - mse: 0.5836 - val_loss: 245.8392 - val_mae: 12.0639 - val_mse: 245.8392\n",
      "Epoch 635/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4739 - mae: 0.4255 - mse: 0.4739 - val_loss: 240.3292 - val_mae: 11.8302 - val_mse: 240.3292\n",
      "Epoch 636/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5460 - mae: 0.4358 - mse: 0.5460 - val_loss: 241.8677 - val_mae: 11.9634 - val_mse: 241.8677\n",
      "Epoch 637/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5021 - mae: 0.4002 - mse: 0.5021 - val_loss: 252.5272 - val_mae: 12.1105 - val_mse: 252.5272\n",
      "Epoch 638/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4577 - mae: 0.3890 - mse: 0.4577 - val_loss: 242.5653 - val_mae: 11.9632 - val_mse: 242.5653\n",
      "Epoch 639/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3788 - mae: 0.3420 - mse: 0.3788 - val_loss: 244.0033 - val_mae: 11.9474 - val_mse: 244.0033\n",
      "Epoch 640/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4171 - mae: 0.3710 - mse: 0.4171 - val_loss: 244.1622 - val_mae: 11.9418 - val_mse: 244.1622\n",
      "Epoch 641/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3601 - mae: 0.3280 - mse: 0.3601 - val_loss: 243.7077 - val_mae: 12.0195 - val_mse: 243.7077\n",
      "Epoch 642/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3711 - mae: 0.3252 - mse: 0.3711 - val_loss: 250.9047 - val_mae: 12.0440 - val_mse: 250.9047\n",
      "Epoch 643/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4716 - mae: 0.4161 - mse: 0.4716 - val_loss: 239.4268 - val_mae: 11.9164 - val_mse: 239.4268\n",
      "Epoch 644/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6430 - mae: 0.5568 - mse: 0.6430 - val_loss: 247.1192 - val_mae: 11.9362 - val_mse: 247.1192\n",
      "Epoch 645/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7650 - mae: 0.5949 - mse: 0.7650 - val_loss: 239.9446 - val_mae: 11.9593 - val_mse: 239.9446\n",
      "Epoch 646/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7829 - mae: 0.6222 - mse: 0.7829 - val_loss: 247.1976 - val_mae: 12.0640 - val_mse: 247.1976\n",
      "Epoch 647/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.8425 - mae: 0.7042 - mse: 0.8425 - val_loss: 251.0512 - val_mae: 12.0671 - val_mse: 251.0512\n",
      "Epoch 648/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.9015 - mae: 0.6370 - mse: 0.9015 - val_loss: 239.3905 - val_mae: 11.8867 - val_mse: 239.3905\n",
      "Epoch 649/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6639 - mae: 0.5841 - mse: 0.6639 - val_loss: 246.5891 - val_mae: 12.0453 - val_mse: 246.5891\n",
      "Epoch 650/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.9638 - mae: 0.7172 - mse: 0.9638 - val_loss: 249.3968 - val_mae: 12.0215 - val_mse: 249.3968\n",
      "Epoch 651/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.1492 - mae: 0.7995 - mse: 1.1492 - val_loss: 236.8583 - val_mae: 11.9559 - val_mse: 236.8583\n",
      "Epoch 652/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.1698 - mae: 0.8405 - mse: 1.1698 - val_loss: 248.5075 - val_mae: 11.9830 - val_mse: 248.5075\n",
      "Epoch 653/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.9954 - mae: 0.6819 - mse: 0.9954 - val_loss: 243.8582 - val_mae: 12.0406 - val_mse: 243.8582\n",
      "Epoch 654/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7524 - mae: 0.6346 - mse: 0.7524 - val_loss: 242.7093 - val_mae: 12.0017 - val_mse: 242.7093\n",
      "Epoch 655/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.0476 - mae: 0.7590 - mse: 1.0476 - val_loss: 248.7713 - val_mae: 12.0843 - val_mse: 248.7713\n",
      "Epoch 656/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6872 - mae: 0.6173 - mse: 0.6872 - val_loss: 248.3788 - val_mae: 12.1405 - val_mse: 248.3788\n",
      "Epoch 657/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.8765 - mae: 0.6758 - mse: 0.8765 - val_loss: 235.0386 - val_mae: 11.8623 - val_mse: 235.0386\n",
      "Epoch 658/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.8632 - mae: 0.6396 - mse: 0.8632 - val_loss: 249.8887 - val_mae: 12.0430 - val_mse: 249.8887\n",
      "Epoch 659/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.8589 - mae: 0.6992 - mse: 0.8589 - val_loss: 238.6659 - val_mae: 11.9985 - val_mse: 238.6659\n",
      "Epoch 660/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.8623 - mae: 0.7228 - mse: 0.8623 - val_loss: 250.9620 - val_mae: 12.0586 - val_mse: 250.9620\n",
      "Epoch 661/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.8520 - mae: 0.6405 - mse: 0.8520 - val_loss: 243.8686 - val_mae: 11.9801 - val_mse: 243.8686\n",
      "Epoch 662/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.7331 - mae: 0.5615 - mse: 0.7331 - val_loss: 240.9216 - val_mae: 11.9258 - val_mse: 240.9216\n",
      "Epoch 663/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.7993 - mae: 0.6260 - mse: 0.7993 - val_loss: 244.7215 - val_mae: 11.9341 - val_mse: 244.7215\n",
      "Epoch 664/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.7608 - mae: 0.6134 - mse: 0.7608 - val_loss: 242.8726 - val_mae: 11.9176 - val_mse: 242.8726\n",
      "Epoch 665/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4563 - mae: 0.4335 - mse: 0.4563 - val_loss: 244.0522 - val_mae: 11.9733 - val_mse: 244.0522\n",
      "Epoch 666/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4231 - mae: 0.3902 - mse: 0.4231 - val_loss: 247.0359 - val_mae: 12.0129 - val_mse: 247.0359\n",
      "Epoch 667/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6223 - mae: 0.4892 - mse: 0.6223 - val_loss: 241.1789 - val_mae: 11.8858 - val_mse: 241.1789\n",
      "Epoch 668/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6556 - mae: 0.5194 - mse: 0.6556 - val_loss: 236.5204 - val_mae: 11.8525 - val_mse: 236.5204\n",
      "Epoch 669/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4803 - mae: 0.4603 - mse: 0.4803 - val_loss: 244.2452 - val_mae: 11.9559 - val_mse: 244.2452\n",
      "Epoch 670/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5466 - mae: 0.5031 - mse: 0.5466 - val_loss: 240.8152 - val_mae: 11.9862 - val_mse: 240.8152\n",
      "Epoch 671/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5402 - mae: 0.5042 - mse: 0.5402 - val_loss: 248.5400 - val_mae: 12.0586 - val_mse: 248.5400\n",
      "Epoch 672/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5914 - mae: 0.5520 - mse: 0.5914 - val_loss: 245.1140 - val_mae: 11.9843 - val_mse: 245.1140\n",
      "Epoch 673/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.5181 - mae: 0.4770 - mse: 0.5181 - val_loss: 240.3784 - val_mae: 11.8962 - val_mse: 240.3784\n",
      "Epoch 674/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5951 - mae: 0.4747 - mse: 0.5951 - val_loss: 237.4096 - val_mae: 11.8076 - val_mse: 237.4096\n",
      "Epoch 675/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4585 - mae: 0.4379 - mse: 0.4585 - val_loss: 241.5650 - val_mae: 11.8864 - val_mse: 241.5650\n",
      "Epoch 676/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.3362 - mae: 0.4284 - mse: 0.336 - 0s 79us/step - loss: 0.4277 - mae: 0.4044 - mse: 0.4277 - val_loss: 248.8502 - val_mae: 12.1070 - val_mse: 248.8502\n",
      "Epoch 677/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4876 - mae: 0.4175 - mse: 0.4876 - val_loss: 245.5507 - val_mae: 11.9976 - val_mse: 245.5507\n",
      "Epoch 678/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3786 - mae: 0.3656 - mse: 0.3786 - val_loss: 239.5984 - val_mae: 11.9171 - val_mse: 239.5984\n",
      "Epoch 679/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4658 - mae: 0.4327 - mse: 0.4658 - val_loss: 242.0858 - val_mae: 11.9299 - val_mse: 242.0858\n",
      "Epoch 680/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4960 - mae: 0.4379 - mse: 0.4960 - val_loss: 241.4390 - val_mae: 11.9549 - val_mse: 241.4390\n",
      "Epoch 681/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5199 - mae: 0.4971 - mse: 0.5199 - val_loss: 249.0872 - val_mae: 11.9737 - val_mse: 249.0872\n",
      "Epoch 682/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.5573 - mae: 0.4988 - mse: 0.5573 - val_loss: 237.6122 - val_mae: 11.8769 - val_mse: 237.6122\n",
      "Epoch 683/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.7365 - mae: 0.5959 - mse: 0.7365 - val_loss: 245.8140 - val_mae: 11.9604 - val_mse: 245.8140\n",
      "Epoch 684/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5775 - mae: 0.5417 - mse: 0.5775 - val_loss: 245.4431 - val_mae: 12.0019 - val_mse: 245.4431\n",
      "Epoch 685/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5905 - mae: 0.5269 - mse: 0.5905 - val_loss: 235.7713 - val_mae: 11.8484 - val_mse: 235.7713\n",
      "Epoch 686/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6012 - mae: 0.5445 - mse: 0.6012 - val_loss: 251.1259 - val_mae: 12.1030 - val_mse: 251.1259\n",
      "Epoch 687/3500\n",
      "126/126 [==============================] - 0s 198us/step - loss: 0.5505 - mae: 0.5197 - mse: 0.5505 - val_loss: 239.0121 - val_mae: 11.9574 - val_mse: 239.0121\n",
      "Epoch 688/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5305 - mae: 0.4628 - mse: 0.5305 - val_loss: 241.8049 - val_mae: 11.8507 - val_mse: 241.8049\n",
      "Epoch 689/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4198 - mae: 0.4040 - mse: 0.4198 - val_loss: 245.6480 - val_mae: 11.9738 - val_mse: 245.6480\n",
      "Epoch 690/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.4218 - mae: 0.3910 - mse: 0.4218 - val_loss: 240.7072 - val_mae: 11.8880 - val_mse: 240.7072\n",
      "Epoch 691/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.3824 - mae: 0.3831 - mse: 0.3824 - val_loss: 243.3696 - val_mae: 11.9304 - val_mse: 243.3696\n",
      "Epoch 692/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5051 - mae: 0.4503 - mse: 0.5051 - val_loss: 243.4568 - val_mae: 12.0050 - val_mse: 243.4568\n",
      "Epoch 693/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3885 - mae: 0.3853 - mse: 0.3885 - val_loss: 246.9409 - val_mae: 12.0322 - val_mse: 246.9409\n",
      "Epoch 694/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4367 - mae: 0.3939 - mse: 0.4367 - val_loss: 246.6971 - val_mae: 11.9278 - val_mse: 246.6971\n",
      "Epoch 695/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4758 - mae: 0.4309 - mse: 0.4758 - val_loss: 239.9380 - val_mae: 11.9251 - val_mse: 239.9380\n",
      "Epoch 696/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5035 - mae: 0.4814 - mse: 0.5035 - val_loss: 249.4673 - val_mae: 12.0119 - val_mse: 249.4673\n",
      "Epoch 697/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5949 - mae: 0.4843 - mse: 0.5949 - val_loss: 237.2486 - val_mae: 11.8699 - val_mse: 237.2486\n",
      "Epoch 698/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5283 - mae: 0.4922 - mse: 0.5283 - val_loss: 239.9550 - val_mae: 11.8992 - val_mse: 239.9550\n",
      "Epoch 699/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4780 - mae: 0.4328 - mse: 0.4780 - val_loss: 241.6874 - val_mae: 11.8881 - val_mse: 241.6874\n",
      "Epoch 700/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4057 - mae: 0.3939 - mse: 0.4057 - val_loss: 242.0312 - val_mae: 11.9307 - val_mse: 242.0312\n",
      "Epoch 701/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4076 - mae: 0.3717 - mse: 0.4076 - val_loss: 244.8630 - val_mae: 11.9431 - val_mse: 244.8630\n",
      "Epoch 702/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3760 - mae: 0.3647 - mse: 0.3760 - val_loss: 243.2592 - val_mae: 11.9202 - val_mse: 243.2592\n",
      "Epoch 703/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4228 - mae: 0.3974 - mse: 0.4228 - val_loss: 242.8324 - val_mae: 11.9545 - val_mse: 242.8324\n",
      "Epoch 704/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3901 - mae: 0.3630 - mse: 0.3901 - val_loss: 241.6961 - val_mae: 11.8914 - val_mse: 241.6961\n",
      "Epoch 705/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3391 - mae: 0.2944 - mse: 0.3391 - val_loss: 246.4975 - val_mae: 12.0114 - val_mse: 246.4975\n",
      "Epoch 706/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4135 - mae: 0.3841 - mse: 0.4135 - val_loss: 242.1022 - val_mae: 11.9238 - val_mse: 242.1022\n",
      "Epoch 707/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.4278 - mae: 0.3865 - mse: 0.4278 - val_loss: 242.5603 - val_mae: 11.9069 - val_mse: 242.5603\n",
      "Epoch 708/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.3338 - mae: 0.3167 - mse: 0.3338 - val_loss: 248.4692 - val_mae: 12.0130 - val_mse: 248.4692\n",
      "Epoch 709/3500\n",
      "126/126 [==============================] - 0s 388us/step - loss: 0.3816 - mae: 0.3437 - mse: 0.3816 - val_loss: 241.6574 - val_mae: 11.9256 - val_mse: 241.6574\n",
      "Epoch 710/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.4039 - mae: 0.3849 - mse: 0.4039 - val_loss: 244.8186 - val_mae: 11.9268 - val_mse: 244.8186\n",
      "Epoch 711/3500\n",
      "126/126 [==============================] - 0s 158us/step - loss: 0.5212 - mae: 0.4245 - mse: 0.5212 - val_loss: 243.1507 - val_mae: 11.9572 - val_mse: 243.1507\n",
      "Epoch 712/3500\n",
      "126/126 [==============================] - 0s 142us/step - loss: 0.4037 - mae: 0.3868 - mse: 0.4037 - val_loss: 243.2089 - val_mae: 11.9461 - val_mse: 243.2089\n",
      "Epoch 713/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4713 - mae: 0.3983 - mse: 0.4713 - val_loss: 246.3865 - val_mae: 11.9701 - val_mse: 246.3865\n",
      "Epoch 714/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.5348 - mae: 0.4740 - mse: 0.5348 - val_loss: 239.9832 - val_mae: 11.9149 - val_mse: 239.9832\n",
      "Epoch 715/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.5949 - mae: 0.5559 - mse: 0.5949 - val_loss: 249.0315 - val_mae: 12.0107 - val_mse: 249.0315\n",
      "Epoch 716/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5378 - mae: 0.4867 - mse: 0.5378 - val_loss: 240.6310 - val_mae: 11.9085 - val_mse: 240.6310\n",
      "Epoch 717/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.5137 - mae: 0.4275 - mse: 0.5137 - val_loss: 244.7209 - val_mae: 11.9302 - val_mse: 244.7209\n",
      "Epoch 718/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4027 - mae: 0.3746 - mse: 0.4027 - val_loss: 247.0064 - val_mae: 12.0092 - val_mse: 247.0064\n",
      "Epoch 719/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4808 - mae: 0.3964 - mse: 0.4808 - val_loss: 243.8233 - val_mae: 11.9151 - val_mse: 243.8233\n",
      "Epoch 720/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4055 - mae: 0.3765 - mse: 0.4055 - val_loss: 241.1308 - val_mae: 11.9289 - val_mse: 241.1308\n",
      "Epoch 721/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3726 - mae: 0.3485 - mse: 0.3726 - val_loss: 243.0506 - val_mae: 11.9114 - val_mse: 243.0506\n",
      "Epoch 722/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.4155 - mae: 0.3772 - mse: 0.4155 - val_loss: 240.5908 - val_mae: 11.8934 - val_mse: 240.5908\n",
      "Epoch 723/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.4763 - mae: 0.4205 - mse: 0.4763 - val_loss: 244.6814 - val_mae: 11.9376 - val_mse: 244.6814\n",
      "Epoch 724/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4393 - mae: 0.3852 - mse: 0.4393 - val_loss: 243.6139 - val_mae: 11.9509 - val_mse: 243.6139\n",
      "Epoch 725/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3887 - mae: 0.3159 - mse: 0.3887 - val_loss: 241.3432 - val_mae: 11.9270 - val_mse: 241.3432\n",
      "Epoch 726/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3831 - mae: 0.3441 - mse: 0.3831 - val_loss: 247.4551 - val_mae: 11.9875 - val_mse: 247.4551\n",
      "Epoch 727/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3531 - mae: 0.3122 - mse: 0.3531 - val_loss: 242.9550 - val_mae: 11.9879 - val_mse: 242.9550\n",
      "Epoch 728/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4432 - mae: 0.3900 - mse: 0.4432 - val_loss: 240.9798 - val_mae: 11.9009 - val_mse: 240.9798\n",
      "Epoch 729/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5586 - mae: 0.4755 - mse: 0.5586 - val_loss: 245.2361 - val_mae: 11.9030 - val_mse: 245.2361\n",
      "Epoch 730/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6733 - mae: 0.6047 - mse: 0.6733 - val_loss: 242.1341 - val_mae: 11.9813 - val_mse: 242.1341\n",
      "Epoch 731/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7326 - mae: 0.5846 - mse: 0.7326 - val_loss: 241.7888 - val_mae: 11.8816 - val_mse: 241.7888\n",
      "Epoch 732/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5404 - mae: 0.4981 - mse: 0.5404 - val_loss: 245.7838 - val_mae: 11.9859 - val_mse: 245.7838\n",
      "Epoch 733/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5127 - mae: 0.5030 - mse: 0.5127 - val_loss: 244.6285 - val_mae: 11.9997 - val_mse: 244.6285\n",
      "Epoch 734/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5594 - mae: 0.5120 - mse: 0.5594 - val_loss: 246.9297 - val_mae: 11.9571 - val_mse: 246.9297\n",
      "Epoch 735/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.5928 - mae: 0.5395 - mse: 0.5928 - val_loss: 243.2816 - val_mae: 11.9194 - val_mse: 243.2816\n",
      "Epoch 736/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5746 - mae: 0.4984 - mse: 0.5746 - val_loss: 244.7506 - val_mae: 11.9426 - val_mse: 244.7506\n",
      "Epoch 737/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6726 - mae: 0.5541 - mse: 0.6726 - val_loss: 247.5347 - val_mae: 11.9538 - val_mse: 247.5347\n",
      "Epoch 738/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6358 - mae: 0.5540 - mse: 0.6358 - val_loss: 241.6413 - val_mae: 11.8889 - val_mse: 241.6413\n",
      "Epoch 739/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5705 - mae: 0.5276 - mse: 0.5705 - val_loss: 240.6102 - val_mae: 11.8340 - val_mse: 240.6102\n",
      "Epoch 740/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5136 - mae: 0.4714 - mse: 0.5136 - val_loss: 248.7065 - val_mae: 12.0949 - val_mse: 248.7065\n",
      "Epoch 741/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5464 - mae: 0.4784 - mse: 0.5464 - val_loss: 247.7336 - val_mae: 12.0931 - val_mse: 247.7336\n",
      "Epoch 742/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6595 - mae: 0.5788 - mse: 0.6595 - val_loss: 249.3756 - val_mae: 12.0637 - val_mse: 249.3756\n",
      "Epoch 743/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7105 - mae: 0.5751 - mse: 0.7105 - val_loss: 238.0895 - val_mae: 11.8304 - val_mse: 238.0895\n",
      "Epoch 744/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6929 - mae: 0.5820 - mse: 0.6929 - val_loss: 240.5531 - val_mae: 11.8770 - val_mse: 240.5531\n",
      "Epoch 745/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5726 - mae: 0.5353 - mse: 0.5726 - val_loss: 249.0007 - val_mae: 12.0568 - val_mse: 249.0007\n",
      "Epoch 746/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.7405 - mae: 0.5932 - mse: 0.7405 - val_loss: 237.1513 - val_mae: 11.8001 - val_mse: 237.1513\n",
      "Epoch 747/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5131 - mae: 0.4948 - mse: 0.5131 - val_loss: 249.0748 - val_mae: 11.9928 - val_mse: 249.0748\n",
      "Epoch 748/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6081 - mae: 0.5090 - mse: 0.6081 - val_loss: 243.3199 - val_mae: 11.9311 - val_mse: 243.3199\n",
      "Epoch 749/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4982 - mae: 0.4491 - mse: 0.4982 - val_loss: 242.2534 - val_mae: 11.8966 - val_mse: 242.2534\n",
      "Epoch 750/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5946 - mae: 0.4807 - mse: 0.5946 - val_loss: 246.0410 - val_mae: 12.0280 - val_mse: 246.0410\n",
      "Epoch 751/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5857 - mae: 0.5381 - mse: 0.5857 - val_loss: 238.1780 - val_mae: 11.8362 - val_mse: 238.1780\n",
      "Epoch 752/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5240 - mae: 0.4772 - mse: 0.5240 - val_loss: 252.3674 - val_mae: 12.0496 - val_mse: 252.3674\n",
      "Epoch 753/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.7185 - mae: 0.5393 - mse: 0.7185 - val_loss: 243.7452 - val_mae: 11.9925 - val_mse: 243.7452\n",
      "Epoch 754/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.7063 - mae: 0.5887 - mse: 0.7063 - val_loss: 243.8315 - val_mae: 11.9555 - val_mse: 243.8315\n",
      "Epoch 755/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5663 - mae: 0.4995 - mse: 0.5663 - val_loss: 243.0279 - val_mae: 11.8265 - val_mse: 243.0279\n",
      "Epoch 756/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.4609 - mae: 0.4467 - mse: 0.4609 - val_loss: 242.4737 - val_mae: 12.0023 - val_mse: 242.4737\n",
      "Epoch 757/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3986 - mae: 0.3845 - mse: 0.3986 - val_loss: 247.7836 - val_mae: 12.0348 - val_mse: 247.7836\n",
      "Epoch 758/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.3722 - mae: 0.3424 - mse: 0.3722 - val_loss: 241.7400 - val_mae: 11.9081 - val_mse: 241.7400\n",
      "Epoch 759/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3043 - mae: 0.2663 - mse: 0.3043 - val_loss: 242.7142 - val_mae: 11.8851 - val_mse: 242.7142\n",
      "Epoch 760/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3289 - mae: 0.2935 - mse: 0.3289 - val_loss: 244.6262 - val_mae: 11.9069 - val_mse: 244.6262\n",
      "Epoch 761/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3777 - mae: 0.3126 - mse: 0.3777 - val_loss: 242.9084 - val_mae: 11.9560 - val_mse: 242.9084\n",
      "Epoch 762/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4411 - mae: 0.4025 - mse: 0.4411 - val_loss: 248.4948 - val_mae: 11.9841 - val_mse: 248.4948\n",
      "Epoch 763/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5051 - mae: 0.4493 - mse: 0.5051 - val_loss: 241.3329 - val_mae: 11.9205 - val_mse: 241.3329\n",
      "Epoch 764/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4155 - mae: 0.4038 - mse: 0.4155 - val_loss: 239.2652 - val_mae: 11.8416 - val_mse: 239.2652\n",
      "Epoch 765/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4806 - mae: 0.4488 - mse: 0.4806 - val_loss: 246.5036 - val_mae: 11.9330 - val_mse: 246.5036\n",
      "Epoch 766/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.5373 - mae: 0.5091 - mse: 0.5373 - val_loss: 240.2170 - val_mae: 11.9153 - val_mse: 240.2170\n",
      "Epoch 767/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5231 - mae: 0.4845 - mse: 0.5231 - val_loss: 248.0116 - val_mae: 12.0001 - val_mse: 248.0116\n",
      "Epoch 768/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.4977 - mae: 0.4562 - mse: 0.4977 - val_loss: 244.6331 - val_mae: 11.9281 - val_mse: 244.6331\n",
      "Epoch 769/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5071 - mae: 0.4519 - mse: 0.5071 - val_loss: 243.4059 - val_mae: 11.9448 - val_mse: 243.4059\n",
      "Epoch 770/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4634 - mae: 0.4400 - mse: 0.4634 - val_loss: 247.1052 - val_mae: 11.9882 - val_mse: 247.1052\n",
      "Epoch 771/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4274 - mae: 0.4228 - mse: 0.4274 - val_loss: 241.4179 - val_mae: 11.9033 - val_mse: 241.4179\n",
      "Epoch 772/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4201 - mae: 0.3849 - mse: 0.4201 - val_loss: 242.7587 - val_mae: 11.8871 - val_mse: 242.7587\n",
      "Epoch 773/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3380 - mae: 0.2957 - mse: 0.3380 - val_loss: 242.0320 - val_mae: 11.9065 - val_mse: 242.0320\n",
      "Epoch 774/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3754 - mae: 0.3549 - mse: 0.3754 - val_loss: 246.6769 - val_mae: 11.9664 - val_mse: 246.6769\n",
      "Epoch 775/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3929 - mae: 0.3613 - mse: 0.3929 - val_loss: 239.6806 - val_mae: 11.8040 - val_mse: 239.6806\n",
      "Epoch 776/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4969 - mae: 0.4207 - mse: 0.4969 - val_loss: 243.1474 - val_mae: 11.8992 - val_mse: 243.1474\n",
      "Epoch 777/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6186 - mae: 0.4844 - mse: 0.6186 - val_loss: 247.7904 - val_mae: 12.0238 - val_mse: 247.7904\n",
      "Epoch 778/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7171 - mae: 0.5954 - mse: 0.7171 - val_loss: 245.8220 - val_mae: 12.0455 - val_mse: 245.8220\n",
      "Epoch 779/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.9653 - mae: 0.7752 - mse: 0.9653 - val_loss: 247.1599 - val_mae: 11.9811 - val_mse: 247.1599\n",
      "Epoch 780/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.0109 - mae: 0.7466 - mse: 1.0109 - val_loss: 247.7392 - val_mae: 12.1875 - val_mse: 247.7392\n",
      "Epoch 781/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.9839 - mae: 0.7482 - mse: 0.9839 - val_loss: 249.1129 - val_mae: 12.0704 - val_mse: 249.1129\n",
      "Epoch 782/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.8387 - mae: 0.6606 - mse: 0.8387 - val_loss: 247.0238 - val_mae: 12.0885 - val_mse: 247.0238\n",
      "Epoch 783/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.8963 - mae: 0.7111 - mse: 0.8963 - val_loss: 242.9684 - val_mae: 11.9152 - val_mse: 242.9684\n",
      "Epoch 784/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.8854 - mae: 0.6666 - mse: 0.8854 - val_loss: 247.8365 - val_mae: 12.0288 - val_mse: 247.8365\n",
      "Epoch 785/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.9879 - mae: 0.7234 - mse: 0.9879 - val_loss: 248.4772 - val_mae: 11.9669 - val_mse: 248.4772\n",
      "Epoch 786/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.9409 - mae: 0.7101 - mse: 0.9409 - val_loss: 236.8791 - val_mae: 11.8261 - val_mse: 236.8791\n",
      "Epoch 787/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.1582 - mae: 0.7791 - mse: 1.1582 - val_loss: 258.8487 - val_mae: 12.0814 - val_mse: 258.8487\n",
      "Epoch 788/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.6909 - mae: 0.5823 - mse: 0.690 - 0s 95us/step - loss: 1.2164 - mae: 0.8241 - mse: 1.2164 - val_loss: 235.1465 - val_mae: 11.9124 - val_mse: 235.1465\n",
      "Epoch 789/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.2555 - mae: 0.8734 - mse: 1.2555 - val_loss: 255.6646 - val_mae: 12.0732 - val_mse: 255.6646\n",
      "Epoch 790/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.4399 - mae: 0.8941 - mse: 1.4399 - val_loss: 234.9970 - val_mae: 11.9489 - val_mse: 234.9970\n",
      "Epoch 791/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.3101 - mae: 0.8561 - mse: 1.3101 - val_loss: 245.8455 - val_mae: 12.0009 - val_mse: 245.8455\n",
      "Epoch 792/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.4315 - mae: 0.8722 - mse: 1.4315 - val_loss: 246.8092 - val_mae: 11.8934 - val_mse: 246.8092\n",
      "Epoch 793/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.1973 - mae: 0.8234 - mse: 1.1973 - val_loss: 230.5530 - val_mae: 11.7360 - val_mse: 230.5530\n",
      "Epoch 794/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.4162 - mae: 0.8797 - mse: 1.4162 - val_loss: 261.1567 - val_mae: 12.2682 - val_mse: 261.1567\n",
      "Epoch 795/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.8973 - mae: 1.0527 - mse: 1.8973 - val_loss: 247.2005 - val_mae: 12.1400 - val_mse: 247.2005\n",
      "Epoch 796/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.5221 - mae: 0.9314 - mse: 1.5221 - val_loss: 229.4125 - val_mae: 11.8998 - val_mse: 229.4125\n",
      "Epoch 797/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.8147 - mae: 1.0685 - mse: 1.8147 - val_loss: 244.3892 - val_mae: 11.8793 - val_mse: 244.3892\n",
      "Epoch 798/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.3698 - mae: 0.9007 - mse: 1.3698 - val_loss: 234.2970 - val_mae: 11.8436 - val_mse: 234.2970\n",
      "Epoch 799/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.4233 - mae: 0.8683 - mse: 1.4233 - val_loss: 249.0935 - val_mae: 12.0119 - val_mse: 249.0935\n",
      "Epoch 800/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.1854 - mae: 0.7631 - mse: 1.1854 - val_loss: 239.9746 - val_mae: 11.7641 - val_mse: 239.9746\n",
      "Epoch 801/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.9373 - mae: 0.6890 - mse: 0.9373 - val_loss: 242.5202 - val_mae: 11.9411 - val_mse: 242.5202\n",
      "Epoch 802/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.0993 - mae: 0.8537 - mse: 1.0993 - val_loss: 248.4787 - val_mae: 11.8684 - val_mse: 248.4787\n",
      "Epoch 803/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.1262 - mae: 0.7827 - mse: 1.1262 - val_loss: 238.4356 - val_mae: 11.7449 - val_mse: 238.4356\n",
      "Epoch 804/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.0085 - mae: 0.6779 - mse: 1.0085 - val_loss: 243.2775 - val_mae: 11.9973 - val_mse: 243.2775\n",
      "Epoch 805/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.2126 - mae: 0.8011 - mse: 1.2126 - val_loss: 243.8338 - val_mae: 11.8252 - val_mse: 243.8338\n",
      "Epoch 806/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.2083 - mae: 0.8440 - mse: 1.2083 - val_loss: 229.8999 - val_mae: 11.7875 - val_mse: 229.8999\n",
      "Epoch 807/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 1.2624 - mae: 0.8330 - mse: 1.2624 - val_loss: 247.0244 - val_mae: 11.9807 - val_mse: 247.0244\n",
      "Epoch 808/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.2700 - mae: 0.8272 - mse: 1.2700 - val_loss: 248.9442 - val_mae: 12.0496 - val_mse: 248.9442\n",
      "Epoch 809/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 1.3134 - mae: 0.8302 - mse: 1.3134 - val_loss: 237.5401 - val_mae: 11.7775 - val_mse: 237.5401\n",
      "Epoch 810/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.6284 - mae: 0.9404 - mse: 1.6284 - val_loss: 239.0259 - val_mae: 11.5945 - val_mse: 239.0259\n",
      "Epoch 811/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.4295 - mae: 0.9279 - mse: 1.4295 - val_loss: 233.8693 - val_mae: 11.7605 - val_mse: 233.8693\n",
      "Epoch 812/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.6833 - mae: 1.0027 - mse: 1.6833 - val_loss: 254.3955 - val_mae: 12.2124 - val_mse: 254.3955\n",
      "Epoch 813/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.2606 - mae: 0.8815 - mse: 1.2606 - val_loss: 236.8634 - val_mae: 11.9418 - val_mse: 236.8634\n",
      "Epoch 814/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.6005 - mae: 0.9532 - mse: 1.6005 - val_loss: 249.2543 - val_mae: 12.0011 - val_mse: 249.2543\n",
      "Epoch 815/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.5276 - mae: 0.9454 - mse: 1.5276 - val_loss: 238.8867 - val_mae: 11.8269 - val_mse: 238.8867\n",
      "Epoch 816/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.3978 - mae: 0.8259 - mse: 1.3978 - val_loss: 241.7592 - val_mae: 11.9531 - val_mse: 241.7592\n",
      "Epoch 817/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.6311 - mae: 0.9694 - mse: 1.6311 - val_loss: 250.5530 - val_mae: 12.1001 - val_mse: 250.5530\n",
      "Epoch 818/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 2.1711 - mae: 1.1985 - mse: 2.1711 - val_loss: 244.5347 - val_mae: 11.9311 - val_mse: 244.5347\n",
      "Epoch 819/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 1.7602 - mae: 1.0164 - mse: 1.7602 - val_loss: 239.7102 - val_mae: 11.7537 - val_mse: 239.7102\n",
      "Epoch 820/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 2.5307 - mae: 1.2740 - mse: 2.5307 - val_loss: 253.3838 - val_mae: 12.1664 - val_mse: 253.3838\n",
      "Epoch 821/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 2.3778 - mae: 1.0434 - mse: 2.3778 - val_loss: 248.2844 - val_mae: 12.1692 - val_mse: 248.2844\n",
      "Epoch 822/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 2.5725 - mae: 1.1460 - mse: 2.5725 - val_loss: 250.7933 - val_mae: 12.0633 - val_mse: 250.7933\n",
      "Epoch 823/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 2.2006 - mae: 1.0831 - mse: 2.2006 - val_loss: 240.4564 - val_mae: 11.8606 - val_mse: 240.4564\n",
      "Epoch 824/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.6700 - mae: 1.0098 - mse: 1.6700 - val_loss: 243.3945 - val_mae: 11.8564 - val_mse: 243.3945\n",
      "Epoch 825/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.6126 - mae: 0.9934 - mse: 1.6126 - val_loss: 240.7148 - val_mae: 11.9307 - val_mse: 240.7148\n",
      "Epoch 826/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.2036 - mae: 0.8529 - mse: 1.2036 - val_loss: 235.7334 - val_mae: 11.5547 - val_mse: 235.7334\n",
      "Epoch 827/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.0901 - mae: 0.7799 - mse: 1.0901 - val_loss: 225.1606 - val_mae: 11.5161 - val_mse: 225.1606\n",
      "Epoch 828/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.0673 - mae: 0.8153 - mse: 1.0673 - val_loss: 245.9596 - val_mae: 11.8502 - val_mse: 245.9596\n",
      "Epoch 829/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.1667 - mae: 0.8333 - mse: 1.1667 - val_loss: 225.9316 - val_mae: 11.6315 - val_mse: 225.9316\n",
      "Epoch 830/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 2.0772 - mae: 1.1086 - mse: 2.0772 - val_loss: 250.2990 - val_mae: 12.1250 - val_mse: 250.2990\n",
      "Epoch 831/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 2.0494 - mae: 1.1871 - mse: 2.0494 - val_loss: 241.2972 - val_mae: 12.1082 - val_mse: 241.2972\n",
      "Epoch 832/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.6996 - mae: 1.0482 - mse: 1.6996 - val_loss: 251.3947 - val_mae: 12.0282 - val_mse: 251.3947\n",
      "Epoch 833/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.2446 - mae: 0.8365 - mse: 1.2446 - val_loss: 242.7914 - val_mae: 11.9700 - val_mse: 242.7914\n",
      "Epoch 834/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.4317 - mae: 0.9568 - mse: 1.4317 - val_loss: 238.2910 - val_mae: 11.7671 - val_mse: 238.2910\n",
      "Epoch 835/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.4815 - mae: 0.8512 - mse: 1.4815 - val_loss: 236.1103 - val_mae: 11.6675 - val_mse: 236.1103\n",
      "Epoch 836/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.2386 - mae: 0.8430 - mse: 1.2386 - val_loss: 227.0816 - val_mae: 11.7089 - val_mse: 227.0816\n",
      "Epoch 837/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.5052 - mae: 0.9108 - mse: 1.5052 - val_loss: 252.7935 - val_mae: 12.0237 - val_mse: 252.7935\n",
      "Epoch 838/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.4797 - mae: 0.9648 - mse: 1.4797 - val_loss: 237.9269 - val_mae: 11.9847 - val_mse: 237.9269\n",
      "Epoch 839/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.8072 - mae: 1.0076 - mse: 1.8072 - val_loss: 247.9725 - val_mae: 12.0782 - val_mse: 247.9725\n",
      "Epoch 840/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.9533 - mae: 1.0824 - mse: 1.9533 - val_loss: 245.8671 - val_mae: 12.0969 - val_mse: 245.8671\n",
      "Epoch 841/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 2.0885 - mae: 1.1346 - mse: 2.0885 - val_loss: 240.5514 - val_mae: 12.1394 - val_mse: 240.5514\n",
      "Epoch 842/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 2.5297 - mae: 1.2672 - mse: 2.5297 - val_loss: 239.7159 - val_mae: 12.0375 - val_mse: 239.7159\n",
      "Epoch 843/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.6215 - mae: 0.9781 - mse: 1.6215 - val_loss: 248.2775 - val_mae: 11.9812 - val_mse: 248.2775\n",
      "Epoch 844/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.3270 - mae: 0.8332 - mse: 1.3270 - val_loss: 234.5685 - val_mae: 11.7314 - val_mse: 234.5685\n",
      "Epoch 845/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.5316 - mae: 0.9269 - mse: 1.5316 - val_loss: 239.8551 - val_mae: 11.7174 - val_mse: 239.8551\n",
      "Epoch 846/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 2.3890 - mae: 1.1342 - mse: 2.3890 - val_loss: 239.4613 - val_mae: 11.6737 - val_mse: 239.4613\n",
      "Epoch 847/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 2.9282 - mae: 1.2904 - mse: 2.9282 - val_loss: 235.2510 - val_mae: 11.8009 - val_mse: 235.2510\n",
      "Epoch 848/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 3.0341 - mae: 1.2826 - mse: 3.0341 - val_loss: 251.6572 - val_mae: 12.0216 - val_mse: 251.6572\n",
      "Epoch 849/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.8555 - mae: 1.1155 - mse: 1.8555 - val_loss: 231.1503 - val_mae: 11.7192 - val_mse: 231.1503\n",
      "Epoch 850/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 2.4281 - mae: 1.2372 - mse: 2.4281 - val_loss: 235.4327 - val_mae: 11.7025 - val_mse: 235.4327\n",
      "Epoch 851/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.8918 - mae: 1.0553 - mse: 1.8918 - val_loss: 238.4308 - val_mae: 11.9891 - val_mse: 238.4308\n",
      "Epoch 852/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 1.6854 - mae: 1.0569 - mse: 1.6854 - val_loss: 250.5339 - val_mae: 12.0126 - val_mse: 250.5339\n",
      "Epoch 853/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.5741 - mae: 0.9896 - mse: 1.5741 - val_loss: 224.0471 - val_mae: 11.5545 - val_mse: 224.0471\n",
      "Epoch 854/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.5431 - mae: 0.8879 - mse: 1.5431 - val_loss: 245.3364 - val_mae: 11.7919 - val_mse: 245.3364\n",
      "Epoch 855/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.4249 - mae: 0.8897 - mse: 1.4249 - val_loss: 228.1178 - val_mae: 11.6075 - val_mse: 228.1178\n",
      "Epoch 856/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.6816 - mae: 0.9601 - mse: 1.6816 - val_loss: 233.8886 - val_mae: 11.8462 - val_mse: 233.8886\n",
      "Epoch 857/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.5316 - mae: 0.9456 - mse: 1.5316 - val_loss: 244.0252 - val_mae: 11.8039 - val_mse: 244.0252\n",
      "Epoch 858/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.4272 - mae: 0.9854 - mse: 1.4272 - val_loss: 226.6750 - val_mae: 11.6572 - val_mse: 226.6750\n",
      "Epoch 859/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.4160 - mae: 0.9033 - mse: 1.4160 - val_loss: 249.7597 - val_mae: 11.9872 - val_mse: 249.7597\n",
      "Epoch 860/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 1.5466 - mae: 0.9402 - mse: 1.5466 - val_loss: 230.9416 - val_mae: 11.6794 - val_mse: 230.9416\n",
      "Epoch 861/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.0606 - mae: 0.7929 - mse: 1.0606 - val_loss: 237.6664 - val_mae: 11.7939 - val_mse: 237.6664\n",
      "Epoch 862/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.4824 - mae: 0.8433 - mse: 1.4824 - val_loss: 236.2265 - val_mae: 11.8536 - val_mse: 236.2265\n",
      "Epoch 863/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.3888 - mae: 0.8606 - mse: 1.3888 - val_loss: 239.9557 - val_mae: 11.8385 - val_mse: 239.9557\n",
      "Epoch 864/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.8509 - mae: 0.7068 - mse: 0.8509 - val_loss: 236.6911 - val_mae: 11.8687 - val_mse: 236.6911\n",
      "Epoch 865/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.7399 - mae: 0.6521 - mse: 0.7399 - val_loss: 243.2273 - val_mae: 11.8383 - val_mse: 243.2273\n",
      "Epoch 866/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7083 - mae: 0.5647 - mse: 0.7083 - val_loss: 232.7003 - val_mae: 11.7118 - val_mse: 232.7003\n",
      "Epoch 867/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6308 - mae: 0.5363 - mse: 0.6308 - val_loss: 236.8603 - val_mae: 11.6971 - val_mse: 236.8603\n",
      "Epoch 868/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5649 - mae: 0.4567 - mse: 0.5649 - val_loss: 231.1836 - val_mae: 11.5802 - val_mse: 231.1836\n",
      "Epoch 869/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5607 - mae: 0.4662 - mse: 0.5607 - val_loss: 234.5926 - val_mae: 11.7430 - val_mse: 234.5926\n",
      "Epoch 870/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.5014 - mae: 0.4410 - mse: 0.5014 - val_loss: 235.6876 - val_mae: 11.7331 - val_mse: 235.6876\n",
      "Epoch 871/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4414 - mae: 0.4087 - mse: 0.4414 - val_loss: 234.9100 - val_mae: 11.8067 - val_mse: 234.9100\n",
      "Epoch 872/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4695 - mae: 0.4457 - mse: 0.4695 - val_loss: 236.8375 - val_mae: 11.7941 - val_mse: 236.8375\n",
      "Epoch 873/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.5759 - mae: 0.3418 - mse: 0.575 - 0s 71us/step - loss: 0.4228 - mae: 0.3754 - mse: 0.4228 - val_loss: 232.9657 - val_mae: 11.6911 - val_mse: 232.9657\n",
      "Epoch 874/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3805 - mae: 0.3677 - mse: 0.3805 - val_loss: 237.7307 - val_mae: 11.7597 - val_mse: 237.7307\n",
      "Epoch 875/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3349 - mae: 0.3163 - mse: 0.3349 - val_loss: 235.2073 - val_mae: 11.7428 - val_mse: 235.2073\n",
      "Epoch 876/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3458 - mae: 0.3116 - mse: 0.3458 - val_loss: 234.0651 - val_mae: 11.6842 - val_mse: 234.0651\n",
      "Epoch 877/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3594 - mae: 0.3060 - mse: 0.3594 - val_loss: 233.5952 - val_mae: 11.7054 - val_mse: 233.5952\n",
      "Epoch 878/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3936 - mae: 0.3385 - mse: 0.3936 - val_loss: 236.1789 - val_mae: 11.7604 - val_mse: 236.1789\n",
      "Epoch 879/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3684 - mae: 0.3387 - mse: 0.3684 - val_loss: 235.8267 - val_mae: 11.7314 - val_mse: 235.8267\n",
      "Epoch 880/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3222 - mae: 0.2717 - mse: 0.3222 - val_loss: 235.2981 - val_mae: 11.7396 - val_mse: 235.2981\n",
      "Epoch 881/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2966 - mae: 0.2397 - mse: 0.2966 - val_loss: 236.2674 - val_mae: 11.7454 - val_mse: 236.2674\n",
      "Epoch 882/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2988 - mae: 0.2383 - mse: 0.2988 - val_loss: 233.0094 - val_mae: 11.6929 - val_mse: 233.0094\n",
      "Epoch 883/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3385 - mae: 0.2946 - mse: 0.3385 - val_loss: 235.0070 - val_mae: 11.7349 - val_mse: 235.0070\n",
      "Epoch 884/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3109 - mae: 0.2725 - mse: 0.3109 - val_loss: 237.7558 - val_mae: 11.7852 - val_mse: 237.7558\n",
      "Epoch 885/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3758 - mae: 0.2870 - mse: 0.3758 - val_loss: 233.9614 - val_mae: 11.7554 - val_mse: 233.9614\n",
      "Epoch 886/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3815 - mae: 0.3165 - mse: 0.3815 - val_loss: 238.0764 - val_mae: 11.7198 - val_mse: 238.0764\n",
      "Epoch 887/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3829 - mae: 0.3178 - mse: 0.3829 - val_loss: 233.5455 - val_mae: 11.7176 - val_mse: 233.5455\n",
      "Epoch 888/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3294 - mae: 0.3147 - mse: 0.3294 - val_loss: 233.2218 - val_mae: 11.7043 - val_mse: 233.2218\n",
      "Epoch 889/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3384 - mae: 0.3129 - mse: 0.3384 - val_loss: 237.3443 - val_mae: 11.7792 - val_mse: 237.3443\n",
      "Epoch 890/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.3550 - mae: 0.3145 - mse: 0.3550 - val_loss: 237.0724 - val_mae: 11.7921 - val_mse: 237.0724\n",
      "Epoch 891/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.3241 - mae: 0.2826 - mse: 0.3241 - val_loss: 237.9015 - val_mae: 11.7787 - val_mse: 237.9015\n",
      "Epoch 892/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3925 - mae: 0.3458 - mse: 0.3925 - val_loss: 236.6070 - val_mae: 11.7287 - val_mse: 236.6070\n",
      "Epoch 893/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4368 - mae: 0.3913 - mse: 0.4368 - val_loss: 232.4538 - val_mae: 11.6531 - val_mse: 232.4538\n",
      "Epoch 894/3500\n",
      "126/126 [==============================] - 0s 142us/step - loss: 0.4878 - mae: 0.4422 - mse: 0.4878 - val_loss: 237.9121 - val_mae: 11.7531 - val_mse: 237.9121\n",
      "Epoch 895/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.6513 - mae: 0.5320 - mse: 0.6513 - val_loss: 235.9701 - val_mae: 11.7660 - val_mse: 235.9701\n",
      "Epoch 896/3500\n",
      "126/126 [==============================] - 0s 142us/step - loss: 0.5749 - mae: 0.5187 - mse: 0.5749 - val_loss: 233.8148 - val_mae: 11.7373 - val_mse: 233.8148\n",
      "Epoch 897/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.5671 - mae: 0.4752 - mse: 0.5671 - val_loss: 240.5529 - val_mae: 11.7949 - val_mse: 240.5529\n",
      "Epoch 898/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.7061 - mae: 0.5370 - mse: 0.7061 - val_loss: 230.4972 - val_mae: 11.6222 - val_mse: 230.4972\n",
      "Epoch 899/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.9030 - mae: 0.6701 - mse: 0.9030 - val_loss: 238.8605 - val_mae: 11.7518 - val_mse: 238.8605\n",
      "Epoch 900/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.9011 - mae: 0.7071 - mse: 0.9011 - val_loss: 239.1629 - val_mae: 11.8899 - val_mse: 239.1629\n",
      "Epoch 901/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.7724 - mae: 0.6421 - mse: 0.7724 - val_loss: 242.0430 - val_mae: 11.8640 - val_mse: 242.0430\n",
      "Epoch 902/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.8689 - mae: 0.6611 - mse: 0.8689 - val_loss: 226.3063 - val_mae: 11.6263 - val_mse: 226.3063\n",
      "Epoch 903/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.9608 - mae: 0.7225 - mse: 0.9608 - val_loss: 245.2455 - val_mae: 11.8692 - val_mse: 245.2455\n",
      "Epoch 904/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.0756 - mae: 0.7159 - mse: 1.0756 - val_loss: 228.0152 - val_mae: 11.5915 - val_mse: 228.0152\n",
      "Epoch 905/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.8987 - mae: 0.7216 - mse: 0.8987 - val_loss: 236.1316 - val_mae: 11.8621 - val_mse: 236.1316\n",
      "Epoch 906/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.9803 - mae: 0.7013 - mse: 0.9803 - val_loss: 242.2280 - val_mae: 11.8931 - val_mse: 242.2280\n",
      "Epoch 907/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.8172 - mae: 0.6747 - mse: 0.8172 - val_loss: 221.5549 - val_mae: 11.4396 - val_mse: 221.5549\n",
      "Epoch 908/3500\n",
      "126/126 [==============================] - 0s 237us/step - loss: 0.8025 - mae: 0.6431 - mse: 0.8025 - val_loss: 231.1467 - val_mae: 11.6368 - val_mse: 231.1467\n",
      "Epoch 909/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.8569 - mae: 0.6704 - mse: 0.8569 - val_loss: 237.7838 - val_mae: 11.7783 - val_mse: 237.7838\n",
      "Epoch 910/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.9247 - mae: 0.7082 - mse: 0.9247 - val_loss: 237.1178 - val_mae: 11.8022 - val_mse: 237.1178\n",
      "Epoch 911/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7417 - mae: 0.6234 - mse: 0.7417 - val_loss: 241.2605 - val_mae: 11.9661 - val_mse: 241.2605\n",
      "Epoch 912/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.7766 - mae: 0.6195 - mse: 0.7766 - val_loss: 233.2138 - val_mae: 11.6085 - val_mse: 233.2138\n",
      "Epoch 913/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.1080 - mae: 0.7788 - mse: 1.1080 - val_loss: 240.9929 - val_mae: 11.7766 - val_mse: 240.9929\n",
      "Epoch 914/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.2652 - mae: 0.8177 - mse: 1.2652 - val_loss: 235.9176 - val_mae: 11.8962 - val_mse: 235.9176\n",
      "Epoch 915/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.9525 - mae: 0.7620 - mse: 0.9525 - val_loss: 242.0752 - val_mae: 11.7667 - val_mse: 242.0752\n",
      "Epoch 916/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.7710 - mae: 0.6613 - mse: 0.7710 - val_loss: 230.1293 - val_mae: 11.6820 - val_mse: 230.1293\n",
      "Epoch 917/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.8093 - mae: 0.6248 - mse: 0.8093 - val_loss: 229.1729 - val_mae: 11.5838 - val_mse: 229.1729\n",
      "Epoch 918/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5081 - mae: 0.5609 - mse: 0.5081 - val_loss: 232.0551 - val_mae: 11.6969 - val_mse: 232.0551\n",
      "Epoch 919/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6912 - mae: 0.5924 - mse: 0.6912 - val_loss: 248.2060 - val_mae: 11.9173 - val_mse: 248.2060\n",
      "Epoch 920/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6737 - mae: 0.5626 - mse: 0.6737 - val_loss: 233.2144 - val_mae: 11.7871 - val_mse: 233.2144\n",
      "Epoch 921/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6102 - mae: 0.5507 - mse: 0.6102 - val_loss: 236.6448 - val_mae: 11.6798 - val_mse: 236.6448\n",
      "Epoch 922/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5513 - mae: 0.5167 - mse: 0.5513 - val_loss: 233.8304 - val_mae: 11.7391 - val_mse: 233.8304\n",
      "Epoch 923/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.7507 - mae: 0.6254 - mse: 0.7507 - val_loss: 236.9725 - val_mae: 11.7760 - val_mse: 236.9725\n",
      "Epoch 924/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6765 - mae: 0.4799 - mse: 0.6765 - val_loss: 234.8735 - val_mae: 11.7349 - val_mse: 234.8735\n",
      "Epoch 925/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5737 - mae: 0.4630 - mse: 0.5737 - val_loss: 237.1651 - val_mae: 11.8512 - val_mse: 237.1651\n",
      "Epoch 926/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4975 - mae: 0.4261 - mse: 0.4975 - val_loss: 236.4022 - val_mae: 11.7122 - val_mse: 236.4022\n",
      "Epoch 927/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4115 - mae: 0.3679 - mse: 0.4115 - val_loss: 234.1338 - val_mae: 11.6715 - val_mse: 234.1338\n",
      "Epoch 928/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4357 - mae: 0.3882 - mse: 0.4357 - val_loss: 234.7978 - val_mae: 11.6777 - val_mse: 234.7978\n",
      "Epoch 929/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3159 - mae: 0.3086 - mse: 0.3159 - val_loss: 230.1437 - val_mae: 11.6124 - val_mse: 230.1437\n",
      "Epoch 930/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4794 - mae: 0.3901 - mse: 0.4794 - val_loss: 236.9498 - val_mae: 11.7778 - val_mse: 236.9498\n",
      "Epoch 931/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4608 - mae: 0.4172 - mse: 0.4608 - val_loss: 237.5680 - val_mae: 11.7944 - val_mse: 237.5680\n",
      "Epoch 932/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3383 - mae: 0.3186 - mse: 0.3383 - val_loss: 233.2786 - val_mae: 11.6796 - val_mse: 233.2786\n",
      "Epoch 933/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4476 - mae: 0.3880 - mse: 0.4476 - val_loss: 236.5421 - val_mae: 11.7377 - val_mse: 236.5421\n",
      "Epoch 934/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3753 - mae: 0.3001 - mse: 0.3753 - val_loss: 238.1793 - val_mae: 11.8058 - val_mse: 238.1793\n",
      "Epoch 935/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3482 - mae: 0.3037 - mse: 0.3482 - val_loss: 233.3625 - val_mae: 11.7442 - val_mse: 233.3625\n",
      "Epoch 936/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3684 - mae: 0.2908 - mse: 0.3684 - val_loss: 239.1446 - val_mae: 11.7882 - val_mse: 239.1446\n",
      "Epoch 937/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4472 - mae: 0.3818 - mse: 0.4472 - val_loss: 236.5514 - val_mae: 11.7993 - val_mse: 236.5514\n",
      "Epoch 938/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5307 - mae: 0.4133 - mse: 0.5307 - val_loss: 232.5717 - val_mae: 11.6486 - val_mse: 232.5717\n",
      "Epoch 939/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4115 - mae: 0.3982 - mse: 0.4115 - val_loss: 235.5996 - val_mae: 11.7285 - val_mse: 235.5996\n",
      "Epoch 940/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3952 - mae: 0.3650 - mse: 0.3952 - val_loss: 238.6321 - val_mae: 11.7993 - val_mse: 238.6321\n",
      "Epoch 941/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4393 - mae: 0.3666 - mse: 0.4393 - val_loss: 232.6208 - val_mae: 11.6733 - val_mse: 232.6208\n",
      "Epoch 942/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3404 - mae: 0.3256 - mse: 0.3404 - val_loss: 238.4882 - val_mae: 11.7817 - val_mse: 238.4882\n",
      "Epoch 943/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4192 - mae: 0.3248 - mse: 0.4192 - val_loss: 237.0466 - val_mae: 11.7471 - val_mse: 237.0466\n",
      "Epoch 944/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3888 - mae: 0.3161 - mse: 0.3888 - val_loss: 233.7753 - val_mae: 11.6968 - val_mse: 233.7753\n",
      "Epoch 945/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3575 - mae: 0.3375 - mse: 0.3575 - val_loss: 239.5601 - val_mae: 11.7947 - val_mse: 239.5601\n",
      "Epoch 946/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3534 - mae: 0.3079 - mse: 0.3534 - val_loss: 232.9562 - val_mae: 11.6950 - val_mse: 232.9562\n",
      "Epoch 947/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3555 - mae: 0.3604 - mse: 0.3555 - val_loss: 236.5479 - val_mae: 11.7550 - val_mse: 236.5479\n",
      "Epoch 948/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4149 - mae: 0.3792 - mse: 0.4149 - val_loss: 236.5520 - val_mae: 11.7212 - val_mse: 236.5520\n",
      "Epoch 949/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3577 - mae: 0.3172 - mse: 0.3577 - val_loss: 233.3570 - val_mae: 11.7085 - val_mse: 233.3570\n",
      "Epoch 950/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4312 - mae: 0.3606 - mse: 0.4312 - val_loss: 241.6183 - val_mae: 11.8059 - val_mse: 241.6183\n",
      "Epoch 951/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3983 - mae: 0.3844 - mse: 0.3983 - val_loss: 230.4100 - val_mae: 11.6657 - val_mse: 230.4100\n",
      "Epoch 952/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5718 - mae: 0.4589 - mse: 0.5718 - val_loss: 241.0783 - val_mae: 11.8063 - val_mse: 241.0783\n",
      "Epoch 953/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.7103 - mae: 0.5800 - mse: 0.7103 - val_loss: 239.2825 - val_mae: 11.7678 - val_mse: 239.2825\n",
      "Epoch 954/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5921 - mae: 0.5249 - mse: 0.5921 - val_loss: 228.9521 - val_mae: 11.6129 - val_mse: 228.9521\n",
      "Epoch 955/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.5846 - mae: 0.5270 - mse: 0.5846 - val_loss: 236.1188 - val_mae: 11.6958 - val_mse: 236.1188\n",
      "Epoch 956/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4384 - mae: 0.3874 - mse: 0.4384 - val_loss: 236.9086 - val_mae: 11.7636 - val_mse: 236.9086\n",
      "Epoch 957/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4001 - mae: 0.3802 - mse: 0.4001 - val_loss: 236.1725 - val_mae: 11.7423 - val_mse: 236.1725\n",
      "Epoch 958/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.3926 - mae: 0.3897 - mse: 0.3926 - val_loss: 235.8279 - val_mae: 11.7608 - val_mse: 235.8279\n",
      "Epoch 959/3500\n",
      "126/126 [==============================] - 0s 150us/step - loss: 0.4795 - mae: 0.4352 - mse: 0.4795 - val_loss: 239.2218 - val_mae: 11.7785 - val_mse: 239.2218\n",
      "Epoch 960/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.3828 - mae: 0.3698 - mse: 0.3828 - val_loss: 232.4612 - val_mae: 11.7072 - val_mse: 232.4612\n",
      "Epoch 961/3500\n",
      "126/126 [==============================] - 0s 166us/step - loss: 0.3545 - mae: 0.3546 - mse: 0.3545 - val_loss: 235.9072 - val_mae: 11.7080 - val_mse: 235.9072\n",
      "Epoch 962/3500\n",
      "126/126 [==============================] - 0s 158us/step - loss: 0.4092 - mae: 0.3638 - mse: 0.4092 - val_loss: 238.5019 - val_mae: 11.7742 - val_mse: 238.5019\n",
      "Epoch 963/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.3427 - mae: 0.2958 - mse: 0.3427 - val_loss: 234.7455 - val_mae: 11.7557 - val_mse: 234.7455\n",
      "Epoch 964/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.3114 - mae: 0.2893 - mse: 0.3114 - val_loss: 237.1407 - val_mae: 11.7209 - val_mse: 237.1407\n",
      "Epoch 965/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4074 - mae: 0.3202 - mse: 0.4074 - val_loss: 235.2560 - val_mae: 11.7186 - val_mse: 235.2560\n",
      "Epoch 966/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3163 - mae: 0.2709 - mse: 0.3163 - val_loss: 235.5591 - val_mae: 11.7720 - val_mse: 235.5591\n",
      "Epoch 967/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3760 - mae: 0.3244 - mse: 0.3760 - val_loss: 239.0562 - val_mae: 11.7787 - val_mse: 239.0562\n",
      "Epoch 968/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4089 - mae: 0.3668 - mse: 0.4089 - val_loss: 234.7714 - val_mae: 11.7540 - val_mse: 234.7714\n",
      "Epoch 969/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.3699 - mae: 0.3337 - mse: 0.3699 - val_loss: 237.6358 - val_mae: 11.7516 - val_mse: 237.6358\n",
      "Epoch 970/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.3429 - mae: 0.3292 - mse: 0.3429 - val_loss: 235.2168 - val_mae: 11.7614 - val_mse: 235.2168\n",
      "Epoch 971/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.3752 - mae: 0.3345 - mse: 0.3752 - val_loss: 237.7092 - val_mae: 11.7603 - val_mse: 237.7092\n",
      "Epoch 972/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3462 - mae: 0.3214 - mse: 0.3462 - val_loss: 234.7714 - val_mae: 11.7674 - val_mse: 234.7714\n",
      "Epoch 973/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.3476 - mae: 0.3142 - mse: 0.3476 - val_loss: 238.5390 - val_mae: 11.7632 - val_mse: 238.5390\n",
      "Epoch 974/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3619 - mae: 0.3055 - mse: 0.3619 - val_loss: 237.5328 - val_mae: 11.7657 - val_mse: 237.5328\n",
      "Epoch 975/3500\n",
      "126/126 [==============================] - 0s 143us/step - loss: 0.3298 - mae: 0.2992 - mse: 0.3298 - val_loss: 236.6915 - val_mae: 11.7814 - val_mse: 236.6915\n",
      "Epoch 976/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3055 - mae: 0.2673 - mse: 0.3055 - val_loss: 237.9805 - val_mae: 11.7589 - val_mse: 237.9805\n",
      "Epoch 977/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3034 - mae: 0.2569 - mse: 0.3034 - val_loss: 237.1189 - val_mae: 11.7748 - val_mse: 237.1189\n",
      "Epoch 978/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3175 - mae: 0.2494 - mse: 0.3175 - val_loss: 235.5655 - val_mae: 11.7588 - val_mse: 235.5655\n",
      "Epoch 979/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2731 - mae: 0.2330 - mse: 0.2731 - val_loss: 236.6727 - val_mae: 11.7435 - val_mse: 236.6727\n",
      "Epoch 980/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3475 - mae: 0.2854 - mse: 0.3475 - val_loss: 239.4313 - val_mae: 11.8260 - val_mse: 239.4313\n",
      "Epoch 981/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3287 - mae: 0.2923 - mse: 0.3287 - val_loss: 235.0231 - val_mae: 11.7019 - val_mse: 235.0231\n",
      "Epoch 982/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4014 - mae: 0.3215 - mse: 0.4014 - val_loss: 239.8047 - val_mae: 11.7912 - val_mse: 239.8047\n",
      "Epoch 983/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3595 - mae: 0.3100 - mse: 0.3595 - val_loss: 236.0894 - val_mae: 11.7738 - val_mse: 236.0894\n",
      "Epoch 984/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4360 - mae: 0.4149 - mse: 0.4360 - val_loss: 238.5414 - val_mae: 11.7418 - val_mse: 238.5414\n",
      "Epoch 985/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4645 - mae: 0.4399 - mse: 0.4645 - val_loss: 239.3311 - val_mae: 11.8506 - val_mse: 239.3311\n",
      "Epoch 986/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5134 - mae: 0.4595 - mse: 0.5134 - val_loss: 237.0130 - val_mae: 11.7792 - val_mse: 237.0130\n",
      "Epoch 987/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4094 - mae: 0.4120 - mse: 0.4094 - val_loss: 237.4761 - val_mae: 11.7541 - val_mse: 237.4761\n",
      "Epoch 988/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4160 - mae: 0.3845 - mse: 0.4160 - val_loss: 235.1911 - val_mae: 11.6925 - val_mse: 235.1911\n",
      "Epoch 989/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3503 - mae: 0.3233 - mse: 0.3503 - val_loss: 236.5876 - val_mae: 11.7992 - val_mse: 236.5876\n",
      "Epoch 990/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.4059 - mae: 0.3159 - mse: 0.4059 - val_loss: 239.9562 - val_mae: 11.7866 - val_mse: 239.9562\n",
      "Epoch 991/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3333 - mae: 0.2704 - mse: 0.3333 - val_loss: 234.9885 - val_mae: 11.7474 - val_mse: 234.9885\n",
      "Epoch 992/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3160 - mae: 0.2529 - mse: 0.3160 - val_loss: 236.8929 - val_mae: 11.7525 - val_mse: 236.8929\n",
      "Epoch 993/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3553 - mae: 0.2516 - mse: 0.3553 - val_loss: 239.9280 - val_mae: 11.7952 - val_mse: 239.9280\n",
      "Epoch 994/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3279 - mae: 0.3012 - mse: 0.3279 - val_loss: 235.5383 - val_mae: 11.7891 - val_mse: 235.5383\n",
      "Epoch 995/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.3786 - mae: 0.3551 - mse: 0.3786 - val_loss: 243.2143 - val_mae: 11.8035 - val_mse: 243.2143\n",
      "Epoch 996/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6212 - mae: 0.5117 - mse: 0.6212 - val_loss: 234.4295 - val_mae: 11.7239 - val_mse: 234.4295\n",
      "Epoch 997/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5495 - mae: 0.4913 - mse: 0.5495 - val_loss: 235.9364 - val_mae: 11.7237 - val_mse: 235.9364\n",
      "Epoch 998/3500\n",
      "126/126 [==============================] - 0s 166us/step - loss: 0.7025 - mae: 0.5790 - mse: 0.7025 - val_loss: 236.2629 - val_mae: 11.7132 - val_mse: 236.2629\n",
      "Epoch 999/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5367 - mae: 0.4834 - mse: 0.5367 - val_loss: 239.2711 - val_mae: 11.8076 - val_mse: 239.2711\n",
      "Epoch 1000/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.4924 - mae: 0.4362 - mse: 0.4924 - val_loss: 241.0160 - val_mae: 11.8871 - val_mse: 241.0160\n",
      "Epoch 1001/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4536 - mae: 0.3928 - mse: 0.4536 - val_loss: 239.1211 - val_mae: 11.7889 - val_mse: 239.1211\n",
      "Epoch 1002/3500\n",
      "126/126 [==============================] - 0s 158us/step - loss: 0.4089 - mae: 0.3961 - mse: 0.4089 - val_loss: 236.3564 - val_mae: 11.7117 - val_mse: 236.3564\n",
      "Epoch 1003/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.3761 - mae: 0.3613 - mse: 0.3761 - val_loss: 235.2885 - val_mae: 11.7521 - val_mse: 235.2885\n",
      "Epoch 1004/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3652 - mae: 0.3378 - mse: 0.3652 - val_loss: 236.8422 - val_mae: 11.7287 - val_mse: 236.8422\n",
      "Epoch 1005/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4085 - mae: 0.3528 - mse: 0.4085 - val_loss: 236.0053 - val_mae: 11.7594 - val_mse: 236.0053\n",
      "Epoch 1006/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4165 - mae: 0.3904 - mse: 0.4165 - val_loss: 238.5607 - val_mae: 11.8209 - val_mse: 238.5607\n",
      "Epoch 1007/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.3904 - mae: 0.3834 - mse: 0.3904 - val_loss: 236.8228 - val_mae: 11.7029 - val_mse: 236.8228\n",
      "Epoch 1008/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3814 - mae: 0.3430 - mse: 0.3814 - val_loss: 236.8458 - val_mae: 11.7844 - val_mse: 236.8458\n",
      "Epoch 1009/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4063 - mae: 0.3431 - mse: 0.4063 - val_loss: 238.3151 - val_mae: 11.7856 - val_mse: 238.3151\n",
      "Epoch 1010/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3903 - mae: 0.3589 - mse: 0.3903 - val_loss: 238.9055 - val_mae: 11.8029 - val_mse: 238.9055\n",
      "Epoch 1011/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4172 - mae: 0.3776 - mse: 0.4172 - val_loss: 236.7030 - val_mae: 11.7432 - val_mse: 236.7030\n",
      "Epoch 1012/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3401 - mae: 0.3396 - mse: 0.3401 - val_loss: 239.9332 - val_mae: 11.7994 - val_mse: 239.9332\n",
      "Epoch 1013/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4291 - mae: 0.4148 - mse: 0.4291 - val_loss: 238.1812 - val_mae: 11.8314 - val_mse: 238.1812\n",
      "Epoch 1014/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4384 - mae: 0.3920 - mse: 0.4384 - val_loss: 238.5109 - val_mae: 11.7816 - val_mse: 238.5109\n",
      "Epoch 1015/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3541 - mae: 0.3217 - mse: 0.3541 - val_loss: 237.2553 - val_mae: 11.8058 - val_mse: 237.2553\n",
      "Epoch 1016/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3469 - mae: 0.3166 - mse: 0.3469 - val_loss: 237.8799 - val_mae: 11.7473 - val_mse: 237.8799\n",
      "Epoch 1017/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3221 - mae: 0.2864 - mse: 0.3221 - val_loss: 237.1561 - val_mae: 11.7495 - val_mse: 237.1561\n",
      "Epoch 1018/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3256 - mae: 0.2959 - mse: 0.3256 - val_loss: 239.7218 - val_mae: 11.8043 - val_mse: 239.7218\n",
      "Epoch 1019/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3372 - mae: 0.3080 - mse: 0.3372 - val_loss: 237.7857 - val_mae: 11.8055 - val_mse: 237.7857\n",
      "Epoch 1020/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3319 - mae: 0.2840 - mse: 0.3319 - val_loss: 237.4224 - val_mae: 11.7296 - val_mse: 237.4224\n",
      "Epoch 1021/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3359 - mae: 0.3062 - mse: 0.3359 - val_loss: 237.8570 - val_mae: 11.7694 - val_mse: 237.8570\n",
      "Epoch 1022/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3586 - mae: 0.3253 - mse: 0.3586 - val_loss: 237.1644 - val_mae: 11.7908 - val_mse: 237.1644\n",
      "Epoch 1023/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4154 - mae: 0.3834 - mse: 0.4154 - val_loss: 236.5454 - val_mae: 11.7221 - val_mse: 236.5454\n",
      "Epoch 1024/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4464 - mae: 0.4159 - mse: 0.4464 - val_loss: 237.3320 - val_mae: 11.7560 - val_mse: 237.3320\n",
      "Epoch 1025/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3787 - mae: 0.3538 - mse: 0.3787 - val_loss: 234.8681 - val_mae: 11.7011 - val_mse: 234.8681\n",
      "Epoch 1026/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4721 - mae: 0.4020 - mse: 0.4721 - val_loss: 242.9133 - val_mae: 11.8556 - val_mse: 242.9133\n",
      "Epoch 1027/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4886 - mae: 0.4804 - mse: 0.4886 - val_loss: 235.6602 - val_mae: 11.7923 - val_mse: 235.6602\n",
      "Epoch 1028/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5887 - mae: 0.5757 - mse: 0.5887 - val_loss: 238.4671 - val_mae: 11.6935 - val_mse: 238.4671\n",
      "Epoch 1029/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7549 - mae: 0.6566 - mse: 0.7549 - val_loss: 235.2752 - val_mae: 11.8736 - val_mse: 235.2752\n",
      "Epoch 1030/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.0394 - mae: 0.7781 - mse: 1.0394 - val_loss: 235.8920 - val_mae: 11.6987 - val_mse: 235.8920\n",
      "Epoch 1031/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.4062 - mae: 0.8883 - mse: 1.4062 - val_loss: 242.7840 - val_mae: 11.9738 - val_mse: 242.7840\n",
      "Epoch 1032/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.2656 - mae: 0.8179 - mse: 1.2656 - val_loss: 231.3674 - val_mae: 11.8242 - val_mse: 231.3674\n",
      "Epoch 1033/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.0972 - mae: 0.7801 - mse: 1.0972 - val_loss: 247.9724 - val_mae: 11.8673 - val_mse: 247.9724\n",
      "Epoch 1034/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.2889 - mae: 0.8450 - mse: 1.2889 - val_loss: 234.1995 - val_mae: 11.7239 - val_mse: 234.1995\n",
      "Epoch 1035/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.6557 - mae: 1.0030 - mse: 1.6557 - val_loss: 243.2312 - val_mae: 11.7868 - val_mse: 243.2312\n",
      "Epoch 1036/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.2231 - mae: 0.8672 - mse: 1.2231 - val_loss: 241.9159 - val_mae: 11.8600 - val_mse: 241.9159\n",
      "Epoch 1037/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.9289 - mae: 0.7612 - mse: 0.9289 - val_loss: 235.6366 - val_mae: 11.8660 - val_mse: 235.6366\n",
      "Epoch 1038/3500\n",
      "126/126 [==============================] - 0s 150us/step - loss: 0.9586 - mae: 0.7007 - mse: 0.9586 - val_loss: 242.2053 - val_mae: 12.0037 - val_mse: 242.2053\n",
      "Epoch 1039/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7715 - mae: 0.6180 - mse: 0.7715 - val_loss: 234.8653 - val_mae: 11.7559 - val_mse: 234.8653\n",
      "Epoch 1040/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.9303 - mae: 0.7038 - mse: 0.9303 - val_loss: 237.0100 - val_mae: 11.6948 - val_mse: 237.0100\n",
      "Epoch 1041/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7848 - mae: 0.6672 - mse: 0.7848 - val_loss: 245.0881 - val_mae: 11.9264 - val_mse: 245.0881\n",
      "Epoch 1042/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7966 - mae: 0.6610 - mse: 0.7966 - val_loss: 234.4503 - val_mae: 11.7321 - val_mse: 234.4503\n",
      "Epoch 1043/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6847 - mae: 0.5599 - mse: 0.6847 - val_loss: 237.5877 - val_mae: 11.7566 - val_mse: 237.5877\n",
      "Epoch 1044/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6408 - mae: 0.5303 - mse: 0.6408 - val_loss: 237.4293 - val_mae: 11.7452 - val_mse: 237.4293\n",
      "Epoch 1045/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4061 - mae: 0.3856 - mse: 0.4061 - val_loss: 237.5322 - val_mae: 11.7828 - val_mse: 237.5322\n",
      "Epoch 1046/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4934 - mae: 0.4633 - mse: 0.4934 - val_loss: 237.8434 - val_mae: 11.7520 - val_mse: 237.8434\n",
      "Epoch 1047/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5511 - mae: 0.4688 - mse: 0.5511 - val_loss: 240.8165 - val_mae: 11.8534 - val_mse: 240.8165\n",
      "Epoch 1048/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.6300 - mae: 0.5393 - mse: 0.6300 - val_loss: 239.5409 - val_mae: 11.8455 - val_mse: 239.5409\n",
      "Epoch 1049/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.6167 - mae: 0.5259 - mse: 0.6167 - val_loss: 230.7301 - val_mae: 11.6117 - val_mse: 230.7301\n",
      "Epoch 1050/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.5053 - mae: 0.4672 - mse: 0.5053 - val_loss: 244.3289 - val_mae: 11.8641 - val_mse: 244.3289\n",
      "Epoch 1051/3500\n",
      "126/126 [==============================] - 0s 142us/step - loss: 0.4723 - mae: 0.4431 - mse: 0.4723 - val_loss: 237.7571 - val_mae: 11.8214 - val_mse: 237.7571\n",
      "Epoch 1052/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5268 - mae: 0.4756 - mse: 0.5268 - val_loss: 239.9477 - val_mae: 11.7982 - val_mse: 239.9477\n",
      "Epoch 1053/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4390 - mae: 0.4203 - mse: 0.4390 - val_loss: 238.8222 - val_mae: 11.7969 - val_mse: 238.8222\n",
      "Epoch 1054/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3308 - mae: 0.3191 - mse: 0.3308 - val_loss: 235.2290 - val_mae: 11.7384 - val_mse: 235.2290\n",
      "Epoch 1055/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3877 - mae: 0.3670 - mse: 0.3877 - val_loss: 239.3701 - val_mae: 11.8166 - val_mse: 239.3701\n",
      "Epoch 1056/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3988 - mae: 0.4157 - mse: 0.3988 - val_loss: 236.9840 - val_mae: 11.7698 - val_mse: 236.9840\n",
      "Epoch 1057/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4183 - mae: 0.3836 - mse: 0.4183 - val_loss: 242.9407 - val_mae: 11.8635 - val_mse: 242.9407\n",
      "Epoch 1058/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4100 - mae: 0.3804 - mse: 0.4100 - val_loss: 232.2319 - val_mae: 11.6953 - val_mse: 232.2319\n",
      "Epoch 1059/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4157 - mae: 0.3955 - mse: 0.4157 - val_loss: 241.0752 - val_mae: 11.7836 - val_mse: 241.0752\n",
      "Epoch 1060/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3875 - mae: 0.3967 - mse: 0.3875 - val_loss: 241.6626 - val_mae: 11.9247 - val_mse: 241.6626\n",
      "Epoch 1061/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3659 - mae: 0.3630 - mse: 0.3659 - val_loss: 239.5220 - val_mae: 11.7708 - val_mse: 239.5220\n",
      "Epoch 1062/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3750 - mae: 0.3073 - mse: 0.3750 - val_loss: 235.6408 - val_mae: 11.7816 - val_mse: 235.6408\n",
      "Epoch 1063/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3576 - mae: 0.3180 - mse: 0.3576 - val_loss: 240.2334 - val_mae: 11.8457 - val_mse: 240.2334\n",
      "Epoch 1064/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3656 - mae: 0.3130 - mse: 0.3656 - val_loss: 239.1580 - val_mae: 11.8246 - val_mse: 239.1580\n",
      "Epoch 1065/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3900 - mae: 0.3182 - mse: 0.3900 - val_loss: 235.6288 - val_mae: 11.7900 - val_mse: 235.6288\n",
      "Epoch 1066/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3262 - mae: 0.3448 - mse: 0.3262 - val_loss: 237.9548 - val_mae: 11.7196 - val_mse: 237.9548\n",
      "Epoch 1067/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4020 - mae: 0.3518 - mse: 0.4020 - val_loss: 237.7569 - val_mae: 11.8403 - val_mse: 237.7569\n",
      "Epoch 1068/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4283 - mae: 0.3880 - mse: 0.4283 - val_loss: 238.9910 - val_mae: 11.8003 - val_mse: 238.9910\n",
      "Epoch 1069/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3078 - mae: 0.3199 - mse: 0.3078 - val_loss: 238.2621 - val_mae: 11.8096 - val_mse: 238.2621\n",
      "Epoch 1070/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4350 - mae: 0.4010 - mse: 0.4350 - val_loss: 242.3172 - val_mae: 11.8476 - val_mse: 242.3172\n",
      "Epoch 1071/3500\n",
      "126/126 [==============================] - 0s 64us/step - loss: 0.4795 - mae: 0.4244 - mse: 0.4795 - val_loss: 237.3611 - val_mae: 11.8275 - val_mse: 237.3611\n",
      "Epoch 1072/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4257 - mae: 0.3894 - mse: 0.4257 - val_loss: 241.3946 - val_mae: 11.8059 - val_mse: 241.3946\n",
      "Epoch 1073/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4796 - mae: 0.4412 - mse: 0.4796 - val_loss: 240.0485 - val_mae: 11.8430 - val_mse: 240.0485\n",
      "Epoch 1074/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5828 - mae: 0.5270 - mse: 0.5828 - val_loss: 233.4452 - val_mae: 11.7106 - val_mse: 233.4452\n",
      "Epoch 1075/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5267 - mae: 0.4672 - mse: 0.5267 - val_loss: 241.5571 - val_mae: 11.8125 - val_mse: 241.5571\n",
      "Epoch 1076/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4409 - mae: 0.3939 - mse: 0.4409 - val_loss: 239.2221 - val_mae: 11.8404 - val_mse: 239.2221\n",
      "Epoch 1077/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4532 - mae: 0.4287 - mse: 0.4532 - val_loss: 242.7423 - val_mae: 11.9087 - val_mse: 242.7423\n",
      "Epoch 1078/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5490 - mae: 0.5034 - mse: 0.5490 - val_loss: 240.1846 - val_mae: 11.7958 - val_mse: 240.1846\n",
      "Epoch 1079/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5068 - mae: 0.4859 - mse: 0.5068 - val_loss: 237.3874 - val_mae: 11.7637 - val_mse: 237.3874\n",
      "Epoch 1080/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5679 - mae: 0.5466 - mse: 0.5679 - val_loss: 243.4251 - val_mae: 11.8493 - val_mse: 243.4251\n",
      "Epoch 1081/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6108 - mae: 0.5142 - mse: 0.6108 - val_loss: 240.7525 - val_mae: 11.7965 - val_mse: 240.7525\n",
      "Epoch 1082/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5338 - mae: 0.4653 - mse: 0.5337 - val_loss: 235.1870 - val_mae: 11.7653 - val_mse: 235.1870\n",
      "Epoch 1083/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5277 - mae: 0.5003 - mse: 0.5277 - val_loss: 239.4842 - val_mae: 11.7448 - val_mse: 239.4842\n",
      "Epoch 1084/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.6012 - mae: 0.5869 - mse: 0.6012 - val_loss: 236.4577 - val_mae: 11.8412 - val_mse: 236.4577\n",
      "Epoch 1085/3500\n",
      "126/126 [==============================] - 0s 47us/step - loss: 0.6178 - mae: 0.5581 - mse: 0.6178 - val_loss: 242.7326 - val_mae: 11.8059 - val_mse: 242.7326\n",
      "Epoch 1086/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.6032 - mae: 0.5400 - mse: 0.6032 - val_loss: 236.7645 - val_mae: 11.8114 - val_mse: 236.7645\n",
      "Epoch 1087/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6009 - mae: 0.5460 - mse: 0.6009 - val_loss: 243.9235 - val_mae: 11.8878 - val_mse: 243.9235\n",
      "Epoch 1088/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5162 - mae: 0.5028 - mse: 0.5162 - val_loss: 234.3252 - val_mae: 11.6819 - val_mse: 234.3252\n",
      "Epoch 1089/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4312 - mae: 0.4160 - mse: 0.4312 - val_loss: 239.7417 - val_mae: 11.8644 - val_mse: 239.7417\n",
      "Epoch 1090/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5063 - mae: 0.4792 - mse: 0.5063 - val_loss: 239.2100 - val_mae: 11.8122 - val_mse: 239.2100\n",
      "Epoch 1091/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4516 - mae: 0.4433 - mse: 0.4516 - val_loss: 238.8536 - val_mae: 11.7696 - val_mse: 238.8536\n",
      "Epoch 1092/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3737 - mae: 0.3649 - mse: 0.3737 - val_loss: 235.5293 - val_mae: 11.7131 - val_mse: 235.5293\n",
      "Epoch 1093/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3729 - mae: 0.3610 - mse: 0.3729 - val_loss: 235.3980 - val_mae: 11.7471 - val_mse: 235.3980\n",
      "Epoch 1094/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3944 - mae: 0.3404 - mse: 0.3944 - val_loss: 241.0009 - val_mae: 11.8260 - val_mse: 241.0009\n",
      "Epoch 1095/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3607 - mae: 0.3127 - mse: 0.3607 - val_loss: 236.8072 - val_mae: 11.7709 - val_mse: 236.8072\n",
      "Epoch 1096/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3492 - mae: 0.3241 - mse: 0.3492 - val_loss: 240.7184 - val_mae: 11.8114 - val_mse: 240.7184\n",
      "Epoch 1097/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4095 - mae: 0.3872 - mse: 0.4095 - val_loss: 237.8029 - val_mae: 11.7804 - val_mse: 237.8029\n",
      "Epoch 1098/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3599 - mae: 0.3445 - mse: 0.3599 - val_loss: 236.6223 - val_mae: 11.7606 - val_mse: 236.6223\n",
      "Epoch 1099/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4514 - mae: 0.3996 - mse: 0.4514 - val_loss: 239.7074 - val_mae: 11.8116 - val_mse: 239.7074\n",
      "Epoch 1100/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3691 - mae: 0.3622 - mse: 0.3691 - val_loss: 238.0021 - val_mae: 11.8076 - val_mse: 238.0021\n",
      "Epoch 1101/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.4134 - mae: 0.3790 - mse: 0.4134 - val_loss: 238.5811 - val_mae: 11.7065 - val_mse: 238.5811\n",
      "Epoch 1102/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.5757 - mae: 0.5050 - mse: 0.5757 - val_loss: 235.8117 - val_mae: 11.7503 - val_mse: 235.8117\n",
      "Epoch 1103/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.7034 - mae: 0.6397 - mse: 0.7034 - val_loss: 237.6644 - val_mae: 11.7490 - val_mse: 237.6644\n",
      "Epoch 1104/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4572 - mae: 0.4750 - mse: 0.4572 - val_loss: 238.0696 - val_mae: 11.8372 - val_mse: 238.0696\n",
      "Epoch 1105/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5822 - mae: 0.5308 - mse: 0.5822 - val_loss: 238.1981 - val_mae: 11.7730 - val_mse: 238.1981\n",
      "Epoch 1106/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4623 - mae: 0.4491 - mse: 0.4623 - val_loss: 237.9289 - val_mae: 11.7234 - val_mse: 237.9289\n",
      "Epoch 1107/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4459 - mae: 0.3971 - mse: 0.4459 - val_loss: 237.2923 - val_mae: 11.7850 - val_mse: 237.2923\n",
      "Epoch 1108/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4176 - mae: 0.3962 - mse: 0.4176 - val_loss: 237.8780 - val_mae: 11.7661 - val_mse: 237.8780\n",
      "Epoch 1109/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4095 - mae: 0.3859 - mse: 0.4095 - val_loss: 239.3284 - val_mae: 11.8813 - val_mse: 239.3284\n",
      "Epoch 1110/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4587 - mae: 0.4139 - mse: 0.4587 - val_loss: 236.0636 - val_mae: 11.7498 - val_mse: 236.0636\n",
      "Epoch 1111/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3753 - mae: 0.3376 - mse: 0.3753 - val_loss: 241.2808 - val_mae: 11.7976 - val_mse: 241.2808\n",
      "Epoch 1112/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3561 - mae: 0.3359 - mse: 0.3561 - val_loss: 235.3395 - val_mae: 11.7434 - val_mse: 235.3395\n",
      "Epoch 1113/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4135 - mae: 0.3800 - mse: 0.4135 - val_loss: 238.6870 - val_mae: 11.7562 - val_mse: 238.6870\n",
      "Epoch 1114/3500\n",
      "126/126 [==============================] - 0s 206us/step - loss: 0.3494 - mae: 0.3314 - mse: 0.3494 - val_loss: 238.1478 - val_mae: 11.7919 - val_mse: 238.1478\n",
      "Epoch 1115/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.3851 - mae: 0.3630 - mse: 0.3851 - val_loss: 233.9624 - val_mae: 11.7442 - val_mse: 233.9624\n",
      "Epoch 1116/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.3184 - mae: 0.3304 - mse: 0.3184 - val_loss: 245.7592 - val_mae: 11.9487 - val_mse: 245.7592\n",
      "Epoch 1117/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.4204 - mae: 0.3710 - mse: 0.4204 - val_loss: 235.7557 - val_mae: 11.7323 - val_mse: 235.7557\n",
      "Epoch 1118/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4085 - mae: 0.3638 - mse: 0.4085 - val_loss: 239.2110 - val_mae: 11.7837 - val_mse: 239.2110\n",
      "Epoch 1119/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3240 - mae: 0.3043 - mse: 0.3240 - val_loss: 237.3767 - val_mae: 11.7896 - val_mse: 237.3767\n",
      "Epoch 1120/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3074 - mae: 0.2831 - mse: 0.3074 - val_loss: 238.9189 - val_mae: 11.7872 - val_mse: 238.9189\n",
      "Epoch 1121/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3230 - mae: 0.2853 - mse: 0.3230 - val_loss: 238.4158 - val_mae: 11.8370 - val_mse: 238.4158\n",
      "Epoch 1122/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3571 - mae: 0.3558 - mse: 0.3571 - val_loss: 239.3815 - val_mae: 11.7481 - val_mse: 239.3815\n",
      "Epoch 1123/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4973 - mae: 0.4170 - mse: 0.4973 - val_loss: 236.3260 - val_mae: 11.7772 - val_mse: 236.3260\n",
      "Epoch 1124/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4881 - mae: 0.4209 - mse: 0.4881 - val_loss: 237.9893 - val_mae: 11.7820 - val_mse: 237.9893\n",
      "Epoch 1125/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4896 - mae: 0.4055 - mse: 0.4896 - val_loss: 242.8990 - val_mae: 11.8559 - val_mse: 242.8990\n",
      "Epoch 1126/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4922 - mae: 0.4579 - mse: 0.4922 - val_loss: 238.1097 - val_mae: 11.7708 - val_mse: 238.1097\n",
      "Epoch 1127/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4703 - mae: 0.4513 - mse: 0.4703 - val_loss: 237.1517 - val_mae: 11.7571 - val_mse: 237.1517\n",
      "Epoch 1128/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4667 - mae: 0.4627 - mse: 0.4667 - val_loss: 239.5869 - val_mae: 11.7130 - val_mse: 239.5869\n",
      "Epoch 1129/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.7557 - mae: 0.6341 - mse: 0.7557 - val_loss: 235.6235 - val_mae: 11.7929 - val_mse: 235.6235\n",
      "Epoch 1130/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.7353 - mae: 0.6547 - mse: 0.7353 - val_loss: 238.6203 - val_mae: 11.7011 - val_mse: 238.6203\n",
      "Epoch 1131/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.7642 - mae: 0.6428 - mse: 0.7642 - val_loss: 231.0208 - val_mae: 11.6706 - val_mse: 231.0208\n",
      "Epoch 1132/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.7583 - mae: 0.6294 - mse: 0.7583 - val_loss: 239.4926 - val_mae: 11.8047 - val_mse: 239.4926\n",
      "Epoch 1133/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 1.1653 - mae: 0.7868 - mse: 1.1653 - val_loss: 241.3813 - val_mae: 11.7697 - val_mse: 241.3813\n",
      "Epoch 1134/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.2821 - mae: 0.9129 - mse: 1.2821 - val_loss: 235.2972 - val_mae: 11.9064 - val_mse: 235.2972\n",
      "Epoch 1135/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.0724 - mae: 0.8081 - mse: 1.0724 - val_loss: 256.5295 - val_mae: 11.9556 - val_mse: 256.5295\n",
      "Epoch 1136/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.7073 - mae: 1.0242 - mse: 1.7073 - val_loss: 237.3420 - val_mae: 11.8396 - val_mse: 237.3420\n",
      "Epoch 1137/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.8958 - mae: 0.9945 - mse: 1.8958 - val_loss: 238.0053 - val_mae: 11.7270 - val_mse: 238.0053\n",
      "Epoch 1138/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.8452 - mae: 0.9754 - mse: 1.8452 - val_loss: 240.0869 - val_mae: 11.8566 - val_mse: 240.0869\n",
      "Epoch 1139/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.4565 - mae: 0.9005 - mse: 1.4565 - val_loss: 236.0025 - val_mae: 11.9867 - val_mse: 236.0025\n",
      "Epoch 1140/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.4442 - mae: 0.9757 - mse: 1.4442 - val_loss: 246.7377 - val_mae: 11.9358 - val_mse: 246.7377\n",
      "Epoch 1141/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.7568 - mae: 1.0297 - mse: 1.7568 - val_loss: 236.0291 - val_mae: 11.8943 - val_mse: 236.0291\n",
      "Epoch 1142/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.6325 - mae: 0.9696 - mse: 1.6325 - val_loss: 245.5262 - val_mae: 11.8843 - val_mse: 245.5262\n",
      "Epoch 1143/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.1448 - mae: 0.8556 - mse: 1.1448 - val_loss: 242.3034 - val_mae: 11.9075 - val_mse: 242.3034\n",
      "Epoch 1144/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.7646 - mae: 0.6471 - mse: 0.7646 - val_loss: 242.3561 - val_mae: 11.8492 - val_mse: 242.3561\n",
      "Epoch 1145/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.9926 - mae: 0.7253 - mse: 0.9926 - val_loss: 228.4590 - val_mae: 11.5100 - val_mse: 228.4590\n",
      "Epoch 1146/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.9722 - mae: 0.7295 - mse: 0.9722 - val_loss: 241.7351 - val_mae: 11.8345 - val_mse: 241.7351\n",
      "Epoch 1147/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.8506 - mae: 0.6620 - mse: 0.8506 - val_loss: 236.9782 - val_mae: 11.7694 - val_mse: 236.9782\n",
      "Epoch 1148/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.8593 - mae: 0.7091 - mse: 0.8593 - val_loss: 241.9264 - val_mae: 11.7966 - val_mse: 241.9264\n",
      "Epoch 1149/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.9745 - mae: 0.6956 - mse: 0.9745 - val_loss: 238.8761 - val_mae: 11.8304 - val_mse: 238.8761\n",
      "Epoch 1150/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.8111 - mae: 0.6883 - mse: 0.8111 - val_loss: 237.4905 - val_mae: 11.6825 - val_mse: 237.4905\n",
      "Epoch 1151/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 1.1304 - mae: 0.8209 - mse: 1.1304 - val_loss: 234.6867 - val_mae: 11.8235 - val_mse: 234.6867\n",
      "Epoch 1152/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 1.4784 - mae: 0.9293 - mse: 1.4784 - val_loss: 262.0648 - val_mae: 12.2085 - val_mse: 262.0648\n",
      "Epoch 1153/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 2.2697 - mae: 1.1192 - mse: 2.2697 - val_loss: 242.1040 - val_mae: 11.9068 - val_mse: 242.1040\n",
      "Epoch 1154/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 2.1403 - mae: 1.0807 - mse: 2.1403 - val_loss: 238.8797 - val_mae: 11.7068 - val_mse: 238.8797\n",
      "Epoch 1155/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 3.0385 - mae: 1.3012 - mse: 3.0385 - val_loss: 248.5496 - val_mae: 11.9619 - val_mse: 248.5496\n",
      "Epoch 1156/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 2.5894 - mae: 1.1515 - mse: 2.5894 - val_loss: 234.4627 - val_mae: 11.8240 - val_mse: 234.4627\n",
      "Epoch 1157/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 2.1922 - mae: 1.0685 - mse: 2.1922 - val_loss: 248.3955 - val_mae: 12.0408 - val_mse: 248.3955\n",
      "Epoch 1158/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 2.2684 - mae: 1.1023 - mse: 2.2684 - val_loss: 229.4946 - val_mae: 11.5700 - val_mse: 229.4946\n",
      "Epoch 1159/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 3.4192 - mae: 1.2365 - mse: 3.4192 - val_loss: 240.1676 - val_mae: 11.8921 - val_mse: 240.1676\n",
      "Epoch 1160/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 5.4770 - mae: 1.6920 - mse: 5.4770 - val_loss: 250.7000 - val_mae: 12.1492 - val_mse: 250.7000\n",
      "Epoch 1161/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 5.1414 - mae: 1.7228 - mse: 5.1414 - val_loss: 222.3445 - val_mae: 11.5873 - val_mse: 222.3445\n",
      "Epoch 1162/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 5.6288 - mae: 1.9122 - mse: 5.6288 - val_loss: 229.7154 - val_mae: 11.6684 - val_mse: 229.7154\n",
      "Epoch 1163/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 5.5142 - mae: 1.7190 - mse: 5.5142 - val_loss: 274.0919 - val_mae: 12.4216 - val_mse: 274.0919\n",
      "Epoch 1164/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 3.9603 - mae: 1.5380 - mse: 3.9603 - val_loss: 236.2030 - val_mae: 11.9814 - val_mse: 236.2030\n",
      "Epoch 1165/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 4.0897 - mae: 1.6707 - mse: 4.0897 - val_loss: 238.7088 - val_mae: 11.6854 - val_mse: 238.7088\n",
      "Epoch 1166/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 3.0897 - mae: 1.3618 - mse: 3.0897 - val_loss: 230.4015 - val_mae: 11.4377 - val_mse: 230.4015\n",
      "Epoch 1167/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.9347 - mae: 1.0875 - mse: 1.9347 - val_loss: 211.8806 - val_mae: 11.2832 - val_mse: 211.8806\n",
      "Epoch 1168/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.6767 - mae: 1.0148 - mse: 1.6767 - val_loss: 245.6600 - val_mae: 11.9301 - val_mse: 245.6600\n",
      "Epoch 1169/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.7217 - mae: 0.9404 - mse: 1.7217 - val_loss: 225.5553 - val_mae: 11.6255 - val_mse: 225.5553\n",
      "Epoch 1170/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.1279 - mae: 0.7983 - mse: 1.1279 - val_loss: 223.9030 - val_mae: 11.4008 - val_mse: 223.9030\n",
      "Epoch 1171/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.2037 - mae: 0.7889 - mse: 1.2037 - val_loss: 228.6629 - val_mae: 11.6173 - val_mse: 228.6629\n",
      "Epoch 1172/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 1.1883 - mae: 0.8582 - mse: 1.1883 - val_loss: 227.8072 - val_mae: 11.5646 - val_mse: 227.8072\n",
      "Epoch 1173/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.0900 - mae: 0.7496 - mse: 1.0900 - val_loss: 231.1316 - val_mae: 11.6430 - val_mse: 231.1316\n",
      "Epoch 1174/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.7833 - mae: 0.6359 - mse: 0.7833 - val_loss: 238.1555 - val_mae: 11.8734 - val_mse: 238.1555\n",
      "Epoch 1175/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.6950 - mae: 0.6074 - mse: 0.6950 - val_loss: 223.8611 - val_mae: 11.5434 - val_mse: 223.8611\n",
      "Epoch 1176/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6496 - mae: 0.5963 - mse: 0.6496 - val_loss: 233.8738 - val_mae: 11.6582 - val_mse: 233.8738\n",
      "Epoch 1177/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6464 - mae: 0.5639 - mse: 0.6464 - val_loss: 229.0968 - val_mae: 11.6302 - val_mse: 229.0968\n",
      "Epoch 1178/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5313 - mae: 0.4958 - mse: 0.5313 - val_loss: 228.3485 - val_mae: 11.5470 - val_mse: 228.3485\n",
      "Epoch 1179/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5676 - mae: 0.4994 - mse: 0.5676 - val_loss: 235.7144 - val_mae: 11.7830 - val_mse: 235.7144\n",
      "Epoch 1180/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5893 - mae: 0.5114 - mse: 0.5893 - val_loss: 239.3749 - val_mae: 11.7847 - val_mse: 239.3749\n",
      "Epoch 1181/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5217 - mae: 0.4376 - mse: 0.5217 - val_loss: 224.4241 - val_mae: 11.5587 - val_mse: 224.4241\n",
      "Epoch 1182/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.8115 - mae: 0.5788 - mse: 0.8115 - val_loss: 236.6790 - val_mae: 11.7937 - val_mse: 236.6790\n",
      "Epoch 1183/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.8854 - mae: 0.6618 - mse: 0.8854 - val_loss: 236.1119 - val_mae: 11.8056 - val_mse: 236.1119\n",
      "Epoch 1184/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6196 - mae: 0.5655 - mse: 0.6196 - val_loss: 232.8317 - val_mae: 11.6435 - val_mse: 232.8317\n",
      "Epoch 1185/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5924 - mae: 0.5204 - mse: 0.5924 - val_loss: 234.3323 - val_mae: 11.7417 - val_mse: 234.3323\n",
      "Epoch 1186/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.7281 - mae: 0.5264 - mse: 0.7281 - val_loss: 235.1898 - val_mae: 11.6659 - val_mse: 235.1898\n",
      "Epoch 1187/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.8048 - mae: 0.6077 - mse: 0.8048 - val_loss: 230.6135 - val_mae: 11.5928 - val_mse: 230.6135\n",
      "Epoch 1188/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7862 - mae: 0.6467 - mse: 0.7862 - val_loss: 231.7372 - val_mae: 11.8169 - val_mse: 231.7372\n",
      "Epoch 1189/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.8934 - mae: 0.6837 - mse: 0.8935 - val_loss: 241.7622 - val_mae: 11.7844 - val_mse: 241.7622\n",
      "Epoch 1190/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.9538 - mae: 0.7218 - mse: 0.9538 - val_loss: 232.5276 - val_mae: 11.7665 - val_mse: 232.5276\n",
      "Epoch 1191/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.9589 - mae: 0.7246 - mse: 0.9589 - val_loss: 232.8958 - val_mae: 11.7262 - val_mse: 232.8958\n",
      "Epoch 1192/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6836 - mae: 0.5975 - mse: 0.6836 - val_loss: 228.1670 - val_mae: 11.5806 - val_mse: 228.1670\n",
      "Epoch 1193/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.7950 - mae: 0.6199 - mse: 0.7950 - val_loss: 236.7381 - val_mae: 11.7391 - val_mse: 236.7381\n",
      "Epoch 1194/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5979 - mae: 0.5305 - mse: 0.5979 - val_loss: 228.3709 - val_mae: 11.6546 - val_mse: 228.3709\n",
      "Epoch 1195/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5961 - mae: 0.5148 - mse: 0.5961 - val_loss: 234.6293 - val_mae: 11.6858 - val_mse: 234.6293\n",
      "Epoch 1196/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6720 - mae: 0.5710 - mse: 0.6720 - val_loss: 239.5966 - val_mae: 11.8847 - val_mse: 239.5966\n",
      "Epoch 1197/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4627 - mae: 0.4555 - mse: 0.4627 - val_loss: 230.5410 - val_mae: 11.6953 - val_mse: 230.5410\n",
      "Epoch 1198/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6002 - mae: 0.4948 - mse: 0.6002 - val_loss: 230.7073 - val_mae: 11.6106 - val_mse: 230.7073\n",
      "Epoch 1199/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5155 - mae: 0.4577 - mse: 0.5155 - val_loss: 236.5376 - val_mae: 11.7561 - val_mse: 236.5376\n",
      "Epoch 1200/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5231 - mae: 0.4779 - mse: 0.5231 - val_loss: 234.5848 - val_mae: 11.7099 - val_mse: 234.5848\n",
      "Epoch 1201/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5568 - mae: 0.4815 - mse: 0.5568 - val_loss: 232.8163 - val_mae: 11.7150 - val_mse: 232.8163\n",
      "Epoch 1202/3500\n",
      "126/126 [==============================] - 0s 150us/step - loss: 0.4168 - mae: 0.4050 - mse: 0.4168 - val_loss: 237.3906 - val_mae: 11.7659 - val_mse: 237.3906\n",
      "Epoch 1203/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.4663 - mae: 0.4321 - mse: 0.4663 - val_loss: 224.5813 - val_mae: 11.5758 - val_mse: 224.5813\n",
      "Epoch 1204/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4592 - mae: 0.4212 - mse: 0.4592 - val_loss: 237.5504 - val_mae: 11.7471 - val_mse: 237.5504\n",
      "Epoch 1205/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5269 - mae: 0.4730 - mse: 0.5269 - val_loss: 235.3248 - val_mae: 11.7992 - val_mse: 235.3248\n",
      "Epoch 1206/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4576 - mae: 0.4119 - mse: 0.4576 - val_loss: 234.3130 - val_mae: 11.7408 - val_mse: 234.3130\n",
      "Epoch 1207/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4402 - mae: 0.4134 - mse: 0.4402 - val_loss: 233.7991 - val_mae: 11.7384 - val_mse: 233.7991\n",
      "Epoch 1208/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4510 - mae: 0.4270 - mse: 0.4510 - val_loss: 231.1347 - val_mae: 11.6384 - val_mse: 231.1347\n",
      "Epoch 1209/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4256 - mae: 0.3978 - mse: 0.4256 - val_loss: 233.7580 - val_mae: 11.7072 - val_mse: 233.7580\n",
      "Epoch 1210/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.4527 - mae: 0.4230 - mse: 0.4527 - val_loss: 232.4417 - val_mae: 11.6962 - val_mse: 232.4417\n",
      "Epoch 1211/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3735 - mae: 0.3633 - mse: 0.3735 - val_loss: 232.5695 - val_mae: 11.6575 - val_mse: 232.5695\n",
      "Epoch 1212/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3480 - mae: 0.3243 - mse: 0.3480 - val_loss: 232.0183 - val_mae: 11.7160 - val_mse: 232.0183\n",
      "Epoch 1213/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4744 - mae: 0.3889 - mse: 0.4744 - val_loss: 234.0173 - val_mae: 11.7206 - val_mse: 234.0173\n",
      "Epoch 1214/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4438 - mae: 0.4414 - mse: 0.4438 - val_loss: 234.5076 - val_mae: 11.7096 - val_mse: 234.5076\n",
      "Epoch 1215/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3910 - mae: 0.3668 - mse: 0.3910 - val_loss: 229.5528 - val_mae: 11.5768 - val_mse: 229.5528\n",
      "Epoch 1216/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4253 - mae: 0.3954 - mse: 0.4253 - val_loss: 231.7509 - val_mae: 11.6732 - val_mse: 231.7509\n",
      "Epoch 1217/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3633 - mae: 0.3192 - mse: 0.3633 - val_loss: 238.9064 - val_mae: 11.7964 - val_mse: 238.9064\n",
      "Epoch 1218/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3667 - mae: 0.3162 - mse: 0.3667 - val_loss: 228.1143 - val_mae: 11.6058 - val_mse: 228.1143\n",
      "Epoch 1219/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3696 - mae: 0.3488 - mse: 0.3696 - val_loss: 231.4686 - val_mae: 11.6699 - val_mse: 231.4686\n",
      "Epoch 1220/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3756 - mae: 0.3276 - mse: 0.3756 - val_loss: 234.6330 - val_mae: 11.7306 - val_mse: 234.6330\n",
      "Epoch 1221/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3805 - mae: 0.3181 - mse: 0.3805 - val_loss: 229.2936 - val_mae: 11.6137 - val_mse: 229.2936\n",
      "Epoch 1222/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3917 - mae: 0.3720 - mse: 0.3917 - val_loss: 236.1470 - val_mae: 11.7515 - val_mse: 236.1470\n",
      "Epoch 1223/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3211 - mae: 0.3106 - mse: 0.3211 - val_loss: 235.2207 - val_mae: 11.7715 - val_mse: 235.2207\n",
      "Epoch 1224/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3499 - mae: 0.3171 - mse: 0.3499 - val_loss: 233.0569 - val_mae: 11.6834 - val_mse: 233.0569\n",
      "Epoch 1225/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4075 - mae: 0.3413 - mse: 0.4075 - val_loss: 235.0638 - val_mae: 11.7499 - val_mse: 235.0638\n",
      "Epoch 1226/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.3297 - mae: 0.4134 - mse: 0.329 - 0s 95us/step - loss: 0.4138 - mae: 0.3787 - mse: 0.4138 - val_loss: 232.8972 - val_mae: 11.7234 - val_mse: 232.8972\n",
      "Epoch 1227/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3035 - mae: 0.2656 - mse: 0.3035 - val_loss: 233.9999 - val_mae: 11.7251 - val_mse: 233.9999\n",
      "Epoch 1228/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3508 - mae: 0.2942 - mse: 0.3508 - val_loss: 233.0103 - val_mae: 11.7146 - val_mse: 233.0103\n",
      "Epoch 1229/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4076 - mae: 0.3407 - mse: 0.4076 - val_loss: 232.6693 - val_mae: 11.6684 - val_mse: 232.6693\n",
      "Epoch 1230/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3438 - mae: 0.3206 - mse: 0.3438 - val_loss: 232.4584 - val_mae: 11.7032 - val_mse: 232.4584\n",
      "Epoch 1231/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3818 - mae: 0.3624 - mse: 0.3818 - val_loss: 234.7354 - val_mae: 11.7717 - val_mse: 234.7354\n",
      "Epoch 1232/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3462 - mae: 0.3315 - mse: 0.3462 - val_loss: 236.1398 - val_mae: 11.7361 - val_mse: 236.1398\n",
      "Epoch 1233/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3082 - mae: 0.2791 - mse: 0.3082 - val_loss: 229.9463 - val_mae: 11.6479 - val_mse: 229.9463\n",
      "Epoch 1234/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2826 - mae: 0.2482 - mse: 0.2826 - val_loss: 234.6901 - val_mae: 11.7050 - val_mse: 234.6901\n",
      "Epoch 1235/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3101 - mae: 0.2786 - mse: 0.3101 - val_loss: 233.0729 - val_mae: 11.7086 - val_mse: 233.0729\n",
      "Epoch 1236/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3696 - mae: 0.3137 - mse: 0.3696 - val_loss: 233.7641 - val_mae: 11.7336 - val_mse: 233.7641\n",
      "Epoch 1237/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3095 - mae: 0.3061 - mse: 0.3095 - val_loss: 235.7154 - val_mae: 11.7309 - val_mse: 235.7154\n",
      "Epoch 1238/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3368 - mae: 0.3068 - mse: 0.3368 - val_loss: 229.8082 - val_mae: 11.6470 - val_mse: 229.8082\n",
      "Epoch 1239/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3861 - mae: 0.3560 - mse: 0.3861 - val_loss: 233.7900 - val_mae: 11.7108 - val_mse: 233.7900\n",
      "Epoch 1240/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3984 - mae: 0.3830 - mse: 0.3984 - val_loss: 236.2073 - val_mae: 11.7787 - val_mse: 236.2073\n",
      "Epoch 1241/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3413 - mae: 0.3308 - mse: 0.3413 - val_loss: 233.0854 - val_mae: 11.7088 - val_mse: 233.0854\n",
      "Epoch 1242/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4298 - mae: 0.3552 - mse: 0.4298 - val_loss: 234.5065 - val_mae: 11.7654 - val_mse: 234.5065\n",
      "Epoch 1243/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3259 - mae: 0.3088 - mse: 0.3259 - val_loss: 233.6098 - val_mae: 11.7071 - val_mse: 233.6098\n",
      "Epoch 1244/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3499 - mae: 0.3082 - mse: 0.3499 - val_loss: 232.7826 - val_mae: 11.6640 - val_mse: 232.7826\n",
      "Epoch 1245/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3748 - mae: 0.3310 - mse: 0.3748 - val_loss: 235.2953 - val_mae: 11.7784 - val_mse: 235.2953\n",
      "Epoch 1246/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4262 - mae: 0.4023 - mse: 0.4262 - val_loss: 237.9711 - val_mae: 11.7694 - val_mse: 237.9711\n",
      "Epoch 1247/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4227 - mae: 0.3497 - mse: 0.4227 - val_loss: 230.4234 - val_mae: 11.6802 - val_mse: 230.4234\n",
      "Epoch 1248/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3975 - mae: 0.3638 - mse: 0.3975 - val_loss: 238.4030 - val_mae: 11.8036 - val_mse: 238.4030\n",
      "Epoch 1249/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.5182 - mae: 0.4129 - mse: 0.5182 - val_loss: 235.4675 - val_mae: 11.7088 - val_mse: 235.4675\n",
      "Epoch 1250/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4983 - mae: 0.4167 - mse: 0.4983 - val_loss: 231.4739 - val_mae: 11.6749 - val_mse: 231.4739\n",
      "Epoch 1251/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4296 - mae: 0.4363 - mse: 0.4296 - val_loss: 235.6086 - val_mae: 11.7824 - val_mse: 235.6086\n",
      "Epoch 1252/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5079 - mae: 0.4708 - mse: 0.5079 - val_loss: 233.5353 - val_mae: 11.6458 - val_mse: 233.5353\n",
      "Epoch 1253/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6080 - mae: 0.5781 - mse: 0.6080 - val_loss: 232.0914 - val_mae: 11.7686 - val_mse: 232.0914\n",
      "Epoch 1254/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5524 - mae: 0.5299 - mse: 0.5524 - val_loss: 241.6605 - val_mae: 11.8045 - val_mse: 241.6605\n",
      "Epoch 1255/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6465 - mae: 0.5418 - mse: 0.6465 - val_loss: 229.4100 - val_mae: 11.6637 - val_mse: 229.4100\n",
      "Epoch 1256/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.8126 - mae: 0.6246 - mse: 0.8126 - val_loss: 232.0767 - val_mae: 11.6905 - val_mse: 232.0767\n",
      "Epoch 1257/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6543 - mae: 0.5706 - mse: 0.6543 - val_loss: 233.6544 - val_mae: 11.6686 - val_mse: 233.6544\n",
      "Epoch 1258/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5695 - mae: 0.5257 - mse: 0.5695 - val_loss: 232.0829 - val_mae: 11.6796 - val_mse: 232.0829\n",
      "Epoch 1259/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5261 - mae: 0.4455 - mse: 0.5261 - val_loss: 232.7661 - val_mae: 11.7483 - val_mse: 232.7661\n",
      "Epoch 1260/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4698 - mae: 0.4303 - mse: 0.4698 - val_loss: 233.1137 - val_mae: 11.6746 - val_mse: 233.1137\n",
      "Epoch 1261/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5202 - mae: 0.4330 - mse: 0.5202 - val_loss: 236.0405 - val_mae: 11.7576 - val_mse: 236.0405\n",
      "Epoch 1262/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3823 - mae: 0.3812 - mse: 0.3823 - val_loss: 234.3285 - val_mae: 11.8005 - val_mse: 234.3285\n",
      "Epoch 1263/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3841 - mae: 0.3335 - mse: 0.3841 - val_loss: 236.2245 - val_mae: 11.7114 - val_mse: 236.2245\n",
      "Epoch 1264/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3682 - mae: 0.3448 - mse: 0.3682 - val_loss: 230.6745 - val_mae: 11.7022 - val_mse: 230.6745\n",
      "Epoch 1265/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4130 - mae: 0.3722 - mse: 0.4130 - val_loss: 234.2057 - val_mae: 11.6897 - val_mse: 234.2057\n",
      "Epoch 1266/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3736 - mae: 0.3433 - mse: 0.3736 - val_loss: 235.7609 - val_mae: 11.6999 - val_mse: 235.7609\n",
      "Epoch 1267/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3412 - mae: 0.3404 - mse: 0.3412 - val_loss: 234.4398 - val_mae: 11.8234 - val_mse: 234.4398\n",
      "Epoch 1268/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3668 - mae: 0.3600 - mse: 0.3668 - val_loss: 236.5958 - val_mae: 11.7700 - val_mse: 236.5958\n",
      "Epoch 1269/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3633 - mae: 0.2967 - mse: 0.3633 - val_loss: 234.0983 - val_mae: 11.7444 - val_mse: 234.0983\n",
      "Epoch 1270/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4822 - mae: 0.3842 - mse: 0.4822 - val_loss: 233.2058 - val_mae: 11.7392 - val_mse: 233.2058\n",
      "Epoch 1271/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3699 - mae: 0.3759 - mse: 0.3699 - val_loss: 237.7748 - val_mae: 11.7281 - val_mse: 237.7748\n",
      "Epoch 1272/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5051 - mae: 0.4047 - mse: 0.5051 - val_loss: 234.5063 - val_mae: 11.7784 - val_mse: 234.5063\n",
      "Epoch 1273/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4574 - mae: 0.4291 - mse: 0.4574 - val_loss: 231.7961 - val_mae: 11.7218 - val_mse: 231.7961\n",
      "Epoch 1274/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4078 - mae: 0.4115 - mse: 0.4078 - val_loss: 238.2738 - val_mae: 11.7956 - val_mse: 238.2738\n",
      "Epoch 1275/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3930 - mae: 0.3346 - mse: 0.3930 - val_loss: 234.1931 - val_mae: 11.7714 - val_mse: 234.1931\n",
      "Epoch 1276/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3321 - mae: 0.2986 - mse: 0.3321 - val_loss: 233.5779 - val_mae: 11.6728 - val_mse: 233.5779\n",
      "Epoch 1277/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3465 - mae: 0.3372 - mse: 0.3465 - val_loss: 233.7465 - val_mae: 11.7177 - val_mse: 233.7465\n",
      "Epoch 1278/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3001 - mae: 0.2845 - mse: 0.3001 - val_loss: 234.0943 - val_mae: 11.7009 - val_mse: 234.0943\n",
      "Epoch 1279/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3074 - mae: 0.2671 - mse: 0.3074 - val_loss: 233.5288 - val_mae: 11.7602 - val_mse: 233.5288\n",
      "Epoch 1280/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3347 - mae: 0.3025 - mse: 0.3347 - val_loss: 233.1493 - val_mae: 11.6445 - val_mse: 233.1493\n",
      "Epoch 1281/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3049 - mae: 0.3030 - mse: 0.3049 - val_loss: 234.3233 - val_mae: 11.7158 - val_mse: 234.3233\n",
      "Epoch 1282/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3189 - mae: 0.2834 - mse: 0.3189 - val_loss: 235.5102 - val_mae: 11.7751 - val_mse: 235.5102\n",
      "Epoch 1283/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3616 - mae: 0.3422 - mse: 0.3616 - val_loss: 234.3242 - val_mae: 11.6762 - val_mse: 234.3242\n",
      "Epoch 1284/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3986 - mae: 0.3912 - mse: 0.3986 - val_loss: 227.2326 - val_mae: 11.6243 - val_mse: 227.2326\n",
      "Epoch 1285/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5286 - mae: 0.4632 - mse: 0.5286 - val_loss: 237.0960 - val_mae: 11.7671 - val_mse: 237.0960\n",
      "Epoch 1286/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5524 - mae: 0.5115 - mse: 0.5524 - val_loss: 236.7560 - val_mae: 11.8173 - val_mse: 236.7560\n",
      "Epoch 1287/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5910 - mae: 0.5027 - mse: 0.5910 - val_loss: 231.0215 - val_mae: 11.6921 - val_mse: 231.0215\n",
      "Epoch 1288/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4791 - mae: 0.4397 - mse: 0.4791 - val_loss: 235.8812 - val_mae: 11.6848 - val_mse: 235.8812\n",
      "Epoch 1289/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5448 - mae: 0.5100 - mse: 0.5448 - val_loss: 231.3300 - val_mae: 11.6968 - val_mse: 231.3300\n",
      "Epoch 1290/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6940 - mae: 0.5752 - mse: 0.6940 - val_loss: 233.0332 - val_mae: 11.6729 - val_mse: 233.0332\n",
      "Epoch 1291/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5905 - mae: 0.4859 - mse: 0.5905 - val_loss: 236.3986 - val_mae: 11.7518 - val_mse: 236.3986\n",
      "Epoch 1292/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3949 - mae: 0.3995 - mse: 0.3949 - val_loss: 233.5380 - val_mae: 11.7364 - val_mse: 233.5380\n",
      "Epoch 1293/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6049 - mae: 0.5368 - mse: 0.6049 - val_loss: 224.7256 - val_mae: 11.5588 - val_mse: 224.7256\n",
      "Epoch 1294/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4514 - mae: 0.4403 - mse: 0.4514 - val_loss: 238.7660 - val_mae: 11.7849 - val_mse: 238.7660\n",
      "Epoch 1295/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5071 - mae: 0.4227 - mse: 0.5071 - val_loss: 234.0303 - val_mae: 11.8012 - val_mse: 234.0303\n",
      "Epoch 1296/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5411 - mae: 0.4773 - mse: 0.5411 - val_loss: 234.1987 - val_mae: 11.7554 - val_mse: 234.1987\n",
      "Epoch 1297/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4910 - mae: 0.4067 - mse: 0.4910 - val_loss: 235.4141 - val_mae: 11.7124 - val_mse: 235.4141\n",
      "Epoch 1298/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3745 - mae: 0.3520 - mse: 0.3745 - val_loss: 232.8443 - val_mae: 11.7060 - val_mse: 232.8443\n",
      "Epoch 1299/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5421 - mae: 0.4796 - mse: 0.5421 - val_loss: 235.9806 - val_mae: 11.7817 - val_mse: 235.9806\n",
      "Epoch 1300/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4145 - mae: 0.4221 - mse: 0.4145 - val_loss: 238.8819 - val_mae: 11.7720 - val_mse: 238.8819\n",
      "Epoch 1301/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.4436 - mae: 0.3919 - mse: 0.4436 - val_loss: 234.2174 - val_mae: 11.7772 - val_mse: 234.2174\n",
      "Epoch 1302/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5073 - mae: 0.4461 - mse: 0.5073 - val_loss: 233.2796 - val_mae: 11.7113 - val_mse: 233.2796\n",
      "Epoch 1303/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4949 - mae: 0.4662 - mse: 0.4949 - val_loss: 231.7319 - val_mae: 11.6002 - val_mse: 231.7319\n",
      "Epoch 1304/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6028 - mae: 0.4746 - mse: 0.6028 - val_loss: 232.9937 - val_mae: 11.7224 - val_mse: 232.9937\n",
      "Epoch 1305/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3575 - mae: 0.3570 - mse: 0.3575 - val_loss: 234.1688 - val_mae: 11.7061 - val_mse: 234.1688\n",
      "Epoch 1306/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4348 - mae: 0.3658 - mse: 0.4348 - val_loss: 233.3173 - val_mae: 11.6512 - val_mse: 233.3173\n",
      "Epoch 1307/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3644 - mae: 0.3618 - mse: 0.3644 - val_loss: 234.5051 - val_mae: 11.7697 - val_mse: 234.5051\n",
      "Epoch 1308/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3368 - mae: 0.3093 - mse: 0.3368 - val_loss: 231.0512 - val_mae: 11.6697 - val_mse: 231.0512\n",
      "Epoch 1309/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3152 - mae: 0.3123 - mse: 0.3152 - val_loss: 235.7807 - val_mae: 11.7684 - val_mse: 235.7807\n",
      "Epoch 1310/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3384 - mae: 0.3099 - mse: 0.3384 - val_loss: 236.0672 - val_mae: 11.7255 - val_mse: 236.0672\n",
      "Epoch 1311/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3732 - mae: 0.3340 - mse: 0.3732 - val_loss: 233.9399 - val_mae: 11.7419 - val_mse: 233.9399\n",
      "Epoch 1312/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3237 - mae: 0.3065 - mse: 0.3237 - val_loss: 238.3857 - val_mae: 11.8031 - val_mse: 238.3857\n",
      "Epoch 1313/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3213 - mae: 0.3015 - mse: 0.3213 - val_loss: 233.1792 - val_mae: 11.7276 - val_mse: 233.1792\n",
      "Epoch 1314/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3531 - mae: 0.3286 - mse: 0.3531 - val_loss: 231.9034 - val_mae: 11.6828 - val_mse: 231.9034\n",
      "Epoch 1315/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3427 - mae: 0.2994 - mse: 0.3427 - val_loss: 237.1869 - val_mae: 11.7361 - val_mse: 237.1869\n",
      "Epoch 1316/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4316 - mae: 0.3500 - mse: 0.4316 - val_loss: 233.9577 - val_mae: 11.7314 - val_mse: 233.9577\n",
      "Epoch 1317/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4855 - mae: 0.3965 - mse: 0.4855 - val_loss: 231.8576 - val_mae: 11.7208 - val_mse: 231.8576\n",
      "Epoch 1318/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.3973 - mae: 0.3729 - mse: 0.3973 - val_loss: 237.7833 - val_mae: 11.7443 - val_mse: 237.7833\n",
      "Epoch 1319/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4459 - mae: 0.4156 - mse: 0.4459 - val_loss: 235.7422 - val_mae: 11.7958 - val_mse: 235.7422\n",
      "Epoch 1320/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6236 - mae: 0.5025 - mse: 0.6236 - val_loss: 234.2527 - val_mae: 11.7016 - val_mse: 234.2527\n",
      "Epoch 1321/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.7972 - mae: 0.6220 - mse: 0.7972 - val_loss: 237.0206 - val_mae: 11.7521 - val_mse: 237.0206\n",
      "Epoch 1322/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6348 - mae: 0.5683 - mse: 0.6348 - val_loss: 234.4732 - val_mae: 11.7239 - val_mse: 234.4732\n",
      "Epoch 1323/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.7403 - mae: 0.6194 - mse: 0.7403 - val_loss: 232.4664 - val_mae: 11.7083 - val_mse: 232.4664\n",
      "Epoch 1324/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.8249 - mae: 0.6386 - mse: 0.8249 - val_loss: 232.0066 - val_mae: 11.6862 - val_mse: 232.0066\n",
      "Epoch 1325/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.0981 - mae: 0.7249 - mse: 1.0981 - val_loss: 235.6069 - val_mae: 11.7245 - val_mse: 235.6069\n",
      "Epoch 1326/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.2554 - mae: 0.7856 - mse: 1.2554 - val_loss: 238.1075 - val_mae: 11.7419 - val_mse: 238.1075\n",
      "Epoch 1327/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6921 - mae: 0.6151 - mse: 0.6921 - val_loss: 228.5813 - val_mae: 11.6554 - val_mse: 228.5813\n",
      "Epoch 1328/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.7821 - mae: 0.6182 - mse: 0.7821 - val_loss: 244.5565 - val_mae: 11.8247 - val_mse: 244.5565\n",
      "Epoch 1329/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.0544 - mae: 0.6909 - mse: 1.0544 - val_loss: 233.1461 - val_mae: 11.7092 - val_mse: 233.1461\n",
      "Epoch 1330/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.5529 - mae: 0.8462 - mse: 1.5529 - val_loss: 236.8118 - val_mae: 11.8014 - val_mse: 236.8118\n",
      "Epoch 1331/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.0599 - mae: 0.7213 - mse: 1.0599 - val_loss: 241.4567 - val_mae: 11.8277 - val_mse: 241.4567\n",
      "Epoch 1332/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.8438 - mae: 0.6118 - mse: 0.8438 - val_loss: 228.9811 - val_mae: 11.5921 - val_mse: 228.9811\n",
      "Epoch 1333/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.7534 - mae: 0.5767 - mse: 0.7534 - val_loss: 235.0543 - val_mae: 11.6404 - val_mse: 235.0543\n",
      "Epoch 1334/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5717 - mae: 0.5252 - mse: 0.5717 - val_loss: 235.1350 - val_mae: 11.7397 - val_mse: 235.1350\n",
      "Epoch 1335/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6137 - mae: 0.5581 - mse: 0.6137 - val_loss: 232.4133 - val_mae: 11.6470 - val_mse: 232.4133\n",
      "Epoch 1336/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6268 - mae: 0.5412 - mse: 0.6268 - val_loss: 237.7323 - val_mae: 11.7675 - val_mse: 237.7323\n",
      "Epoch 1337/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4122 - mae: 0.4254 - mse: 0.4122 - val_loss: 235.1619 - val_mae: 11.7438 - val_mse: 235.1619\n",
      "Epoch 1338/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3986 - mae: 0.3975 - mse: 0.3986 - val_loss: 238.3541 - val_mae: 11.8111 - val_mse: 238.3541\n",
      "Epoch 1339/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4914 - mae: 0.4536 - mse: 0.4914 - val_loss: 230.4067 - val_mae: 11.6495 - val_mse: 230.4067\n",
      "Epoch 1340/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4313 - mae: 0.4473 - mse: 0.4313 - val_loss: 233.8180 - val_mae: 11.6553 - val_mse: 233.8180\n",
      "Epoch 1341/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4468 - mae: 0.4236 - mse: 0.4468 - val_loss: 235.7136 - val_mae: 11.7541 - val_mse: 235.7136\n",
      "Epoch 1342/3500\n",
      "126/126 [==============================] - 0s 142us/step - loss: 0.4325 - mae: 0.3996 - mse: 0.4325 - val_loss: 233.2532 - val_mae: 11.6672 - val_mse: 233.2532\n",
      "Epoch 1343/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.4408 - mae: 0.3911 - mse: 0.4408 - val_loss: 236.4830 - val_mae: 11.7144 - val_mse: 236.4830\n",
      "Epoch 1344/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.5869 - mae: 0.5030 - mse: 0.5869 - val_loss: 232.2166 - val_mae: 11.7197 - val_mse: 232.2166\n",
      "Epoch 1345/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.6172 - mae: 0.5721 - mse: 0.6172 - val_loss: 231.6460 - val_mae: 11.6488 - val_mse: 231.6460\n",
      "Epoch 1346/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4826 - mae: 0.4673 - mse: 0.4826 - val_loss: 238.2735 - val_mae: 11.7961 - val_mse: 238.2735\n",
      "Epoch 1347/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4592 - mae: 0.4261 - mse: 0.4592 - val_loss: 231.6809 - val_mae: 11.6416 - val_mse: 231.6809\n",
      "Epoch 1348/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4378 - mae: 0.4118 - mse: 0.4378 - val_loss: 236.4964 - val_mae: 11.7320 - val_mse: 236.4964\n",
      "Epoch 1349/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4610 - mae: 0.4106 - mse: 0.4610 - val_loss: 235.3310 - val_mae: 11.7409 - val_mse: 235.3310\n",
      "Epoch 1350/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.5102 - mae: 0.4781 - mse: 0.5102 - val_loss: 231.1685 - val_mae: 11.5885 - val_mse: 231.1685\n",
      "Epoch 1351/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.6269 - mae: 0.5709 - mse: 0.6269 - val_loss: 233.5688 - val_mae: 11.7647 - val_mse: 233.5688\n",
      "Epoch 1352/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.6435 - mae: 0.5767 - mse: 0.6435 - val_loss: 233.7598 - val_mae: 11.6407 - val_mse: 233.7598\n",
      "Epoch 1353/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.7137 - mae: 0.5839 - mse: 0.7137 - val_loss: 235.1524 - val_mae: 11.7344 - val_mse: 235.1524\n",
      "Epoch 1354/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.4729 - mae: 0.4357 - mse: 0.4729 - val_loss: 238.6506 - val_mae: 11.8424 - val_mse: 238.6506\n",
      "Epoch 1355/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3751 - mae: 0.3697 - mse: 0.3751 - val_loss: 232.8882 - val_mae: 11.6206 - val_mse: 232.8882\n",
      "Epoch 1356/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4182 - mae: 0.3779 - mse: 0.4182 - val_loss: 235.5511 - val_mae: 11.7519 - val_mse: 235.5511\n",
      "Epoch 1357/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4030 - mae: 0.4081 - mse: 0.4030 - val_loss: 231.8198 - val_mae: 11.6631 - val_mse: 231.8198\n",
      "Epoch 1358/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3069 - mae: 0.2921 - mse: 0.3069 - val_loss: 231.6117 - val_mae: 11.6200 - val_mse: 231.6117\n",
      "Epoch 1359/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3370 - mae: 0.2939 - mse: 0.3370 - val_loss: 236.9598 - val_mae: 11.7872 - val_mse: 236.9598\n",
      "Epoch 1360/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3215 - mae: 0.2896 - mse: 0.3215 - val_loss: 233.4110 - val_mae: 11.6734 - val_mse: 233.4110\n",
      "Epoch 1361/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2877 - mae: 0.2679 - mse: 0.2877 - val_loss: 233.2611 - val_mae: 11.6450 - val_mse: 233.2611\n",
      "Epoch 1362/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2884 - mae: 0.2610 - mse: 0.2884 - val_loss: 235.0892 - val_mae: 11.7309 - val_mse: 235.0892\n",
      "Epoch 1363/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2975 - mae: 0.2850 - mse: 0.2975 - val_loss: 235.7821 - val_mae: 11.6973 - val_mse: 235.7821\n",
      "Epoch 1364/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3184 - mae: 0.2790 - mse: 0.3184 - val_loss: 232.7646 - val_mae: 11.6771 - val_mse: 232.7646\n",
      "Epoch 1365/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3276 - mae: 0.2910 - mse: 0.3276 - val_loss: 233.9042 - val_mae: 11.7037 - val_mse: 233.9042\n",
      "Epoch 1366/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3199 - mae: 0.2992 - mse: 0.3199 - val_loss: 236.2154 - val_mae: 11.7555 - val_mse: 236.2154\n",
      "Epoch 1367/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3278 - mae: 0.2890 - mse: 0.3278 - val_loss: 233.2539 - val_mae: 11.6663 - val_mse: 233.2539\n",
      "Epoch 1368/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3008 - mae: 0.2483 - mse: 0.3008 - val_loss: 233.0594 - val_mae: 11.6761 - val_mse: 233.0594\n",
      "Epoch 1369/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2687 - mae: 0.2063 - mse: 0.2687 - val_loss: 234.2563 - val_mae: 11.6919 - val_mse: 234.2563\n",
      "Epoch 1370/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.2716 - mae: 0.2076 - mse: 0.2716 - val_loss: 235.0045 - val_mae: 11.7160 - val_mse: 235.0045\n",
      "Epoch 1371/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2695 - mae: 0.2075 - mse: 0.2695 - val_loss: 233.3384 - val_mae: 11.6973 - val_mse: 233.3384\n",
      "Epoch 1372/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.2850 - mae: 0.2327 - mse: 0.2850 - val_loss: 234.2248 - val_mae: 11.7179 - val_mse: 234.2248\n",
      "Epoch 1373/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2939 - mae: 0.2608 - mse: 0.2939 - val_loss: 235.7048 - val_mae: 11.7068 - val_mse: 235.7048\n",
      "Epoch 1374/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2944 - mae: 0.2535 - mse: 0.2944 - val_loss: 232.6941 - val_mae: 11.6814 - val_mse: 232.6941\n",
      "Epoch 1375/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2804 - mae: 0.2170 - mse: 0.2804 - val_loss: 234.0310 - val_mae: 11.7225 - val_mse: 234.0310\n",
      "Epoch 1376/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2730 - mae: 0.1866 - mse: 0.2730 - val_loss: 235.3886 - val_mae: 11.7196 - val_mse: 235.3886\n",
      "Epoch 1377/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2708 - mae: 0.1833 - mse: 0.2708 - val_loss: 233.1797 - val_mae: 11.7073 - val_mse: 233.1797\n",
      "Epoch 1378/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2804 - mae: 0.2363 - mse: 0.2804 - val_loss: 236.3559 - val_mae: 11.7060 - val_mse: 236.3559\n",
      "Epoch 1379/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.2966 - mae: 0.2643 - mse: 0.2966 - val_loss: 233.4903 - val_mae: 11.6911 - val_mse: 233.4903\n",
      "Epoch 1380/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3725 - mae: 0.3070 - mse: 0.3725 - val_loss: 231.7526 - val_mae: 11.6618 - val_mse: 231.7526\n",
      "Epoch 1381/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3222 - mae: 0.2635 - mse: 0.3222 - val_loss: 236.5581 - val_mae: 11.7104 - val_mse: 236.5581\n",
      "Epoch 1382/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.0378 - mae: 0.1520 - mse: 0.037 - 0s 79us/step - loss: 0.2966 - mae: 0.2689 - mse: 0.2966 - val_loss: 232.5591 - val_mae: 11.6814 - val_mse: 232.5591\n",
      "Epoch 1383/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2899 - mae: 0.2711 - mse: 0.2899 - val_loss: 234.0461 - val_mae: 11.6800 - val_mse: 234.0461\n",
      "Epoch 1384/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2887 - mae: 0.2646 - mse: 0.2887 - val_loss: 234.4573 - val_mae: 11.7014 - val_mse: 234.4573\n",
      "Epoch 1385/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3283 - mae: 0.2896 - mse: 0.3283 - val_loss: 231.9123 - val_mae: 11.6331 - val_mse: 231.9123\n",
      "Epoch 1386/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2978 - mae: 0.2413 - mse: 0.2978 - val_loss: 236.4037 - val_mae: 11.7334 - val_mse: 236.4037\n",
      "Epoch 1387/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2824 - mae: 0.2334 - mse: 0.2824 - val_loss: 233.2086 - val_mae: 11.6661 - val_mse: 233.2086\n",
      "Epoch 1388/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3159 - mae: 0.2809 - mse: 0.3159 - val_loss: 235.3062 - val_mae: 11.6851 - val_mse: 235.3062\n",
      "Epoch 1389/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3321 - mae: 0.2999 - mse: 0.3321 - val_loss: 232.5676 - val_mae: 11.6839 - val_mse: 232.5676\n",
      "Epoch 1390/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3128 - mae: 0.2964 - mse: 0.3128 - val_loss: 236.0332 - val_mae: 11.6879 - val_mse: 236.0332\n",
      "Epoch 1391/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.3239 - mae: 0.3020 - mse: 0.3239 - val_loss: 235.3728 - val_mae: 11.7607 - val_mse: 235.3728\n",
      "Epoch 1392/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2927 - mae: 0.2721 - mse: 0.2927 - val_loss: 232.8974 - val_mae: 11.6409 - val_mse: 232.8974\n",
      "Epoch 1393/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3194 - mae: 0.2666 - mse: 0.3194 - val_loss: 238.4145 - val_mae: 11.7745 - val_mse: 238.4145\n",
      "Epoch 1394/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3879 - mae: 0.3568 - mse: 0.3879 - val_loss: 233.2524 - val_mae: 11.7189 - val_mse: 233.2524\n",
      "Epoch 1395/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4136 - mae: 0.4275 - mse: 0.4136 - val_loss: 236.6032 - val_mae: 11.6776 - val_mse: 236.6032\n",
      "Epoch 1396/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4000 - mae: 0.4086 - mse: 0.4000 - val_loss: 232.3039 - val_mae: 11.6918 - val_mse: 232.3039\n",
      "Epoch 1397/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3431 - mae: 0.3420 - mse: 0.3431 - val_loss: 235.1589 - val_mae: 11.7213 - val_mse: 235.1589\n",
      "Epoch 1398/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4058 - mae: 0.3518 - mse: 0.4058 - val_loss: 239.7560 - val_mae: 11.8214 - val_mse: 239.7560\n",
      "Epoch 1399/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3944 - mae: 0.3880 - mse: 0.3944 - val_loss: 232.9991 - val_mae: 11.6727 - val_mse: 232.9991\n",
      "Epoch 1400/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4801 - mae: 0.4131 - mse: 0.4801 - val_loss: 234.3022 - val_mae: 11.6389 - val_mse: 234.3022\n",
      "Epoch 1401/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4395 - mae: 0.3931 - mse: 0.4395 - val_loss: 239.0116 - val_mae: 11.7738 - val_mse: 239.0116\n",
      "Epoch 1402/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.4383 - mae: 0.4197 - mse: 0.4383 - val_loss: 234.9649 - val_mae: 11.7578 - val_mse: 234.9649\n",
      "Epoch 1403/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.3867 - mae: 0.3627 - mse: 0.3867 - val_loss: 236.9281 - val_mae: 11.7071 - val_mse: 236.9281\n",
      "Epoch 1404/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3505 - mae: 0.3558 - mse: 0.3505 - val_loss: 230.1281 - val_mae: 11.6403 - val_mse: 230.1281\n",
      "Epoch 1405/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3979 - mae: 0.3836 - mse: 0.3979 - val_loss: 235.2119 - val_mae: 11.6745 - val_mse: 235.2119\n",
      "Epoch 1406/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3992 - mae: 0.3867 - mse: 0.3992 - val_loss: 240.3611 - val_mae: 11.8292 - val_mse: 240.3611\n",
      "Epoch 1407/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3534 - mae: 0.3532 - mse: 0.3534 - val_loss: 234.4976 - val_mae: 11.7282 - val_mse: 234.4976\n",
      "Epoch 1408/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5203 - mae: 0.4732 - mse: 0.5203 - val_loss: 236.5305 - val_mae: 11.6874 - val_mse: 236.5305\n",
      "Epoch 1409/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6050 - mae: 0.5400 - mse: 0.6050 - val_loss: 236.4142 - val_mae: 11.7368 - val_mse: 236.4142\n",
      "Epoch 1410/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.5333 - mae: 0.5012 - mse: 0.5333 - val_loss: 236.8141 - val_mae: 11.7581 - val_mse: 236.8141\n",
      "Epoch 1411/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.7313 - mae: 0.6136 - mse: 0.7313 - val_loss: 233.4344 - val_mae: 11.6688 - val_mse: 233.4344\n",
      "Epoch 1412/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5441 - mae: 0.5097 - mse: 0.5441 - val_loss: 236.3462 - val_mae: 11.7235 - val_mse: 236.3462\n",
      "Epoch 1413/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4993 - mae: 0.4748 - mse: 0.4993 - val_loss: 234.9721 - val_mae: 11.7650 - val_mse: 234.9721\n",
      "Epoch 1414/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4101 - mae: 0.4311 - mse: 0.4101 - val_loss: 236.7325 - val_mae: 11.6868 - val_mse: 236.7325\n",
      "Epoch 1415/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6899 - mae: 0.5572 - mse: 0.6899 - val_loss: 235.8116 - val_mae: 11.7961 - val_mse: 235.8116\n",
      "Epoch 1416/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6639 - mae: 0.5623 - mse: 0.6639 - val_loss: 233.2652 - val_mae: 11.6719 - val_mse: 233.2652\n",
      "Epoch 1417/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5225 - mae: 0.4878 - mse: 0.5225 - val_loss: 232.4395 - val_mae: 11.6022 - val_mse: 232.4395\n",
      "Epoch 1418/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4259 - mae: 0.4101 - mse: 0.4259 - val_loss: 236.8031 - val_mae: 11.7004 - val_mse: 236.8031\n",
      "Epoch 1419/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4746 - mae: 0.4764 - mse: 0.4746 - val_loss: 234.1396 - val_mae: 11.7175 - val_mse: 234.1396\n",
      "Epoch 1420/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4542 - mae: 0.4230 - mse: 0.4542 - val_loss: 240.5910 - val_mae: 11.8007 - val_mse: 240.5910\n",
      "Epoch 1421/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3410 - mae: 0.3169 - mse: 0.3410 - val_loss: 235.4108 - val_mae: 11.7498 - val_mse: 235.4108\n",
      "Epoch 1422/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4156 - mae: 0.3618 - mse: 0.4156 - val_loss: 233.8249 - val_mae: 11.6480 - val_mse: 233.8249\n",
      "Epoch 1423/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3850 - mae: 0.3641 - mse: 0.3850 - val_loss: 234.2177 - val_mae: 11.6735 - val_mse: 234.2177\n",
      "Epoch 1424/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3539 - mae: 0.3228 - mse: 0.3539 - val_loss: 236.2302 - val_mae: 11.7688 - val_mse: 236.2302\n",
      "Epoch 1425/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4076 - mae: 0.4083 - mse: 0.4076 - val_loss: 241.7385 - val_mae: 11.8218 - val_mse: 241.7385\n",
      "Epoch 1426/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4887 - mae: 0.4337 - mse: 0.4887 - val_loss: 230.4996 - val_mae: 11.6205 - val_mse: 230.4996\n",
      "Epoch 1427/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4292 - mae: 0.4310 - mse: 0.4292 - val_loss: 235.7932 - val_mae: 11.7319 - val_mse: 235.7932\n",
      "Epoch 1428/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4619 - mae: 0.4187 - mse: 0.4619 - val_loss: 239.8739 - val_mae: 11.8030 - val_mse: 239.8739\n",
      "Epoch 1429/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4324 - mae: 0.4254 - mse: 0.4324 - val_loss: 237.0961 - val_mae: 11.7585 - val_mse: 237.0961\n",
      "Epoch 1430/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4982 - mae: 0.4938 - mse: 0.4982 - val_loss: 238.7434 - val_mae: 11.7634 - val_mse: 238.7434\n",
      "Epoch 1431/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5388 - mae: 0.5092 - mse: 0.5388 - val_loss: 236.1751 - val_mae: 11.7475 - val_mse: 236.1751\n",
      "Epoch 1432/3500\n",
      "126/126 [==============================] - 0s 182us/step - loss: 0.4547 - mae: 0.4670 - mse: 0.4547 - val_loss: 233.1256 - val_mae: 11.7103 - val_mse: 233.1256\n",
      "Epoch 1433/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4252 - mae: 0.3970 - mse: 0.4252 - val_loss: 241.9444 - val_mae: 11.8536 - val_mse: 241.9444\n",
      "Epoch 1434/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5510 - mae: 0.4561 - mse: 0.5510 - val_loss: 235.7121 - val_mae: 11.7584 - val_mse: 235.7121\n",
      "Epoch 1435/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6714 - mae: 0.5550 - mse: 0.6714 - val_loss: 231.6963 - val_mae: 11.6360 - val_mse: 231.6963\n",
      "Epoch 1436/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.5713 - mae: 0.4677 - mse: 0.5713 - val_loss: 240.5300 - val_mae: 11.7217 - val_mse: 240.5300\n",
      "Epoch 1437/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.6967 - mae: 0.6159 - mse: 0.6967 - val_loss: 232.5692 - val_mae: 11.7312 - val_mse: 232.5692\n",
      "Epoch 1438/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 1.0473 - mae: 0.7042 - mse: 1.0473 - val_loss: 237.7549 - val_mae: 11.7698 - val_mse: 237.7549\n",
      "Epoch 1439/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.5453 - mae: 0.4289 - mse: 0.545 - 0s 119us/step - loss: 0.6783 - mae: 0.5518 - mse: 0.6783 - val_loss: 234.2207 - val_mae: 11.6753 - val_mse: 234.2207\n",
      "Epoch 1440/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.5391 - mae: 0.5195 - mse: 0.5391 - val_loss: 230.9986 - val_mae: 11.6830 - val_mse: 230.9986\n",
      "Epoch 1441/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6102 - mae: 0.5322 - mse: 0.6102 - val_loss: 244.5013 - val_mae: 11.8475 - val_mse: 244.5013\n",
      "Epoch 1442/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6416 - mae: 0.5298 - mse: 0.6416 - val_loss: 231.7813 - val_mae: 11.6626 - val_mse: 231.7813\n",
      "Epoch 1443/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5657 - mae: 0.5110 - mse: 0.5657 - val_loss: 230.3115 - val_mae: 11.6390 - val_mse: 230.3115\n",
      "Epoch 1444/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.5087 - mae: 0.4552 - mse: 0.5087 - val_loss: 237.2498 - val_mae: 11.6663 - val_mse: 237.2498\n",
      "Epoch 1445/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6440 - mae: 0.5828 - mse: 0.6440 - val_loss: 230.5039 - val_mae: 11.7088 - val_mse: 230.5039\n",
      "Epoch 1446/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.9096 - mae: 0.7008 - mse: 0.9096 - val_loss: 239.1405 - val_mae: 11.7496 - val_mse: 239.1405\n",
      "Epoch 1447/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4473 - mae: 0.4559 - mse: 0.4473 - val_loss: 236.6738 - val_mae: 11.7615 - val_mse: 236.6738\n",
      "Epoch 1448/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4788 - mae: 0.4350 - mse: 0.4788 - val_loss: 231.7914 - val_mae: 11.6358 - val_mse: 231.7914\n",
      "Epoch 1449/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3512 - mae: 0.3585 - mse: 0.3512 - val_loss: 235.0149 - val_mae: 11.6652 - val_mse: 235.0149\n",
      "Epoch 1450/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4062 - mae: 0.3723 - mse: 0.4062 - val_loss: 238.3490 - val_mae: 11.7631 - val_mse: 238.3490\n",
      "Epoch 1451/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3877 - mae: 0.3587 - mse: 0.3877 - val_loss: 237.8302 - val_mae: 11.7914 - val_mse: 237.8302\n",
      "Epoch 1452/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3781 - mae: 0.3749 - mse: 0.3781 - val_loss: 239.0028 - val_mae: 11.7293 - val_mse: 239.0028\n",
      "Epoch 1453/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4622 - mae: 0.4323 - mse: 0.4622 - val_loss: 233.9744 - val_mae: 11.7065 - val_mse: 233.9744\n",
      "Epoch 1454/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4378 - mae: 0.4309 - mse: 0.4378 - val_loss: 238.5833 - val_mae: 11.7608 - val_mse: 238.5833\n",
      "Epoch 1455/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4780 - mae: 0.4098 - mse: 0.4780 - val_loss: 237.8403 - val_mae: 11.7540 - val_mse: 237.8403\n",
      "Epoch 1456/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4040 - mae: 0.3611 - mse: 0.4040 - val_loss: 235.6384 - val_mae: 11.7375 - val_mse: 235.6384\n",
      "Epoch 1457/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3906 - mae: 0.3438 - mse: 0.3906 - val_loss: 238.5219 - val_mae: 11.7470 - val_mse: 238.5219\n",
      "Epoch 1458/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3878 - mae: 0.3694 - mse: 0.3878 - val_loss: 230.8056 - val_mae: 11.6503 - val_mse: 230.8056\n",
      "Epoch 1459/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4669 - mae: 0.4090 - mse: 0.4669 - val_loss: 238.3892 - val_mae: 11.7251 - val_mse: 238.3892\n",
      "Epoch 1460/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4243 - mae: 0.4266 - mse: 0.4243 - val_loss: 237.2142 - val_mae: 11.7532 - val_mse: 237.2142\n",
      "Epoch 1461/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3718 - mae: 0.3627 - mse: 0.3718 - val_loss: 234.8003 - val_mae: 11.6649 - val_mse: 234.8003\n",
      "Epoch 1462/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3416 - mae: 0.3434 - mse: 0.3416 - val_loss: 237.4772 - val_mae: 11.7726 - val_mse: 237.4772\n",
      "Epoch 1463/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4095 - mae: 0.3561 - mse: 0.4095 - val_loss: 234.3827 - val_mae: 11.7168 - val_mse: 234.3827\n",
      "Epoch 1464/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4324 - mae: 0.3902 - mse: 0.4324 - val_loss: 238.1125 - val_mae: 11.7333 - val_mse: 238.1125\n",
      "Epoch 1465/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3638 - mae: 0.3713 - mse: 0.3638 - val_loss: 236.6165 - val_mae: 11.7333 - val_mse: 236.6165\n",
      "Epoch 1466/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3806 - mae: 0.3178 - mse: 0.3806 - val_loss: 235.1620 - val_mae: 11.7172 - val_mse: 235.1620\n",
      "Epoch 1467/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3651 - mae: 0.3458 - mse: 0.3651 - val_loss: 237.3741 - val_mae: 11.6930 - val_mse: 237.3741\n",
      "Epoch 1468/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4152 - mae: 0.4161 - mse: 0.4152 - val_loss: 232.3369 - val_mae: 11.6846 - val_mse: 232.3369\n",
      "Epoch 1469/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4853 - mae: 0.4429 - mse: 0.4853 - val_loss: 243.1027 - val_mae: 11.8593 - val_mse: 243.1027\n",
      "Epoch 1470/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5574 - mae: 0.5199 - mse: 0.5574 - val_loss: 235.7364 - val_mae: 11.6689 - val_mse: 235.7364\n",
      "Epoch 1471/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5565 - mae: 0.5287 - mse: 0.5565 - val_loss: 229.7287 - val_mae: 11.6993 - val_mse: 229.7287\n",
      "Epoch 1472/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5900 - mae: 0.5406 - mse: 0.5900 - val_loss: 244.4421 - val_mae: 11.8265 - val_mse: 244.4421\n",
      "Epoch 1473/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6398 - mae: 0.5739 - mse: 0.6398 - val_loss: 228.4960 - val_mae: 11.6781 - val_mse: 228.4960\n",
      "Epoch 1474/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.2713 - mae: 0.8840 - mse: 1.2713 - val_loss: 244.4880 - val_mae: 11.8343 - val_mse: 244.4880\n",
      "Epoch 1475/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.0594 - mae: 0.7348 - mse: 1.0594 - val_loss: 240.4799 - val_mae: 11.8662 - val_mse: 240.4799\n",
      "Epoch 1476/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.9275 - mae: 0.6881 - mse: 0.9275 - val_loss: 233.7820 - val_mae: 11.6880 - val_mse: 233.7820\n",
      "Epoch 1477/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.9517 - mae: 0.6302 - mse: 0.9517 - val_loss: 239.6126 - val_mae: 11.8239 - val_mse: 239.6126\n",
      "Epoch 1478/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.8034 - mae: 0.6665 - mse: 0.8034 - val_loss: 231.7342 - val_mae: 11.6060 - val_mse: 231.7342\n",
      "Epoch 1479/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6727 - mae: 0.5391 - mse: 0.6727 - val_loss: 234.4311 - val_mae: 11.7015 - val_mse: 234.4311\n",
      "Epoch 1480/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5767 - mae: 0.5011 - mse: 0.5767 - val_loss: 233.7885 - val_mae: 11.6641 - val_mse: 233.7885\n",
      "Epoch 1481/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4383 - mae: 0.3973 - mse: 0.4383 - val_loss: 240.0231 - val_mae: 11.8258 - val_mse: 240.0231\n",
      "Epoch 1482/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4115 - mae: 0.3821 - mse: 0.4115 - val_loss: 234.8381 - val_mae: 11.7427 - val_mse: 234.8381\n",
      "Epoch 1483/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3960 - mae: 0.3639 - mse: 0.3960 - val_loss: 234.4969 - val_mae: 11.6313 - val_mse: 234.4969\n",
      "Epoch 1484/3500\n",
      "126/126 [==============================] - 0s 261us/step - loss: 0.4060 - mae: 0.3547 - mse: 0.4060 - val_loss: 239.6085 - val_mae: 11.8389 - val_mse: 239.6085\n",
      "Epoch 1485/3500\n",
      "126/126 [==============================] - 0s 174us/step - loss: 0.3958 - mae: 0.4018 - mse: 0.3958 - val_loss: 233.7529 - val_mae: 11.6736 - val_mse: 233.7529\n",
      "Epoch 1486/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4298 - mae: 0.3870 - mse: 0.4298 - val_loss: 234.5237 - val_mae: 11.6836 - val_mse: 234.5237\n",
      "Epoch 1487/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5523 - mae: 0.4936 - mse: 0.5523 - val_loss: 239.2861 - val_mae: 11.7854 - val_mse: 239.2861\n",
      "Epoch 1488/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6565 - mae: 0.5128 - mse: 0.6565 - val_loss: 230.9693 - val_mae: 11.6273 - val_mse: 230.9693\n",
      "Epoch 1489/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7843 - mae: 0.5610 - mse: 0.7843 - val_loss: 233.3203 - val_mae: 11.6490 - val_mse: 233.3203\n",
      "Epoch 1490/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.7055 - mae: 0.5638 - mse: 0.7055 - val_loss: 234.0126 - val_mae: 11.6186 - val_mse: 234.0126\n",
      "Epoch 1491/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6734 - mae: 0.5992 - mse: 0.6734 - val_loss: 240.0773 - val_mae: 11.8406 - val_mse: 240.0773\n",
      "Epoch 1492/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.9551 - mae: 0.7438 - mse: 0.9551 - val_loss: 236.8966 - val_mae: 11.6605 - val_mse: 236.8966\n",
      "Epoch 1493/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.9227 - mae: 0.7741 - mse: 0.9227 - val_loss: 228.0921 - val_mae: 11.6379 - val_mse: 228.0921\n",
      "Epoch 1494/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.7810 - mae: 0.6668 - mse: 0.7810 - val_loss: 241.4207 - val_mae: 11.8329 - val_mse: 241.4207\n",
      "Epoch 1495/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.7110 - mae: 0.6206 - mse: 0.7110 - val_loss: 237.9527 - val_mae: 11.7751 - val_mse: 237.9527\n",
      "Epoch 1496/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.7170 - mae: 0.5863 - mse: 0.7170 - val_loss: 239.2715 - val_mae: 11.7774 - val_mse: 239.2715\n",
      "Epoch 1497/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6029 - mae: 0.5677 - mse: 0.6029 - val_loss: 237.8554 - val_mae: 11.7848 - val_mse: 237.8554\n",
      "Epoch 1498/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.8323 - mae: 0.6763 - mse: 0.8323 - val_loss: 227.0579 - val_mae: 11.5567 - val_mse: 227.0579\n",
      "Epoch 1499/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.7464 - mae: 0.6631 - mse: 0.7464 - val_loss: 230.5744 - val_mae: 11.5835 - val_mse: 230.5744\n",
      "Epoch 1500/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.8872 - mae: 0.7207 - mse: 0.8872 - val_loss: 244.1690 - val_mae: 11.9399 - val_mse: 244.1690\n",
      "Epoch 1501/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7442 - mae: 0.6461 - mse: 0.7442 - val_loss: 229.8479 - val_mae: 11.6138 - val_mse: 229.8479\n",
      "Epoch 1502/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.7091 - mae: 0.6172 - mse: 0.7091 - val_loss: 239.2187 - val_mae: 11.6603 - val_mse: 239.2187\n",
      "Epoch 1503/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 1.0445 - mae: 0.8083 - mse: 1.0445 - val_loss: 234.6043 - val_mae: 11.8311 - val_mse: 234.6043\n",
      "Epoch 1504/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.7236 - mae: 1.0815 - mse: 1.7236 - val_loss: 240.6592 - val_mae: 11.7748 - val_mse: 240.6592\n",
      "Epoch 1505/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.7957 - mae: 0.9918 - mse: 1.7957 - val_loss: 225.8671 - val_mae: 11.5513 - val_mse: 225.8671\n",
      "Epoch 1506/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.3746 - mae: 0.9452 - mse: 1.3746 - val_loss: 238.9001 - val_mae: 11.7940 - val_mse: 238.9001\n",
      "Epoch 1507/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 1.8165 - mae: 1.0939 - mse: 1.8165 - val_loss: 246.4561 - val_mae: 11.9975 - val_mse: 246.4561\n",
      "Epoch 1508/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 1.8966 - mae: 1.1258 - mse: 1.8966 - val_loss: 224.8049 - val_mae: 11.6841 - val_mse: 224.8049\n",
      "Epoch 1509/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 2.4907 - mae: 1.1963 - mse: 2.4907 - val_loss: 241.2635 - val_mae: 11.7971 - val_mse: 241.2635\n",
      "Epoch 1510/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 3.6058 - mae: 1.3559 - mse: 3.6058 - val_loss: 229.2802 - val_mae: 11.6598 - val_mse: 229.2802\n",
      "Epoch 1511/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 2.9658 - mae: 1.4502 - mse: 2.9658 - val_loss: 249.8996 - val_mae: 12.2596 - val_mse: 249.8996\n",
      "Epoch 1512/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 4.0033 - mae: 1.5720 - mse: 4.0033 - val_loss: 256.5212 - val_mae: 12.1200 - val_mse: 256.5212\n",
      "Epoch 1513/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 4.5200 - mae: 1.6737 - mse: 4.5200 - val_loss: 227.7958 - val_mae: 11.5692 - val_mse: 227.7958\n",
      "Epoch 1514/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 3.2140 - mae: 1.4179 - mse: 3.2140 - val_loss: 240.0485 - val_mae: 11.5496 - val_mse: 240.0485\n",
      "Epoch 1515/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 2.4057 - mae: 1.2007 - mse: 2.4057 - val_loss: 231.6492 - val_mae: 11.6161 - val_mse: 231.6492\n",
      "Epoch 1516/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.9454 - mae: 1.0916 - mse: 1.9454 - val_loss: 242.0464 - val_mae: 11.9571 - val_mse: 242.0464\n",
      "Epoch 1517/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 2.7010 - mae: 1.1139 - mse: 2.7010 - val_loss: 231.2423 - val_mae: 11.6810 - val_mse: 231.2423\n",
      "Epoch 1518/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 3.1925 - mae: 1.3740 - mse: 3.1925 - val_loss: 229.7506 - val_mae: 11.5965 - val_mse: 229.7506\n",
      "Epoch 1519/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 3.3168 - mae: 1.4497 - mse: 3.3168 - val_loss: 243.4796 - val_mae: 11.8284 - val_mse: 243.4796\n",
      "Epoch 1520/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 2.5868 - mae: 1.2173 - mse: 2.5868 - val_loss: 228.5193 - val_mae: 11.8075 - val_mse: 228.5193\n",
      "Epoch 1521/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 3.4535 - mae: 1.3705 - mse: 3.453 - 0s 79us/step - loss: 3.6507 - mae: 1.3996 - mse: 3.6507 - val_loss: 247.4384 - val_mae: 11.8837 - val_mse: 247.4384\n",
      "Epoch 1522/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 2.9082 - mae: 1.0937 - mse: 2.9082 - val_loss: 218.8150 - val_mae: 11.3541 - val_mse: 218.8150\n",
      "Epoch 1523/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 2.3629 - mae: 1.1705 - mse: 2.3629 - val_loss: 222.1025 - val_mae: 11.5314 - val_mse: 222.1025\n",
      "Epoch 1524/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.9136 - mae: 1.1315 - mse: 1.9136 - val_loss: 241.4539 - val_mae: 11.8292 - val_mse: 241.4539\n",
      "Epoch 1525/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.8264 - mae: 1.0998 - mse: 1.8264 - val_loss: 219.7478 - val_mae: 11.6078 - val_mse: 219.7478\n",
      "Epoch 1526/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.6699 - mae: 0.9802 - mse: 1.6699 - val_loss: 252.8353 - val_mae: 11.9230 - val_mse: 252.8353\n",
      "Epoch 1527/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.7967 - mae: 1.1024 - mse: 1.7967 - val_loss: 226.0544 - val_mae: 11.6600 - val_mse: 226.0544\n",
      "Epoch 1528/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.8085 - mae: 1.0933 - mse: 1.8085 - val_loss: 237.9674 - val_mae: 11.7657 - val_mse: 237.9674\n",
      "Epoch 1529/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.9105 - mae: 1.0587 - mse: 1.9105 - val_loss: 242.3247 - val_mae: 11.9070 - val_mse: 242.3247\n",
      "Epoch 1530/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 1.5098 - mae: 1.0102 - mse: 1.5098 - val_loss: 229.3084 - val_mae: 11.7313 - val_mse: 229.3084\n",
      "Epoch 1531/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.5116 - mae: 0.9753 - mse: 1.5116 - val_loss: 226.8405 - val_mae: 11.6143 - val_mse: 226.8405\n",
      "Epoch 1532/3500\n",
      "126/126 [==============================] - 0s 190us/step - loss: 1.3808 - mae: 0.9278 - mse: 1.3808 - val_loss: 242.4640 - val_mae: 11.9110 - val_mse: 242.4640\n",
      "Epoch 1533/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.0831 - mae: 0.7888 - mse: 1.0831 - val_loss: 239.7091 - val_mae: 11.9417 - val_mse: 239.7091\n",
      "Epoch 1534/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.8544 - mae: 0.7001 - mse: 0.8544 - val_loss: 235.1837 - val_mae: 11.6908 - val_mse: 235.1837\n",
      "Epoch 1535/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.8999 - mae: 0.6049 - mse: 0.8999 - val_loss: 228.7135 - val_mae: 11.6796 - val_mse: 228.7135\n",
      "Epoch 1536/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.9421 - mae: 0.6734 - mse: 0.9421 - val_loss: 238.6370 - val_mae: 11.8130 - val_mse: 238.6370\n",
      "Epoch 1537/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5762 - mae: 0.5184 - mse: 0.5762 - val_loss: 229.6072 - val_mae: 11.6611 - val_mse: 229.6072\n",
      "Epoch 1538/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5112 - mae: 0.4887 - mse: 0.5112 - val_loss: 231.7451 - val_mae: 11.6907 - val_mse: 231.7451\n",
      "Epoch 1539/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5866 - mae: 0.4848 - mse: 0.5866 - val_loss: 235.0023 - val_mae: 11.8375 - val_mse: 235.0023\n",
      "Epoch 1540/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.7374 - mae: 0.5930 - mse: 0.7374 - val_loss: 235.2870 - val_mae: 11.7432 - val_mse: 235.2870\n",
      "Epoch 1541/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.7024 - mae: 0.6218 - mse: 0.7024 - val_loss: 232.8078 - val_mae: 11.7753 - val_mse: 232.8078\n",
      "Epoch 1542/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.8968 - mae: 0.6801 - mse: 0.8968 - val_loss: 238.5800 - val_mae: 11.7321 - val_mse: 238.5800\n",
      "Epoch 1543/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.8807 - mae: 0.6417 - mse: 0.8807 - val_loss: 222.8674 - val_mae: 11.5124 - val_mse: 222.8674\n",
      "Epoch 1544/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6926 - mae: 0.5610 - mse: 0.6926 - val_loss: 234.2970 - val_mae: 11.7493 - val_mse: 234.2970\n",
      "Epoch 1545/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.1781 - mae: 0.6805 - mse: 1.1781 - val_loss: 232.7416 - val_mae: 11.7088 - val_mse: 232.7416\n",
      "Epoch 1546/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7599 - mae: 0.6467 - mse: 0.7599 - val_loss: 230.5498 - val_mae: 11.6181 - val_mse: 230.5498\n",
      "Epoch 1547/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6209 - mae: 0.5691 - mse: 0.6209 - val_loss: 234.8816 - val_mae: 11.8114 - val_mse: 234.8816\n",
      "Epoch 1548/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5968 - mae: 0.5220 - mse: 0.5968 - val_loss: 230.6608 - val_mae: 11.5860 - val_mse: 230.6608\n",
      "Epoch 1549/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5083 - mae: 0.4919 - mse: 0.5083 - val_loss: 226.2526 - val_mae: 11.5639 - val_mse: 226.2526\n",
      "Epoch 1550/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4431 - mae: 0.4088 - mse: 0.4431 - val_loss: 233.5190 - val_mae: 11.7750 - val_mse: 233.5190\n",
      "Epoch 1551/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3701 - mae: 0.3561 - mse: 0.3701 - val_loss: 231.1285 - val_mae: 11.6319 - val_mse: 231.1285\n",
      "Epoch 1552/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3532 - mae: 0.3506 - mse: 0.3532 - val_loss: 230.1445 - val_mae: 11.6589 - val_mse: 230.1445\n",
      "Epoch 1553/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3474 - mae: 0.3523 - mse: 0.3474 - val_loss: 232.3800 - val_mae: 11.7257 - val_mse: 232.3800\n",
      "Epoch 1554/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3955 - mae: 0.3389 - mse: 0.3955 - val_loss: 231.9649 - val_mae: 11.6573 - val_mse: 231.9649\n",
      "Epoch 1555/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3842 - mae: 0.3619 - mse: 0.3842 - val_loss: 229.4490 - val_mae: 11.6768 - val_mse: 229.4490\n",
      "Epoch 1556/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4260 - mae: 0.3969 - mse: 0.4260 - val_loss: 230.4567 - val_mae: 11.6199 - val_mse: 230.4567\n",
      "Epoch 1557/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3930 - mae: 0.3755 - mse: 0.3930 - val_loss: 230.2229 - val_mae: 11.6727 - val_mse: 230.2229\n",
      "Epoch 1558/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3547 - mae: 0.3860 - mse: 0.3547 - val_loss: 235.1608 - val_mae: 11.7280 - val_mse: 235.1608\n",
      "Epoch 1559/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4150 - mae: 0.4007 - mse: 0.4150 - val_loss: 227.9053 - val_mae: 11.6045 - val_mse: 227.9053\n",
      "Epoch 1560/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3699 - mae: 0.3408 - mse: 0.3699 - val_loss: 232.9162 - val_mae: 11.6603 - val_mse: 232.9162\n",
      "Epoch 1561/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3486 - mae: 0.3371 - mse: 0.3486 - val_loss: 228.1069 - val_mae: 11.6536 - val_mse: 228.1069\n",
      "Epoch 1562/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3689 - mae: 0.3682 - mse: 0.3689 - val_loss: 232.4020 - val_mae: 11.6789 - val_mse: 232.4020\n",
      "Epoch 1563/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3690 - mae: 0.3817 - mse: 0.3690 - val_loss: 230.0394 - val_mae: 11.6935 - val_mse: 230.0394\n",
      "Epoch 1564/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3888 - mae: 0.3751 - mse: 0.3888 - val_loss: 235.2569 - val_mae: 11.7303 - val_mse: 235.2569\n",
      "Epoch 1565/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3307 - mae: 0.3327 - mse: 0.3307 - val_loss: 230.3266 - val_mae: 11.7003 - val_mse: 230.3266\n",
      "Epoch 1566/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3602 - mae: 0.3343 - mse: 0.3602 - val_loss: 232.2914 - val_mae: 11.6710 - val_mse: 232.2914\n",
      "Epoch 1567/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3341 - mae: 0.2916 - mse: 0.3341 - val_loss: 230.3362 - val_mae: 11.6918 - val_mse: 230.3362\n",
      "Epoch 1568/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3180 - mae: 0.2728 - mse: 0.3180 - val_loss: 234.3395 - val_mae: 11.7309 - val_mse: 234.3395\n",
      "Epoch 1569/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3446 - mae: 0.2847 - mse: 0.3446 - val_loss: 229.8907 - val_mae: 11.7019 - val_mse: 229.8907\n",
      "Epoch 1570/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3363 - mae: 0.3134 - mse: 0.3363 - val_loss: 232.7673 - val_mae: 11.6631 - val_mse: 232.7673\n",
      "Epoch 1571/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3738 - mae: 0.3422 - mse: 0.3738 - val_loss: 232.2653 - val_mae: 11.7305 - val_mse: 232.2653\n",
      "Epoch 1572/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4351 - mae: 0.3869 - mse: 0.4351 - val_loss: 231.9969 - val_mae: 11.7045 - val_mse: 231.9969\n",
      "Epoch 1573/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3769 - mae: 0.3637 - mse: 0.3769 - val_loss: 232.4643 - val_mae: 11.6728 - val_mse: 232.4643\n",
      "Epoch 1574/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3664 - mae: 0.3706 - mse: 0.3664 - val_loss: 232.6670 - val_mae: 11.7458 - val_mse: 232.6670\n",
      "Epoch 1575/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3706 - mae: 0.3666 - mse: 0.3706 - val_loss: 232.9076 - val_mae: 11.6681 - val_mse: 232.9076\n",
      "Epoch 1576/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3491 - mae: 0.3654 - mse: 0.3491 - val_loss: 227.2545 - val_mae: 11.6202 - val_mse: 227.2545\n",
      "Epoch 1577/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3641 - mae: 0.3588 - mse: 0.3641 - val_loss: 234.4482 - val_mae: 11.6629 - val_mse: 234.4482\n",
      "Epoch 1578/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4444 - mae: 0.4317 - mse: 0.4444 - val_loss: 229.8789 - val_mae: 11.6869 - val_mse: 229.8789\n",
      "Epoch 1579/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4454 - mae: 0.4110 - mse: 0.4454 - val_loss: 232.0916 - val_mae: 11.6851 - val_mse: 232.0916\n",
      "Epoch 1580/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.3629 - mae: 0.3204 - mse: 0.3629 - val_loss: 228.8351 - val_mae: 11.6013 - val_mse: 228.8351\n",
      "Epoch 1581/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3288 - mae: 0.3189 - mse: 0.3288 - val_loss: 229.3202 - val_mae: 11.6586 - val_mse: 229.3202\n",
      "Epoch 1582/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3194 - mae: 0.2866 - mse: 0.3194 - val_loss: 234.6938 - val_mae: 11.7401 - val_mse: 234.6938\n",
      "Epoch 1583/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3153 - mae: 0.2802 - mse: 0.3153 - val_loss: 231.4460 - val_mae: 11.7086 - val_mse: 231.4460\n",
      "Epoch 1584/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3068 - mae: 0.2797 - mse: 0.3068 - val_loss: 228.8568 - val_mae: 11.5773 - val_mse: 228.8568\n",
      "Epoch 1585/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2913 - mae: 0.2640 - mse: 0.2913 - val_loss: 230.8534 - val_mae: 11.6975 - val_mse: 230.8534\n",
      "Epoch 1586/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.2874 - mae: 0.2501 - mse: 0.2874 - val_loss: 234.8503 - val_mae: 11.7578 - val_mse: 234.8503\n",
      "Epoch 1587/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.2968 - mae: 0.2504 - mse: 0.2968 - val_loss: 230.4677 - val_mae: 11.6662 - val_mse: 230.4677\n",
      "Epoch 1588/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.2994 - mae: 0.2466 - mse: 0.2994 - val_loss: 233.3605 - val_mae: 11.7537 - val_mse: 233.3605\n",
      "Epoch 1589/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.2772 - mae: 0.2647 - mse: 0.2772 - val_loss: 231.3738 - val_mae: 11.6942 - val_mse: 231.3738\n",
      "Epoch 1590/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2810 - mae: 0.2479 - mse: 0.2810 - val_loss: 231.3179 - val_mae: 11.6489 - val_mse: 231.3179\n",
      "Epoch 1591/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2968 - mae: 0.2517 - mse: 0.2968 - val_loss: 232.7526 - val_mae: 11.7199 - val_mse: 232.7526\n",
      "Epoch 1592/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2872 - mae: 0.2460 - mse: 0.2872 - val_loss: 231.8642 - val_mae: 11.6957 - val_mse: 231.8642\n",
      "Epoch 1593/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2922 - mae: 0.2412 - mse: 0.2922 - val_loss: 231.1793 - val_mae: 11.6744 - val_mse: 231.1793\n",
      "Epoch 1594/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2745 - mae: 0.2169 - mse: 0.2745 - val_loss: 230.8326 - val_mae: 11.6828 - val_mse: 230.8326\n",
      "Epoch 1595/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2828 - mae: 0.2356 - mse: 0.2828 - val_loss: 233.0015 - val_mae: 11.6988 - val_mse: 233.0015\n",
      "Epoch 1596/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.2884 - mae: 0.2127 - mse: 0.2884 - val_loss: 232.0429 - val_mae: 11.7010 - val_mse: 232.0429\n",
      "Epoch 1597/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.2502 - mae: 0.1871 - mse: 0.2502 - val_loss: 232.0925 - val_mae: 11.6755 - val_mse: 232.0925\n",
      "Epoch 1598/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.2628 - mae: 0.2181 - mse: 0.2628 - val_loss: 231.4550 - val_mae: 11.6880 - val_mse: 231.4550\n",
      "Epoch 1599/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2531 - mae: 0.1991 - mse: 0.2531 - val_loss: 231.3229 - val_mae: 11.6875 - val_mse: 231.3229\n",
      "Epoch 1600/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3134 - mae: 0.2120 - mse: 0.3134 - val_loss: 234.1719 - val_mae: 11.7124 - val_mse: 234.1719\n",
      "Epoch 1601/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2782 - mae: 0.2438 - mse: 0.2782 - val_loss: 229.8368 - val_mae: 11.6833 - val_mse: 229.8368\n",
      "Epoch 1602/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2814 - mae: 0.2550 - mse: 0.2814 - val_loss: 235.3805 - val_mae: 11.7118 - val_mse: 235.3805\n",
      "Epoch 1603/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2947 - mae: 0.2547 - mse: 0.2947 - val_loss: 231.2941 - val_mae: 11.6905 - val_mse: 231.2941\n",
      "Epoch 1604/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2907 - mae: 0.2619 - mse: 0.2907 - val_loss: 232.5220 - val_mae: 11.6593 - val_mse: 232.5220\n",
      "Epoch 1605/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3606 - mae: 0.3405 - mse: 0.3606 - val_loss: 230.2070 - val_mae: 11.6967 - val_mse: 230.2070\n",
      "Epoch 1606/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2929 - mae: 0.2944 - mse: 0.2929 - val_loss: 232.7610 - val_mae: 11.6613 - val_mse: 232.7610\n",
      "Epoch 1607/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4594 - mae: 0.4022 - mse: 0.4594 - val_loss: 231.7080 - val_mae: 11.6812 - val_mse: 231.7080\n",
      "Epoch 1608/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3000 - mae: 0.2973 - mse: 0.3000 - val_loss: 235.2321 - val_mae: 11.7664 - val_mse: 235.2321\n",
      "Epoch 1609/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3574 - mae: 0.3443 - mse: 0.3574 - val_loss: 233.0196 - val_mae: 11.7174 - val_mse: 233.0196\n",
      "Epoch 1610/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3089 - mae: 0.3078 - mse: 0.3089 - val_loss: 232.8362 - val_mae: 11.7136 - val_mse: 232.8362\n",
      "Epoch 1611/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3061 - mae: 0.2837 - mse: 0.3061 - val_loss: 232.8160 - val_mae: 11.7293 - val_mse: 232.8160\n",
      "Epoch 1612/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3135 - mae: 0.2729 - mse: 0.3135 - val_loss: 231.4828 - val_mae: 11.6515 - val_mse: 231.4828\n",
      "Epoch 1613/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3436 - mae: 0.3285 - mse: 0.3436 - val_loss: 229.1628 - val_mae: 11.6792 - val_mse: 229.1628\n",
      "Epoch 1614/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3456 - mae: 0.3318 - mse: 0.3456 - val_loss: 237.4485 - val_mae: 11.7487 - val_mse: 237.4485\n",
      "Epoch 1615/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3468 - mae: 0.3282 - mse: 0.3468 - val_loss: 226.9003 - val_mae: 11.6483 - val_mse: 226.9003\n",
      "Epoch 1616/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3413 - mae: 0.3240 - mse: 0.3413 - val_loss: 234.3840 - val_mae: 11.7055 - val_mse: 234.3840\n",
      "Epoch 1617/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3550 - mae: 0.3397 - mse: 0.3550 - val_loss: 235.4852 - val_mae: 11.8063 - val_mse: 235.4852\n",
      "Epoch 1618/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3408 - mae: 0.3331 - mse: 0.3408 - val_loss: 233.1157 - val_mae: 11.7145 - val_mse: 233.1157\n",
      "Epoch 1619/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3392 - mae: 0.3065 - mse: 0.3392 - val_loss: 234.0242 - val_mae: 11.7153 - val_mse: 234.0242\n",
      "Epoch 1620/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.2924 - mae: 0.2428 - mse: 0.2924 - val_loss: 231.6023 - val_mae: 11.7069 - val_mse: 231.6023\n",
      "Epoch 1621/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2901 - mae: 0.2725 - mse: 0.2901 - val_loss: 232.7722 - val_mae: 11.7025 - val_mse: 232.7722\n",
      "Epoch 1622/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2784 - mae: 0.2634 - mse: 0.2784 - val_loss: 234.1678 - val_mae: 11.7361 - val_mse: 234.1678\n",
      "Epoch 1623/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3056 - mae: 0.2865 - mse: 0.3056 - val_loss: 232.6455 - val_mae: 11.6894 - val_mse: 232.6455\n",
      "Epoch 1624/3500\n",
      "126/126 [==============================] - 0s 166us/step - loss: 0.3117 - mae: 0.2577 - mse: 0.3117 - val_loss: 233.2625 - val_mae: 11.6996 - val_mse: 233.2625\n",
      "Epoch 1625/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.3052 - mae: 0.2720 - mse: 0.3052 - val_loss: 231.6983 - val_mae: 11.6806 - val_mse: 231.6983\n",
      "Epoch 1626/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2951 - mae: 0.2280 - mse: 0.2951 - val_loss: 234.8591 - val_mae: 11.7159 - val_mse: 234.8591\n",
      "Epoch 1627/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2923 - mae: 0.2738 - mse: 0.2923 - val_loss: 234.1058 - val_mae: 11.7662 - val_mse: 234.1058\n",
      "Epoch 1628/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3122 - mae: 0.2736 - mse: 0.3122 - val_loss: 232.6649 - val_mae: 11.7029 - val_mse: 232.6649\n",
      "Epoch 1629/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3025 - mae: 0.2764 - mse: 0.3025 - val_loss: 232.4103 - val_mae: 11.7061 - val_mse: 232.4103\n",
      "Epoch 1630/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2619 - mae: 0.2214 - mse: 0.2619 - val_loss: 233.4093 - val_mae: 11.7082 - val_mse: 233.4093\n",
      "Epoch 1631/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.4960 - mae: 0.2635 - mse: 0.496 - 0s 103us/step - loss: 0.2768 - mae: 0.2288 - mse: 0.2768 - val_loss: 232.1909 - val_mae: 11.6796 - val_mse: 232.1909\n",
      "Epoch 1632/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2828 - mae: 0.2448 - mse: 0.2828 - val_loss: 232.2763 - val_mae: 11.7332 - val_mse: 232.2763\n",
      "Epoch 1633/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.3282 - mae: 0.2711 - mse: 0.3282 - val_loss: 235.2847 - val_mae: 11.7245 - val_mse: 235.2847\n",
      "Epoch 1634/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3063 - mae: 0.2896 - mse: 0.3063 - val_loss: 233.1734 - val_mae: 11.7361 - val_mse: 233.1734\n",
      "Epoch 1635/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3072 - mae: 0.2661 - mse: 0.3072 - val_loss: 232.6468 - val_mae: 11.7189 - val_mse: 232.6468\n",
      "Epoch 1636/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3225 - mae: 0.2687 - mse: 0.3225 - val_loss: 236.2907 - val_mae: 11.7390 - val_mse: 236.2907\n",
      "Epoch 1637/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2995 - mae: 0.2733 - mse: 0.2995 - val_loss: 231.1844 - val_mae: 11.7004 - val_mse: 231.1844\n",
      "Epoch 1638/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3230 - mae: 0.3120 - mse: 0.3230 - val_loss: 229.8464 - val_mae: 11.6486 - val_mse: 229.8464\n",
      "Epoch 1639/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3112 - mae: 0.2726 - mse: 0.3112 - val_loss: 234.8157 - val_mae: 11.7364 - val_mse: 234.8157\n",
      "Epoch 1640/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2895 - mae: 0.2590 - mse: 0.2895 - val_loss: 234.1537 - val_mae: 11.7355 - val_mse: 234.1537\n",
      "Epoch 1641/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2964 - mae: 0.2869 - mse: 0.2964 - val_loss: 231.0982 - val_mae: 11.6855 - val_mse: 231.0982\n",
      "Epoch 1642/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3942 - mae: 0.3568 - mse: 0.3942 - val_loss: 234.5760 - val_mae: 11.7262 - val_mse: 234.5760\n",
      "Epoch 1643/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4147 - mae: 0.3770 - mse: 0.4147 - val_loss: 233.3672 - val_mae: 11.6642 - val_mse: 233.3672\n",
      "Epoch 1644/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3561 - mae: 0.3344 - mse: 0.3561 - val_loss: 231.0646 - val_mae: 11.7201 - val_mse: 231.0646\n",
      "Epoch 1645/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3376 - mae: 0.3374 - mse: 0.3376 - val_loss: 234.3862 - val_mae: 11.7292 - val_mse: 234.3862\n",
      "Epoch 1646/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.2920 - mae: 0.2822 - mse: 0.2920 - val_loss: 231.1828 - val_mae: 11.7102 - val_mse: 231.1828\n",
      "Epoch 1647/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3705 - mae: 0.3561 - mse: 0.3705 - val_loss: 234.6405 - val_mae: 11.7561 - val_mse: 234.6405\n",
      "Epoch 1648/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3309 - mae: 0.3264 - mse: 0.3309 - val_loss: 231.7602 - val_mae: 11.6917 - val_mse: 231.7602\n",
      "Epoch 1649/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3040 - mae: 0.3009 - mse: 0.3040 - val_loss: 233.5914 - val_mae: 11.7143 - val_mse: 233.5914\n",
      "Epoch 1650/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3291 - mae: 0.3265 - mse: 0.3291 - val_loss: 235.7422 - val_mae: 11.7489 - val_mse: 235.7422\n",
      "Epoch 1651/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4744 - mae: 0.4033 - mse: 0.4744 - val_loss: 228.6921 - val_mae: 11.6796 - val_mse: 228.6921\n",
      "Epoch 1652/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4841 - mae: 0.4526 - mse: 0.4841 - val_loss: 237.3756 - val_mae: 11.6932 - val_mse: 237.3756\n",
      "Epoch 1653/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4595 - mae: 0.4590 - mse: 0.4595 - val_loss: 226.5108 - val_mae: 11.6811 - val_mse: 226.5108\n",
      "Epoch 1654/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.7810 - mae: 0.6356 - mse: 0.7810 - val_loss: 243.7015 - val_mae: 11.8388 - val_mse: 243.7015\n",
      "Epoch 1655/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.8357 - mae: 0.6695 - mse: 0.8357 - val_loss: 235.4509 - val_mae: 11.8016 - val_mse: 235.4509\n",
      "Epoch 1656/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.7891 - mae: 0.6061 - mse: 0.7891 - val_loss: 231.4403 - val_mae: 11.7391 - val_mse: 231.4403\n",
      "Epoch 1657/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6988 - mae: 0.5915 - mse: 0.6988 - val_loss: 240.1540 - val_mae: 11.7891 - val_mse: 240.1540\n",
      "Epoch 1658/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.7837 - mae: 0.6331 - mse: 0.7837 - val_loss: 229.2951 - val_mae: 11.7154 - val_mse: 229.2951\n",
      "Epoch 1659/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5770 - mae: 0.5319 - mse: 0.5770 - val_loss: 238.8004 - val_mae: 11.7696 - val_mse: 238.8004\n",
      "Epoch 1660/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.5994 - mae: 0.5677 - mse: 0.5994 - val_loss: 233.1620 - val_mae: 11.7675 - val_mse: 233.1620\n",
      "Epoch 1661/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.7138 - mae: 0.5779 - mse: 0.7138 - val_loss: 237.3237 - val_mae: 11.7374 - val_mse: 237.3237\n",
      "Epoch 1662/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6477 - mae: 0.5718 - mse: 0.6477 - val_loss: 237.2775 - val_mae: 11.8503 - val_mse: 237.2775\n",
      "Epoch 1663/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6097 - mae: 0.5658 - mse: 0.6097 - val_loss: 237.0425 - val_mae: 11.7857 - val_mse: 237.0425\n",
      "Epoch 1664/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.7109 - mae: 0.5695 - mse: 0.7109 - val_loss: 229.6070 - val_mae: 11.6011 - val_mse: 229.6070\n",
      "Epoch 1665/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.9119 - mae: 0.7073 - mse: 0.9119 - val_loss: 231.4467 - val_mae: 11.7056 - val_mse: 231.4467\n",
      "Epoch 1666/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.9412 - mae: 0.6728 - mse: 0.9412 - val_loss: 239.3436 - val_mae: 11.7288 - val_mse: 239.3436\n",
      "Epoch 1667/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.7819 - mae: 0.6589 - mse: 0.7819 - val_loss: 225.5600 - val_mae: 11.6022 - val_mse: 225.5600\n",
      "Epoch 1668/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.7003 - mae: 0.6103 - mse: 0.7003 - val_loss: 230.3267 - val_mae: 11.6349 - val_mse: 230.3267\n",
      "Epoch 1669/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4917 - mae: 0.4708 - mse: 0.4917 - val_loss: 237.1980 - val_mae: 11.8166 - val_mse: 237.1980\n",
      "Epoch 1670/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.6363 - mae: 0.5254 - mse: 0.6363 - val_loss: 232.8295 - val_mae: 11.7003 - val_mse: 232.8295\n",
      "Epoch 1671/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5280 - mae: 0.5060 - mse: 0.5280 - val_loss: 232.5399 - val_mae: 11.7396 - val_mse: 232.5399\n",
      "Epoch 1672/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5235 - mae: 0.4913 - mse: 0.5235 - val_loss: 234.2202 - val_mae: 11.6963 - val_mse: 234.2202\n",
      "Epoch 1673/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5209 - mae: 0.4772 - mse: 0.5209 - val_loss: 231.7996 - val_mae: 11.7213 - val_mse: 231.7996\n",
      "Epoch 1674/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4633 - mae: 0.4657 - mse: 0.4633 - val_loss: 234.4004 - val_mae: 11.6698 - val_mse: 234.4004\n",
      "Epoch 1675/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5195 - mae: 0.5066 - mse: 0.5195 - val_loss: 233.5865 - val_mae: 11.7181 - val_mse: 233.5865\n",
      "Epoch 1676/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5441 - mae: 0.5087 - mse: 0.5441 - val_loss: 235.1649 - val_mae: 11.7854 - val_mse: 235.1649\n",
      "Epoch 1677/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4737 - mae: 0.4207 - mse: 0.4737 - val_loss: 234.3922 - val_mae: 11.7237 - val_mse: 234.3922\n",
      "Epoch 1678/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5063 - mae: 0.4657 - mse: 0.5063 - val_loss: 231.7730 - val_mae: 11.7050 - val_mse: 231.7730\n",
      "Epoch 1679/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5324 - mae: 0.4922 - mse: 0.5324 - val_loss: 237.7949 - val_mae: 11.7283 - val_mse: 237.7949\n",
      "Epoch 1680/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4801 - mae: 0.4323 - mse: 0.4801 - val_loss: 235.6882 - val_mae: 11.8991 - val_mse: 235.6882\n",
      "Epoch 1681/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.5253 - mae: 0.4768 - mse: 0.5253 - val_loss: 234.7527 - val_mae: 11.7470 - val_mse: 234.7527\n",
      "Epoch 1682/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.6527 - mae: 0.4782 - mse: 0.6527 - val_loss: 230.8396 - val_mae: 11.6607 - val_mse: 230.8396\n",
      "Epoch 1683/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4624 - mae: 0.4214 - mse: 0.4624 - val_loss: 232.6651 - val_mae: 11.7807 - val_mse: 232.6651\n",
      "Epoch 1684/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4113 - mae: 0.4341 - mse: 0.4113 - val_loss: 236.2380 - val_mae: 11.6876 - val_mse: 236.2380\n",
      "Epoch 1685/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5724 - mae: 0.5362 - mse: 0.5724 - val_loss: 232.3842 - val_mae: 11.7558 - val_mse: 232.3842\n",
      "Epoch 1686/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5898 - mae: 0.5799 - mse: 0.5898 - val_loss: 234.4604 - val_mae: 11.6719 - val_mse: 234.4604\n",
      "Epoch 1687/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5684 - mae: 0.5174 - mse: 0.5684 - val_loss: 229.4494 - val_mae: 11.6853 - val_mse: 229.4494\n",
      "Epoch 1688/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6187 - mae: 0.5458 - mse: 0.6187 - val_loss: 237.1172 - val_mae: 11.7874 - val_mse: 237.1172\n",
      "Epoch 1689/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4914 - mae: 0.4641 - mse: 0.4914 - val_loss: 233.6200 - val_mae: 11.7929 - val_mse: 233.6200\n",
      "Epoch 1690/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4247 - mae: 0.4266 - mse: 0.4247 - val_loss: 236.2902 - val_mae: 11.8247 - val_mse: 236.2902\n",
      "Epoch 1691/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.5786 - mae: 0.4998 - mse: 0.5786 - val_loss: 233.5202 - val_mae: 11.6500 - val_mse: 233.5202\n",
      "Epoch 1692/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6817 - mae: 0.6034 - mse: 0.6817 - val_loss: 231.9808 - val_mae: 11.7994 - val_mse: 231.9808\n",
      "Epoch 1693/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.9147 - mae: 0.7057 - mse: 0.9147 - val_loss: 239.3232 - val_mae: 11.7596 - val_mse: 239.3232\n",
      "Epoch 1694/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.1885 - mae: 0.8066 - mse: 1.1885 - val_loss: 233.1317 - val_mae: 11.8630 - val_mse: 233.1317\n",
      "Epoch 1695/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.3316 - mae: 0.9175 - mse: 1.3316 - val_loss: 234.9266 - val_mae: 11.6755 - val_mse: 234.9266\n",
      "Epoch 1696/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.9830 - mae: 0.7100 - mse: 0.9830 - val_loss: 226.9748 - val_mae: 11.6428 - val_mse: 226.9748\n",
      "Epoch 1697/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.7280 - mae: 0.6050 - mse: 0.7280 - val_loss: 240.1411 - val_mae: 11.8643 - val_mse: 240.1411\n",
      "Epoch 1698/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5885 - mae: 0.5161 - mse: 0.5885 - val_loss: 237.0609 - val_mae: 11.7947 - val_mse: 237.0609\n",
      "Epoch 1699/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4769 - mae: 0.4618 - mse: 0.4769 - val_loss: 228.6642 - val_mae: 11.5766 - val_mse: 228.6642\n",
      "Epoch 1700/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5034 - mae: 0.4818 - mse: 0.5034 - val_loss: 229.6877 - val_mae: 11.5739 - val_mse: 229.6877\n",
      "Epoch 1701/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3306 - mae: 0.3416 - mse: 0.3306 - val_loss: 234.6313 - val_mae: 11.7964 - val_mse: 234.6313\n",
      "Epoch 1702/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3842 - mae: 0.3844 - mse: 0.3842 - val_loss: 237.6736 - val_mae: 11.8124 - val_mse: 237.6736\n",
      "Epoch 1703/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3286 - mae: 0.3299 - mse: 0.3286 - val_loss: 227.3125 - val_mae: 11.6212 - val_mse: 227.3125\n",
      "Epoch 1704/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3798 - mae: 0.3794 - mse: 0.3798 - val_loss: 234.8036 - val_mae: 11.6884 - val_mse: 234.8036\n",
      "Epoch 1705/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3750 - mae: 0.3511 - mse: 0.3750 - val_loss: 231.0270 - val_mae: 11.6929 - val_mse: 231.0270\n",
      "Epoch 1706/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3294 - mae: 0.3104 - mse: 0.3294 - val_loss: 234.0394 - val_mae: 11.7133 - val_mse: 234.0394\n",
      "Epoch 1707/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4094 - mae: 0.3389 - mse: 0.4094 - val_loss: 237.3861 - val_mae: 11.7664 - val_mse: 237.3861\n",
      "Epoch 1708/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3393 - mae: 0.3193 - mse: 0.3393 - val_loss: 230.2118 - val_mae: 11.6769 - val_mse: 230.2118\n",
      "Epoch 1709/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.3686 - mae: 0.3421 - mse: 0.3686 - val_loss: 233.6829 - val_mae: 11.6729 - val_mse: 233.6829\n",
      "Epoch 1710/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4270 - mae: 0.3488 - mse: 0.4270 - val_loss: 235.3785 - val_mae: 11.7412 - val_mse: 235.3785\n",
      "Epoch 1711/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3248 - mae: 0.3096 - mse: 0.3248 - val_loss: 231.1765 - val_mae: 11.6991 - val_mse: 231.1765\n",
      "Epoch 1712/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3340 - mae: 0.3305 - mse: 0.3340 - val_loss: 235.7179 - val_mae: 11.7514 - val_mse: 235.7179\n",
      "Epoch 1713/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3212 - mae: 0.2989 - mse: 0.3212 - val_loss: 232.4263 - val_mae: 11.7073 - val_mse: 232.4263\n",
      "Epoch 1714/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3361 - mae: 0.3030 - mse: 0.3361 - val_loss: 230.6316 - val_mae: 11.6484 - val_mse: 230.6316\n",
      "Epoch 1715/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4091 - mae: 0.3661 - mse: 0.4091 - val_loss: 235.4051 - val_mae: 11.7328 - val_mse: 235.4051\n",
      "Epoch 1716/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4811 - mae: 0.4768 - mse: 0.4811 - val_loss: 230.9774 - val_mae: 11.7453 - val_mse: 230.9774\n",
      "Epoch 1717/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5552 - mae: 0.5502 - mse: 0.5552 - val_loss: 236.7374 - val_mae: 11.6956 - val_mse: 236.7374\n",
      "Epoch 1718/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4940 - mae: 0.4748 - mse: 0.4940 - val_loss: 230.0761 - val_mae: 11.7100 - val_mse: 230.0761\n",
      "Epoch 1719/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.7216 - mae: 0.6128 - mse: 0.7216 - val_loss: 235.0441 - val_mae: 11.6609 - val_mse: 235.0441\n",
      "Epoch 1720/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.7033 - mae: 0.5720 - mse: 0.7033 - val_loss: 228.0748 - val_mae: 11.6001 - val_mse: 228.0748\n",
      "Epoch 1721/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.6858 - mae: 0.5497 - mse: 0.6858 - val_loss: 248.4345 - val_mae: 11.8744 - val_mse: 248.4345\n",
      "Epoch 1722/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.6861 - mae: 0.5830 - mse: 0.6861 - val_loss: 225.3169 - val_mae: 11.5392 - val_mse: 225.3169\n",
      "Epoch 1723/3500\n",
      "126/126 [==============================] - 0s 166us/step - loss: 0.6808 - mae: 0.5747 - mse: 0.6808 - val_loss: 240.1583 - val_mae: 11.8072 - val_mse: 240.1583\n",
      "Epoch 1724/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6042 - mae: 0.5696 - mse: 0.6042 - val_loss: 235.0029 - val_mae: 11.7167 - val_mse: 235.0029\n",
      "Epoch 1725/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.5792 - mae: 0.5327 - mse: 0.5792 - val_loss: 224.1525 - val_mae: 11.5964 - val_mse: 224.1525\n",
      "Epoch 1726/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5810 - mae: 0.5361 - mse: 0.5810 - val_loss: 245.1672 - val_mae: 11.8720 - val_mse: 245.1672\n",
      "Epoch 1727/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7919 - mae: 0.6517 - mse: 0.7919 - val_loss: 227.2874 - val_mae: 11.6560 - val_mse: 227.2874\n",
      "Epoch 1728/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6411 - mae: 0.5901 - mse: 0.6411 - val_loss: 231.4436 - val_mae: 11.6526 - val_mse: 231.4436\n",
      "Epoch 1729/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4985 - mae: 0.4805 - mse: 0.4985 - val_loss: 233.4852 - val_mae: 11.7167 - val_mse: 233.4852\n",
      "Epoch 1730/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5724 - mae: 0.4840 - mse: 0.5724 - val_loss: 233.2767 - val_mae: 11.6958 - val_mse: 233.2767\n",
      "Epoch 1731/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5388 - mae: 0.4928 - mse: 0.5388 - val_loss: 231.0722 - val_mae: 11.7088 - val_mse: 231.0722\n",
      "Epoch 1732/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.4128 - mae: 0.4111 - mse: 0.4128 - val_loss: 236.2925 - val_mae: 11.7354 - val_mse: 236.2925\n",
      "Epoch 1733/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3381 - mae: 0.3436 - mse: 0.3381 - val_loss: 232.0377 - val_mae: 11.7082 - val_mse: 232.0377\n",
      "Epoch 1734/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3570 - mae: 0.3293 - mse: 0.3570 - val_loss: 234.7983 - val_mae: 11.7690 - val_mse: 234.7983\n",
      "Epoch 1735/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3328 - mae: 0.3264 - mse: 0.3328 - val_loss: 235.3372 - val_mae: 11.7769 - val_mse: 235.3372\n",
      "Epoch 1736/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3469 - mae: 0.3440 - mse: 0.3469 - val_loss: 230.6071 - val_mae: 11.6662 - val_mse: 230.6071\n",
      "Epoch 1737/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3610 - mae: 0.3685 - mse: 0.3610 - val_loss: 233.0913 - val_mae: 11.7302 - val_mse: 233.0913\n",
      "Epoch 1738/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3272 - mae: 0.3177 - mse: 0.3272 - val_loss: 233.6608 - val_mae: 11.7060 - val_mse: 233.6608\n",
      "Epoch 1739/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4189 - mae: 0.3929 - mse: 0.4189 - val_loss: 235.6758 - val_mae: 11.7619 - val_mse: 235.6758\n",
      "Epoch 1740/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3334 - mae: 0.3384 - mse: 0.3334 - val_loss: 231.3649 - val_mae: 11.7220 - val_mse: 231.3649\n",
      "Epoch 1741/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4273 - mae: 0.4235 - mse: 0.4273 - val_loss: 233.9407 - val_mae: 11.7034 - val_mse: 233.9407\n",
      "Epoch 1742/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3393 - mae: 0.3413 - mse: 0.3393 - val_loss: 231.8279 - val_mae: 11.7161 - val_mse: 231.8279\n",
      "Epoch 1743/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3605 - mae: 0.3579 - mse: 0.3605 - val_loss: 233.4095 - val_mae: 11.7255 - val_mse: 233.4095\n",
      "Epoch 1744/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4366 - mae: 0.3986 - mse: 0.4366 - val_loss: 234.1028 - val_mae: 11.7074 - val_mse: 234.1028\n",
      "Epoch 1745/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5186 - mae: 0.4570 - mse: 0.5186 - val_loss: 233.2977 - val_mae: 11.7325 - val_mse: 233.2977\n",
      "Epoch 1746/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3736 - mae: 0.3834 - mse: 0.3736 - val_loss: 233.1183 - val_mae: 11.7431 - val_mse: 233.1183\n",
      "Epoch 1747/3500\n",
      "126/126 [==============================] - 0s 222us/step - loss: 0.3817 - mae: 0.3670 - mse: 0.3817 - val_loss: 237.4023 - val_mae: 11.8305 - val_mse: 237.4023\n",
      "Epoch 1748/3500\n",
      "126/126 [==============================] - 0s 206us/step - loss: 0.3542 - mae: 0.3537 - mse: 0.3542 - val_loss: 237.5960 - val_mae: 11.8002 - val_mse: 237.5960\n",
      "Epoch 1749/3500\n",
      "126/126 [==============================] - 0s 150us/step - loss: 0.3937 - mae: 0.3634 - mse: 0.3937 - val_loss: 232.5190 - val_mae: 11.7240 - val_mse: 232.5190\n",
      "Epoch 1750/3500\n",
      "126/126 [==============================] - 0s 174us/step - loss: 0.4078 - mae: 0.3769 - mse: 0.4078 - val_loss: 236.8509 - val_mae: 11.7630 - val_mse: 236.8509\n",
      "Epoch 1751/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.4154 - mae: 0.4000 - mse: 0.4154 - val_loss: 233.4759 - val_mae: 11.7855 - val_mse: 233.4759\n",
      "Epoch 1752/3500\n",
      "126/126 [==============================] - 0s 214us/step - loss: 0.5316 - mae: 0.4966 - mse: 0.5316 - val_loss: 233.8324 - val_mae: 11.7350 - val_mse: 233.8324\n",
      "Epoch 1753/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.6531 - mae: 0.4942 - mse: 0.6531 - val_loss: 236.6651 - val_mae: 11.8094 - val_mse: 236.6651\n",
      "Epoch 1754/3500\n",
      "126/126 [==============================] - 0s 142us/step - loss: 0.7366 - mae: 0.5888 - mse: 0.7366 - val_loss: 228.7583 - val_mae: 11.5814 - val_mse: 228.7583\n",
      "Epoch 1755/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.6202 - mae: 0.5631 - mse: 0.6202 - val_loss: 241.5867 - val_mae: 11.8749 - val_mse: 241.5867\n",
      "Epoch 1756/3500\n",
      "126/126 [==============================] - 0s 190us/step - loss: 0.5846 - mae: 0.5617 - mse: 0.5846 - val_loss: 232.6426 - val_mae: 11.7404 - val_mse: 232.6426\n",
      "Epoch 1757/3500\n",
      "126/126 [==============================] - 0s 150us/step - loss: 0.6253 - mae: 0.5641 - mse: 0.6253 - val_loss: 230.3467 - val_mae: 11.6773 - val_mse: 230.3467\n",
      "Epoch 1758/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.6295 - mae: 0.5266 - mse: 0.6295 - val_loss: 240.1161 - val_mae: 11.8333 - val_mse: 240.1161\n",
      "Epoch 1759/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.5406 - mae: 0.6879 - mse: 0.540 - 0s 150us/step - loss: 0.5109 - mae: 0.5358 - mse: 0.5109 - val_loss: 229.0650 - val_mae: 11.6590 - val_mse: 229.0650\n",
      "Epoch 1760/3500\n",
      "126/126 [==============================] - 0s 158us/step - loss: 0.7193 - mae: 0.6171 - mse: 0.7193 - val_loss: 238.2113 - val_mae: 11.8466 - val_mse: 238.2113\n",
      "Epoch 1761/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.5879 - mae: 0.5219 - mse: 0.5879 - val_loss: 238.1613 - val_mae: 11.7439 - val_mse: 238.1613\n",
      "Epoch 1762/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.7292 - mae: 0.6322 - mse: 0.7292 - val_loss: 228.2061 - val_mae: 11.7052 - val_mse: 228.2061\n",
      "Epoch 1763/3500\n",
      "126/126 [==============================] - 0s 142us/step - loss: 0.6076 - mae: 0.5449 - mse: 0.6076 - val_loss: 241.8212 - val_mae: 11.8414 - val_mse: 241.8212\n",
      "Epoch 1764/3500\n",
      "126/126 [==============================] - 0s 150us/step - loss: 0.6174 - mae: 0.5668 - mse: 0.6174 - val_loss: 228.2535 - val_mae: 11.6855 - val_mse: 228.2535\n",
      "Epoch 1765/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.5296 - mae: 0.4983 - mse: 0.5296 - val_loss: 235.5801 - val_mae: 11.7968 - val_mse: 235.5801\n",
      "Epoch 1766/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.5401 - mae: 0.5379 - mse: 0.5401 - val_loss: 232.5462 - val_mae: 11.7174 - val_mse: 232.5462\n",
      "Epoch 1767/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.5556 - mae: 0.5224 - mse: 0.5556 - val_loss: 238.1299 - val_mae: 11.8537 - val_mse: 238.1299\n",
      "Epoch 1768/3500\n",
      "126/126 [==============================] - 0s 396us/step - loss: 0.6543 - mae: 0.5466 - mse: 0.6543 - val_loss: 238.4308 - val_mae: 11.7885 - val_mse: 238.4308\n",
      "Epoch 1769/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.5402 - mae: 0.5297 - mse: 0.5402 - val_loss: 227.0796 - val_mae: 11.5726 - val_mse: 227.0796\n",
      "Epoch 1770/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.7997 - mae: 0.6097 - mse: 0.7997 - val_loss: 238.1716 - val_mae: 11.7402 - val_mse: 238.1716\n",
      "Epoch 1771/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.6761 - mae: 0.5456 - mse: 0.6761 - val_loss: 234.8239 - val_mae: 11.7961 - val_mse: 234.8239\n",
      "Epoch 1772/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5842 - mae: 0.5302 - mse: 0.5842 - val_loss: 238.0696 - val_mae: 11.8611 - val_mse: 238.0696\n",
      "Epoch 1773/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.8191 - mae: 0.6876 - mse: 0.8191 - val_loss: 236.4462 - val_mae: 11.6700 - val_mse: 236.4462\n",
      "Epoch 1774/3500\n",
      "126/126 [==============================] - 0s 166us/step - loss: 0.8038 - mae: 0.6842 - mse: 0.8038 - val_loss: 228.1741 - val_mae: 11.5818 - val_mse: 228.1741\n",
      "Epoch 1775/3500\n",
      "126/126 [==============================] - 0s 206us/step - loss: 0.7016 - mae: 0.6015 - mse: 0.7016 - val_loss: 243.6261 - val_mae: 11.8684 - val_mse: 243.6261\n",
      "Epoch 1776/3500\n",
      "126/126 [==============================] - 0s 182us/step - loss: 0.6406 - mae: 0.5735 - mse: 0.6406 - val_loss: 239.6932 - val_mae: 11.8598 - val_mse: 239.6932\n",
      "Epoch 1777/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.5532 - mae: 0.5233 - mse: 0.5532 - val_loss: 230.0709 - val_mae: 11.7104 - val_mse: 230.0709\n",
      "Epoch 1778/3500\n",
      "126/126 [==============================] - 0s 253us/step - loss: 0.4621 - mae: 0.4220 - mse: 0.4621 - val_loss: 233.1051 - val_mae: 11.6801 - val_mse: 233.1051\n",
      "Epoch 1779/3500\n",
      "126/126 [==============================] - 0s 230us/step - loss: 0.4842 - mae: 0.4473 - mse: 0.4842 - val_loss: 233.2353 - val_mae: 11.7188 - val_mse: 233.2353\n",
      "Epoch 1780/3500\n",
      "126/126 [==============================] - 0s 174us/step - loss: 0.4844 - mae: 0.4113 - mse: 0.4844 - val_loss: 233.4529 - val_mae: 11.7542 - val_mse: 233.4529\n",
      "Epoch 1781/3500\n",
      "126/126 [==============================] - 0s 245us/step - loss: 0.3929 - mae: 0.3783 - mse: 0.3929 - val_loss: 240.2132 - val_mae: 11.7829 - val_mse: 240.2132\n",
      "Epoch 1782/3500\n",
      "126/126 [==============================] - 0s 309us/step - loss: 0.3940 - mae: 0.3909 - mse: 0.3940 - val_loss: 230.0659 - val_mae: 11.6781 - val_mse: 230.0659\n",
      "Epoch 1783/3500\n",
      "126/126 [==============================] - 0s 214us/step - loss: 0.4189 - mae: 0.4062 - mse: 0.4189 - val_loss: 239.4825 - val_mae: 11.7818 - val_mse: 239.4825\n",
      "Epoch 1784/3500\n",
      "126/126 [==============================] - 0s 182us/step - loss: 0.4259 - mae: 0.3943 - mse: 0.4259 - val_loss: 233.4919 - val_mae: 11.7077 - val_mse: 233.4919\n",
      "Epoch 1785/3500\n",
      "126/126 [==============================] - 0s 245us/step - loss: 0.4584 - mae: 0.4304 - mse: 0.4584 - val_loss: 231.3429 - val_mae: 11.7186 - val_mse: 231.3429\n",
      "Epoch 1786/3500\n",
      "126/126 [==============================] - 0s 396us/step - loss: 0.3990 - mae: 0.3992 - mse: 0.3990 - val_loss: 236.8045 - val_mae: 11.7290 - val_mse: 236.8045\n",
      "Epoch 1787/3500\n",
      "126/126 [==============================] - 0s 253us/step - loss: 0.4692 - mae: 0.4544 - mse: 0.4692 - val_loss: 230.4341 - val_mae: 11.6875 - val_mse: 230.4341\n",
      "Epoch 1788/3500\n",
      "126/126 [==============================] - 0s 206us/step - loss: 0.6057 - mae: 0.5430 - mse: 0.6057 - val_loss: 233.1406 - val_mae: 11.7120 - val_mse: 233.1406\n",
      "Epoch 1789/3500\n",
      "126/126 [==============================] - 0s 206us/step - loss: 0.5084 - mae: 0.4504 - mse: 0.5084 - val_loss: 240.1095 - val_mae: 11.8245 - val_mse: 240.1095\n",
      "Epoch 1790/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3866 - mae: 0.3757 - mse: 0.3866 - val_loss: 230.3967 - val_mae: 11.6584 - val_mse: 230.3967\n",
      "Epoch 1791/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4756 - mae: 0.4148 - mse: 0.4756 - val_loss: 237.1210 - val_mae: 11.7133 - val_mse: 237.1210\n",
      "Epoch 1792/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.5585 - mae: 0.5389 - mse: 0.5585 - val_loss: 234.3140 - val_mae: 11.7817 - val_mse: 234.3140\n",
      "Epoch 1793/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.7163 - mae: 0.6242 - mse: 0.7163 - val_loss: 231.7621 - val_mae: 11.7464 - val_mse: 231.7621\n",
      "Epoch 1794/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.7053 - mae: 0.5849 - mse: 0.7053 - val_loss: 241.7570 - val_mae: 11.8913 - val_mse: 241.7570\n",
      "Epoch 1795/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6317 - mae: 0.5701 - mse: 0.6317 - val_loss: 230.2281 - val_mae: 11.7370 - val_mse: 230.2281\n",
      "Epoch 1796/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6961 - mae: 0.5698 - mse: 0.6961 - val_loss: 238.9326 - val_mae: 11.7575 - val_mse: 238.9326\n",
      "Epoch 1797/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.6120 - mae: 0.5371 - mse: 0.6120 - val_loss: 230.9024 - val_mae: 11.6526 - val_mse: 230.9024\n",
      "Epoch 1798/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6401 - mae: 0.5367 - mse: 0.6401 - val_loss: 233.3225 - val_mae: 11.7299 - val_mse: 233.3225\n",
      "Epoch 1799/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5631 - mae: 0.5208 - mse: 0.5631 - val_loss: 240.3362 - val_mae: 11.8255 - val_mse: 240.3362\n",
      "Epoch 1800/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5652 - mae: 0.5285 - mse: 0.5652 - val_loss: 226.0035 - val_mae: 11.6734 - val_mse: 226.0035\n",
      "Epoch 1801/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6705 - mae: 0.5695 - mse: 0.6705 - val_loss: 245.0572 - val_mae: 11.8845 - val_mse: 245.0572\n",
      "Epoch 1802/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5613 - mae: 0.5225 - mse: 0.5613 - val_loss: 233.2551 - val_mae: 11.7150 - val_mse: 233.2551\n",
      "Epoch 1803/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5954 - mae: 0.5370 - mse: 0.5954 - val_loss: 233.8145 - val_mae: 11.6545 - val_mse: 233.8145\n",
      "Epoch 1804/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.5825 - mae: 0.4775 - mse: 0.5825 - val_loss: 234.9567 - val_mae: 11.6648 - val_mse: 234.9567\n",
      "Epoch 1805/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4996 - mae: 0.4988 - mse: 0.4996 - val_loss: 229.6992 - val_mae: 11.7071 - val_mse: 229.6992\n",
      "Epoch 1806/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.7129 - mae: 0.5910 - mse: 0.7129 - val_loss: 244.6758 - val_mae: 11.8717 - val_mse: 244.6758\n",
      "Epoch 1807/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.9429 - mae: 0.7114 - mse: 0.9429 - val_loss: 220.8570 - val_mae: 11.4700 - val_mse: 220.8570\n",
      "Epoch 1808/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 1.0418 - mae: 0.7695 - mse: 1.0418 - val_loss: 229.5311 - val_mae: 11.5573 - val_mse: 229.5311\n",
      "Epoch 1809/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.6794 - mae: 0.6000 - mse: 0.6794 - val_loss: 230.8862 - val_mae: 11.6079 - val_mse: 230.8862\n",
      "Epoch 1810/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.8003 - mae: 0.6308 - mse: 0.8003 - val_loss: 238.1294 - val_mae: 11.8809 - val_mse: 238.1294\n",
      "Epoch 1811/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.9802 - mae: 0.7374 - mse: 0.9802 - val_loss: 240.2721 - val_mae: 11.8380 - val_mse: 240.2721\n",
      "Epoch 1812/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.0009 - mae: 0.7696 - mse: 1.0009 - val_loss: 231.8934 - val_mae: 11.8265 - val_mse: 231.8934\n",
      "Epoch 1813/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.3539 - mae: 0.9147 - mse: 1.3539 - val_loss: 242.6618 - val_mae: 11.9028 - val_mse: 242.6618\n",
      "Epoch 1814/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.8164 - mae: 0.6826 - mse: 0.8164 - val_loss: 234.5211 - val_mae: 11.9045 - val_mse: 234.5211\n",
      "Epoch 1815/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.9567 - mae: 0.6956 - mse: 0.9567 - val_loss: 246.6738 - val_mae: 11.9586 - val_mse: 246.6738\n",
      "Epoch 1816/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 1.1269 - mae: 0.7974 - mse: 1.1269 - val_loss: 232.5636 - val_mae: 11.7352 - val_mse: 232.5636\n",
      "Epoch 1817/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.1249 - mae: 0.7674 - mse: 1.1249 - val_loss: 236.5551 - val_mae: 11.8081 - val_mse: 236.5551\n",
      "Epoch 1818/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 1.2457 - mae: 0.8147 - mse: 1.2457 - val_loss: 243.7402 - val_mae: 11.8382 - val_mse: 243.7402\n",
      "Epoch 1819/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.3461 - mae: 0.8861 - mse: 1.3461 - val_loss: 221.2386 - val_mae: 11.3826 - val_mse: 221.2386\n",
      "Epoch 1820/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.1945 - mae: 0.8312 - mse: 1.1945 - val_loss: 231.1099 - val_mae: 11.5799 - val_mse: 231.1099\n",
      "Epoch 1821/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.9878 - mae: 0.7271 - mse: 0.9878 - val_loss: 234.3638 - val_mae: 11.7279 - val_mse: 234.3638\n",
      "Epoch 1822/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.2590 - mae: 0.8551 - mse: 1.2590 - val_loss: 227.9742 - val_mae: 11.7860 - val_mse: 227.9742\n",
      "Epoch 1823/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.5533 - mae: 0.9398 - mse: 1.5533 - val_loss: 234.8947 - val_mae: 11.5830 - val_mse: 234.8947\n",
      "Epoch 1824/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.5230 - mae: 1.0144 - mse: 1.5230 - val_loss: 227.4034 - val_mae: 11.7293 - val_mse: 227.4034\n",
      "Epoch 1825/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 2.0726 - mae: 1.1851 - mse: 2.0726 - val_loss: 240.9873 - val_mae: 11.6961 - val_mse: 240.9873\n",
      "Epoch 1826/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 2.3626 - mae: 1.2474 - mse: 2.3626 - val_loss: 232.2688 - val_mae: 11.8436 - val_mse: 232.2688\n",
      "Epoch 1827/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.5058 - mae: 0.9625 - mse: 1.5058 - val_loss: 245.9687 - val_mae: 12.0417 - val_mse: 245.9687\n",
      "Epoch 1828/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 2.1755 - mae: 1.1356 - mse: 2.1755 - val_loss: 236.0345 - val_mae: 11.7691 - val_mse: 236.0345\n",
      "Epoch 1829/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 2.3358 - mae: 1.0977 - mse: 2.3358 - val_loss: 223.8257 - val_mae: 11.4264 - val_mse: 223.8257\n",
      "Epoch 1830/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 2.3300 - mae: 1.1593 - mse: 2.3300 - val_loss: 236.6705 - val_mae: 11.7185 - val_mse: 236.6705\n",
      "Epoch 1831/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.5631 - mae: 0.9627 - mse: 1.5631 - val_loss: 242.0206 - val_mae: 12.1491 - val_mse: 242.0206\n",
      "Epoch 1832/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.4513 - mae: 0.9649 - mse: 1.4513 - val_loss: 244.1463 - val_mae: 11.8228 - val_mse: 244.1463\n",
      "Epoch 1833/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.5394 - mae: 0.9280 - mse: 1.5394 - val_loss: 226.4244 - val_mae: 11.7386 - val_mse: 226.4244\n",
      "Epoch 1834/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.5454 - mae: 0.9711 - mse: 1.5454 - val_loss: 236.0634 - val_mae: 11.6645 - val_mse: 236.0634\n",
      "Epoch 1835/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.4539 - mae: 0.7465 - mse: 1.4539 - val_loss: 226.6275 - val_mae: 11.6399 - val_mse: 226.6275\n",
      "Epoch 1836/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.6636 - mae: 0.8116 - mse: 1.6636 - val_loss: 232.7592 - val_mae: 11.8137 - val_mse: 232.7592\n",
      "Epoch 1837/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.4259 - mae: 0.9127 - mse: 1.4259 - val_loss: 238.6143 - val_mae: 11.7260 - val_mse: 238.6143\n",
      "Epoch 1838/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 1.7247 - mae: 1.0128 - mse: 1.7247 - val_loss: 236.6955 - val_mae: 11.9932 - val_mse: 236.6955\n",
      "Epoch 1839/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.9192 - mae: 1.1134 - mse: 1.9192 - val_loss: 240.6195 - val_mae: 11.7977 - val_mse: 240.6195\n",
      "Epoch 1840/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 2.6940 - mae: 1.2691 - mse: 2.6940 - val_loss: 212.6063 - val_mae: 11.3384 - val_mse: 212.6063\n",
      "Epoch 1841/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.6289 - mae: 0.9660 - mse: 1.6289 - val_loss: 234.7758 - val_mae: 11.7538 - val_mse: 234.7758\n",
      "Epoch 1842/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.0843 - mae: 0.8220 - mse: 1.0843 - val_loss: 221.2197 - val_mae: 11.4800 - val_mse: 221.2197\n",
      "Epoch 1843/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 1.0935 - mae: 0.8324 - mse: 1.0935 - val_loss: 229.0197 - val_mae: 11.5986 - val_mse: 229.0197\n",
      "Epoch 1844/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.1158 - mae: 0.7932 - mse: 1.1158 - val_loss: 233.4329 - val_mae: 11.7192 - val_mse: 233.4329\n",
      "Epoch 1845/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.8595 - mae: 0.6886 - mse: 0.8595 - val_loss: 231.7326 - val_mae: 11.6464 - val_mse: 231.7326\n",
      "Epoch 1846/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.7425 - mae: 0.6640 - mse: 0.7425 - val_loss: 231.1988 - val_mae: 11.7209 - val_mse: 231.1988\n",
      "Epoch 1847/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6509 - mae: 0.6032 - mse: 0.6509 - val_loss: 234.2534 - val_mae: 11.6733 - val_mse: 234.2534\n",
      "Epoch 1848/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6008 - mae: 0.5285 - mse: 0.6008 - val_loss: 230.8431 - val_mae: 11.7419 - val_mse: 230.8431\n",
      "Epoch 1849/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5703 - mae: 0.5331 - mse: 0.5703 - val_loss: 232.1948 - val_mae: 11.6759 - val_mse: 232.1948\n",
      "Epoch 1850/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5002 - mae: 0.4351 - mse: 0.5002 - val_loss: 227.4226 - val_mae: 11.5437 - val_mse: 227.4226\n",
      "Epoch 1851/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.5587 - mae: 0.4820 - mse: 0.5587 - val_loss: 231.4917 - val_mae: 11.6917 - val_mse: 231.4917\n",
      "Epoch 1852/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4318 - mae: 0.4465 - mse: 0.4318 - val_loss: 234.1451 - val_mae: 11.7040 - val_mse: 234.1451\n",
      "Epoch 1853/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4799 - mae: 0.4935 - mse: 0.4799 - val_loss: 228.1028 - val_mae: 11.6297 - val_mse: 228.1028\n",
      "Epoch 1854/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4571 - mae: 0.4763 - mse: 0.4571 - val_loss: 234.2888 - val_mae: 11.6755 - val_mse: 234.2888\n",
      "Epoch 1855/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3735 - mae: 0.3607 - mse: 0.3735 - val_loss: 224.4685 - val_mae: 11.5403 - val_mse: 224.4685\n",
      "Epoch 1856/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3537 - mae: 0.3313 - mse: 0.3537 - val_loss: 233.4982 - val_mae: 11.6469 - val_mse: 233.4982\n",
      "Epoch 1857/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4320 - mae: 0.3850 - mse: 0.4320 - val_loss: 228.3417 - val_mae: 11.5944 - val_mse: 228.3417\n",
      "Epoch 1858/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3915 - mae: 0.3632 - mse: 0.3915 - val_loss: 229.6777 - val_mae: 11.5493 - val_mse: 229.6777\n",
      "Epoch 1859/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4099 - mae: 0.3754 - mse: 0.4099 - val_loss: 227.9491 - val_mae: 11.5680 - val_mse: 227.9491\n",
      "Epoch 1860/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4986 - mae: 0.5066 - mse: 0.4986 - val_loss: 227.8433 - val_mae: 11.5348 - val_mse: 227.8433\n",
      "Epoch 1861/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4666 - mae: 0.4528 - mse: 0.4666 - val_loss: 232.7814 - val_mae: 11.6894 - val_mse: 232.7814\n",
      "Epoch 1862/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5711 - mae: 0.4985 - mse: 0.5711 - val_loss: 232.5705 - val_mae: 11.6933 - val_mse: 232.5705\n",
      "Epoch 1863/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4767 - mae: 0.4733 - mse: 0.4767 - val_loss: 229.8456 - val_mae: 11.5949 - val_mse: 229.8456\n",
      "Epoch 1864/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4284 - mae: 0.4445 - mse: 0.4284 - val_loss: 232.3853 - val_mae: 11.7176 - val_mse: 232.3853\n",
      "Epoch 1865/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5218 - mae: 0.4493 - mse: 0.5218 - val_loss: 232.9726 - val_mae: 11.6086 - val_mse: 232.9726\n",
      "Epoch 1866/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4445 - mae: 0.4504 - mse: 0.4445 - val_loss: 226.3061 - val_mae: 11.5988 - val_mse: 226.3061\n",
      "Epoch 1867/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5066 - mae: 0.4837 - mse: 0.5066 - val_loss: 232.0290 - val_mae: 11.5825 - val_mse: 232.0290\n",
      "Epoch 1868/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5302 - mae: 0.4456 - mse: 0.5302 - val_loss: 228.2758 - val_mae: 11.5867 - val_mse: 228.2758\n",
      "Epoch 1869/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5084 - mae: 0.4242 - mse: 0.5084 - val_loss: 230.6269 - val_mae: 11.6387 - val_mse: 230.6269\n",
      "Epoch 1870/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3955 - mae: 0.4161 - mse: 0.3955 - val_loss: 230.0314 - val_mae: 11.5060 - val_mse: 230.0314\n",
      "Epoch 1871/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4683 - mae: 0.4286 - mse: 0.4683 - val_loss: 227.6232 - val_mae: 11.6239 - val_mse: 227.6232\n",
      "Epoch 1872/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4073 - mae: 0.3827 - mse: 0.4073 - val_loss: 232.2070 - val_mae: 11.5774 - val_mse: 232.2070\n",
      "Epoch 1873/3500\n",
      "126/126 [==============================] - 0s 158us/step - loss: 0.4085 - mae: 0.3991 - mse: 0.4085 - val_loss: 229.6858 - val_mae: 11.5941 - val_mse: 229.6858\n",
      "Epoch 1874/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.3289 - mae: 0.3463 - mse: 0.3289 - val_loss: 233.4571 - val_mae: 11.6634 - val_mse: 233.4571\n",
      "Epoch 1875/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3612 - mae: 0.3863 - mse: 0.3612 - val_loss: 228.1604 - val_mae: 11.5700 - val_mse: 228.1604\n",
      "Epoch 1876/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4272 - mae: 0.4211 - mse: 0.4272 - val_loss: 231.7153 - val_mae: 11.6206 - val_mse: 231.7153\n",
      "Epoch 1877/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4276 - mae: 0.3928 - mse: 0.4276 - val_loss: 232.3724 - val_mae: 11.6668 - val_mse: 232.3724\n",
      "Epoch 1878/3500\n",
      "126/126 [==============================] - 0s 142us/step - loss: 0.4492 - mae: 0.4271 - mse: 0.4492 - val_loss: 228.0817 - val_mae: 11.5556 - val_mse: 228.0817\n",
      "Epoch 1879/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.5157 - mae: 0.4506 - mse: 0.5157 - val_loss: 230.4777 - val_mae: 11.5840 - val_mse: 230.4777\n",
      "Epoch 1880/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4843 - mae: 0.4192 - mse: 0.4843 - val_loss: 235.9684 - val_mae: 11.7111 - val_mse: 235.9684\n",
      "Epoch 1881/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5619 - mae: 0.4741 - mse: 0.5619 - val_loss: 230.9019 - val_mae: 11.6399 - val_mse: 230.9019\n",
      "Epoch 1882/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3524 - mae: 0.3505 - mse: 0.3524 - val_loss: 228.4595 - val_mae: 11.5366 - val_mse: 228.4595\n",
      "Epoch 1883/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3506 - mae: 0.3273 - mse: 0.3506 - val_loss: 230.2108 - val_mae: 11.5942 - val_mse: 230.2108\n",
      "Epoch 1884/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2902 - mae: 0.2882 - mse: 0.2902 - val_loss: 228.0896 - val_mae: 11.5763 - val_mse: 228.0896\n",
      "Epoch 1885/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3104 - mae: 0.2924 - mse: 0.3104 - val_loss: 231.6350 - val_mae: 11.6413 - val_mse: 231.6350\n",
      "Epoch 1886/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4677 - mae: 0.4259 - mse: 0.4677 - val_loss: 233.6096 - val_mae: 11.6522 - val_mse: 233.6096\n",
      "Epoch 1887/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3987 - mae: 0.4098 - mse: 0.3987 - val_loss: 226.8028 - val_mae: 11.6064 - val_mse: 226.8028\n",
      "Epoch 1888/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4373 - mae: 0.4484 - mse: 0.4373 - val_loss: 232.6443 - val_mae: 11.5613 - val_mse: 232.6443\n",
      "Epoch 1889/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4242 - mae: 0.4259 - mse: 0.4242 - val_loss: 228.9051 - val_mae: 11.5964 - val_mse: 228.9051\n",
      "Epoch 1890/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.2405 - mae: 0.3754 - mse: 0.240 - 0s 71us/step - loss: 0.4258 - mae: 0.4108 - mse: 0.4258 - val_loss: 232.8785 - val_mae: 11.6109 - val_mse: 232.8785\n",
      "Epoch 1891/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3763 - mae: 0.3846 - mse: 0.3763 - val_loss: 230.5998 - val_mae: 11.5792 - val_mse: 230.5998\n",
      "Epoch 1892/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4354 - mae: 0.4464 - mse: 0.4354 - val_loss: 231.4043 - val_mae: 11.6427 - val_mse: 231.4043\n",
      "Epoch 1893/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4907 - mae: 0.4201 - mse: 0.4907 - val_loss: 231.9367 - val_mae: 11.5591 - val_mse: 231.9367\n",
      "Epoch 1894/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.3877 - mae: 0.3900 - mse: 0.3877 - val_loss: 229.1156 - val_mae: 11.5983 - val_mse: 229.1156\n",
      "Epoch 1895/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4627 - mae: 0.4615 - mse: 0.4627 - val_loss: 232.5422 - val_mae: 11.6599 - val_mse: 232.5422\n",
      "Epoch 1896/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4629 - mae: 0.4253 - mse: 0.4629 - val_loss: 229.7628 - val_mae: 11.6331 - val_mse: 229.7628\n",
      "Epoch 1897/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5695 - mae: 0.5103 - mse: 0.5695 - val_loss: 233.4094 - val_mae: 11.7084 - val_mse: 233.4094\n",
      "Epoch 1898/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5659 - mae: 0.5344 - mse: 0.5659 - val_loss: 232.7529 - val_mae: 11.6204 - val_mse: 232.7529\n",
      "Epoch 1899/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4404 - mae: 0.4310 - mse: 0.4404 - val_loss: 226.8363 - val_mae: 11.5257 - val_mse: 226.8363\n",
      "Epoch 1900/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4809 - mae: 0.3895 - mse: 0.4809 - val_loss: 230.5228 - val_mae: 11.5919 - val_mse: 230.5228\n",
      "Epoch 1901/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4806 - mae: 0.4321 - mse: 0.4806 - val_loss: 228.7558 - val_mae: 11.5532 - val_mse: 228.7558\n",
      "Epoch 1902/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5868 - mae: 0.5085 - mse: 0.5868 - val_loss: 230.5344 - val_mae: 11.6305 - val_mse: 230.5344\n",
      "Epoch 1903/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5441 - mae: 0.4720 - mse: 0.5441 - val_loss: 235.8048 - val_mae: 11.7253 - val_mse: 235.8048\n",
      "Epoch 1904/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4166 - mae: 0.3974 - mse: 0.4166 - val_loss: 225.8875 - val_mae: 11.5140 - val_mse: 225.8875\n",
      "Epoch 1905/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.7343 - mae: 0.5157 - mse: 0.7343 - val_loss: 232.3837 - val_mae: 11.6507 - val_mse: 232.3837\n",
      "Epoch 1906/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.0418 - mae: 0.6648 - mse: 1.0418 - val_loss: 235.8633 - val_mae: 11.6916 - val_mse: 235.8633\n",
      "Epoch 1907/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.9690 - mae: 0.7380 - mse: 0.9690 - val_loss: 224.2313 - val_mae: 11.5420 - val_mse: 224.2313\n",
      "Epoch 1908/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7079 - mae: 0.5950 - mse: 0.7079 - val_loss: 244.2815 - val_mae: 11.7485 - val_mse: 244.2815\n",
      "Epoch 1909/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.8110 - mae: 0.6353 - mse: 0.8110 - val_loss: 226.9224 - val_mae: 11.5898 - val_mse: 226.9224\n",
      "Epoch 1910/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.7829 - mae: 0.6484 - mse: 0.7829 - val_loss: 237.3838 - val_mae: 11.7462 - val_mse: 237.3838\n",
      "Epoch 1911/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.9122 - mae: 0.7318 - mse: 0.9122 - val_loss: 233.4393 - val_mae: 11.7090 - val_mse: 233.4393\n",
      "Epoch 1912/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.7592 - mae: 0.6217 - mse: 0.7592 - val_loss: 229.7486 - val_mae: 11.5822 - val_mse: 229.7486\n",
      "Epoch 1913/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7985 - mae: 0.6479 - mse: 0.7985 - val_loss: 229.1944 - val_mae: 11.5728 - val_mse: 229.1944\n",
      "Epoch 1914/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.7788 - mae: 0.6797 - mse: 0.7788 - val_loss: 232.4379 - val_mae: 11.6312 - val_mse: 232.4379\n",
      "Epoch 1915/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5949 - mae: 0.5479 - mse: 0.5949 - val_loss: 226.2942 - val_mae: 11.5455 - val_mse: 226.2942\n",
      "Epoch 1916/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.7200 - mae: 0.6286 - mse: 0.7200 - val_loss: 229.4980 - val_mae: 11.5265 - val_mse: 229.4980\n",
      "Epoch 1917/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6646 - mae: 0.5705 - mse: 0.6646 - val_loss: 227.7797 - val_mae: 11.4978 - val_mse: 227.7797\n",
      "Epoch 1918/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.6779 - mae: 0.5988 - mse: 0.6779 - val_loss: 227.6729 - val_mae: 11.6311 - val_mse: 227.6729\n",
      "Epoch 1919/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.7637 - mae: 0.6164 - mse: 0.7637 - val_loss: 230.0997 - val_mae: 11.5578 - val_mse: 230.0997\n",
      "Epoch 1920/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.6092 - mae: 0.5988 - mse: 0.6092 - val_loss: 229.9758 - val_mae: 11.6689 - val_mse: 229.9758\n",
      "Epoch 1921/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5816 - mae: 0.5397 - mse: 0.5816 - val_loss: 244.0919 - val_mae: 11.8594 - val_mse: 244.0919\n",
      "Epoch 1922/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6216 - mae: 0.5485 - mse: 0.6216 - val_loss: 221.9789 - val_mae: 11.4664 - val_mse: 221.9789\n",
      "Epoch 1923/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6774 - mae: 0.5439 - mse: 0.6774 - val_loss: 229.1495 - val_mae: 11.5375 - val_mse: 229.1495\n",
      "Epoch 1924/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5980 - mae: 0.5220 - mse: 0.5980 - val_loss: 224.8170 - val_mae: 11.4365 - val_mse: 224.8170\n",
      "Epoch 1925/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.5776 - mae: 0.5227 - mse: 0.5776 - val_loss: 231.3962 - val_mae: 11.6066 - val_mse: 231.3962\n",
      "Epoch 1926/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.5755 - mae: 0.5149 - mse: 0.5755 - val_loss: 241.5854 - val_mae: 11.7596 - val_mse: 241.5854\n",
      "Epoch 1927/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.7610 - mae: 0.6285 - mse: 0.7610 - val_loss: 226.7328 - val_mae: 11.6286 - val_mse: 226.7328\n",
      "Epoch 1928/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6541 - mae: 0.5753 - mse: 0.6541 - val_loss: 231.3291 - val_mae: 11.5258 - val_mse: 231.3291\n",
      "Epoch 1929/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6368 - mae: 0.5853 - mse: 0.6368 - val_loss: 226.7178 - val_mae: 11.5937 - val_mse: 226.7178\n",
      "Epoch 1930/3500\n",
      "126/126 [==============================] - 0s 198us/step - loss: 0.7158 - mae: 0.5985 - mse: 0.7158 - val_loss: 233.2672 - val_mae: 11.6736 - val_mse: 233.2672\n",
      "Epoch 1931/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6912 - mae: 0.5672 - mse: 0.6912 - val_loss: 231.5881 - val_mae: 11.5536 - val_mse: 231.5881\n",
      "Epoch 1932/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.8067 - mae: 0.6771 - mse: 0.8067 - val_loss: 231.0248 - val_mae: 11.7364 - val_mse: 231.0248\n",
      "Epoch 1933/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.0239 - mae: 0.7526 - mse: 1.0239 - val_loss: 236.5507 - val_mae: 11.6775 - val_mse: 236.5507\n",
      "Epoch 1934/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.9029 - mae: 0.7379 - mse: 0.9029 - val_loss: 220.4407 - val_mae: 11.5287 - val_mse: 220.4407\n",
      "Epoch 1935/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.0552 - mae: 0.7886 - mse: 1.0552 - val_loss: 234.8790 - val_mae: 11.5746 - val_mse: 234.8790\n",
      "Epoch 1936/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.8476 - mae: 0.6727 - mse: 0.8476 - val_loss: 226.3806 - val_mae: 11.6067 - val_mse: 226.3806\n",
      "Epoch 1937/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.8662 - mae: 0.6660 - mse: 0.8662 - val_loss: 239.9812 - val_mae: 11.7948 - val_mse: 239.9812\n",
      "Epoch 1938/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.0314 - mae: 0.7431 - mse: 1.0314 - val_loss: 227.1115 - val_mae: 11.5423 - val_mse: 227.1115\n",
      "Epoch 1939/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.8615 - mae: 0.6662 - mse: 0.8615 - val_loss: 227.8012 - val_mae: 11.6362 - val_mse: 227.8012\n",
      "Epoch 1940/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.8913 - mae: 0.7126 - mse: 0.8913 - val_loss: 235.8446 - val_mae: 11.6600 - val_mse: 235.8446\n",
      "Epoch 1941/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.0678 - mae: 0.7722 - mse: 1.0678 - val_loss: 218.5329 - val_mae: 11.4729 - val_mse: 218.5329\n",
      "Epoch 1942/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.3702 - mae: 0.9030 - mse: 1.3702 - val_loss: 250.1442 - val_mae: 11.8587 - val_mse: 250.1442\n",
      "Epoch 1943/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.6165 - mae: 0.9610 - mse: 1.6165 - val_loss: 221.1331 - val_mae: 11.5829 - val_mse: 221.1331\n",
      "Epoch 1944/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.3392 - mae: 0.9096 - mse: 1.3392 - val_loss: 233.3670 - val_mae: 11.5894 - val_mse: 233.3670\n",
      "Epoch 1945/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.4359 - mae: 0.8936 - mse: 1.4359 - val_loss: 231.0915 - val_mae: 11.6768 - val_mse: 231.0915\n",
      "Epoch 1946/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.9598 - mae: 0.7475 - mse: 0.9598 - val_loss: 234.8810 - val_mae: 11.7384 - val_mse: 234.8810\n",
      "Epoch 1947/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.0397 - mae: 0.7836 - mse: 1.0397 - val_loss: 241.7750 - val_mae: 11.8179 - val_mse: 241.7750\n",
      "Epoch 1948/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.0191 - mae: 0.7681 - mse: 1.0191 - val_loss: 221.8478 - val_mae: 11.4525 - val_mse: 221.8478\n",
      "Epoch 1949/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7374 - mae: 0.6342 - mse: 0.7374 - val_loss: 229.1578 - val_mae: 11.6689 - val_mse: 229.1578\n",
      "Epoch 1950/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.7466 - mae: 0.6214 - mse: 0.7466 - val_loss: 232.8339 - val_mae: 11.6884 - val_mse: 232.8339\n",
      "Epoch 1951/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6756 - mae: 0.6207 - mse: 0.6756 - val_loss: 229.0422 - val_mae: 11.5631 - val_mse: 229.0422\n",
      "Epoch 1952/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4848 - mae: 0.4399 - mse: 0.4848 - val_loss: 229.6616 - val_mae: 11.6433 - val_mse: 229.6616\n",
      "Epoch 1953/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5740 - mae: 0.5285 - mse: 0.5740 - val_loss: 230.2202 - val_mae: 11.6082 - val_mse: 230.2202\n",
      "Epoch 1954/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4682 - mae: 0.4616 - mse: 0.4682 - val_loss: 227.4240 - val_mae: 11.5349 - val_mse: 227.4240\n",
      "Epoch 1955/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6371 - mae: 0.5595 - mse: 0.6371 - val_loss: 230.0401 - val_mae: 11.6315 - val_mse: 230.0401\n",
      "Epoch 1956/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4910 - mae: 0.4989 - mse: 0.4910 - val_loss: 229.4341 - val_mae: 11.5152 - val_mse: 229.4341\n",
      "Epoch 1957/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5648 - mae: 0.5292 - mse: 0.5648 - val_loss: 232.6200 - val_mae: 11.6275 - val_mse: 232.6200\n",
      "Epoch 1958/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3783 - mae: 0.3922 - mse: 0.3783 - val_loss: 228.2683 - val_mae: 11.6073 - val_mse: 228.2683\n",
      "Epoch 1959/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4196 - mae: 0.4282 - mse: 0.4196 - val_loss: 228.0612 - val_mae: 11.5331 - val_mse: 228.0612\n",
      "Epoch 1960/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4560 - mae: 0.4250 - mse: 0.4560 - val_loss: 235.3636 - val_mae: 11.7328 - val_mse: 235.3636\n",
      "Epoch 1961/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3751 - mae: 0.3702 - mse: 0.3751 - val_loss: 228.2587 - val_mae: 11.6084 - val_mse: 228.2587\n",
      "Epoch 1962/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3419 - mae: 0.3402 - mse: 0.3419 - val_loss: 228.4577 - val_mae: 11.5575 - val_mse: 228.4577\n",
      "Epoch 1963/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4355 - mae: 0.4125 - mse: 0.4355 - val_loss: 232.8848 - val_mae: 11.6657 - val_mse: 232.8848\n",
      "Epoch 1964/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.7381 - mae: 0.5927 - mse: 0.7381 - val_loss: 228.6621 - val_mae: 11.6016 - val_mse: 228.6621\n",
      "Epoch 1965/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.9011 - mae: 0.7154 - mse: 0.9011 - val_loss: 234.5865 - val_mae: 11.7286 - val_mse: 234.5865\n",
      "Epoch 1966/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.7678 - mae: 0.6409 - mse: 0.7678 - val_loss: 233.9178 - val_mae: 11.6616 - val_mse: 233.9178\n",
      "Epoch 1967/3500\n",
      "126/126 [==============================] - 0s 214us/step - loss: 0.5235 - mae: 0.4828 - mse: 0.5235 - val_loss: 225.2135 - val_mae: 11.4741 - val_mse: 225.2135\n",
      "Epoch 1968/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5962 - mae: 0.4772 - mse: 0.5962 - val_loss: 229.5723 - val_mae: 11.6098 - val_mse: 229.5723\n",
      "Epoch 1969/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5285 - mae: 0.4271 - mse: 0.5285 - val_loss: 233.8916 - val_mae: 11.6151 - val_mse: 233.8916\n",
      "Epoch 1970/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5740 - mae: 0.4908 - mse: 0.5740 - val_loss: 229.5743 - val_mae: 11.5547 - val_mse: 229.5743\n",
      "Epoch 1971/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5107 - mae: 0.4721 - mse: 0.5107 - val_loss: 235.1550 - val_mae: 11.7368 - val_mse: 235.1550\n",
      "Epoch 1972/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.6217 - mae: 0.5226 - mse: 0.6217 - val_loss: 235.2613 - val_mae: 11.6511 - val_mse: 235.2613\n",
      "Epoch 1973/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5604 - mae: 0.5055 - mse: 0.5604 - val_loss: 225.6224 - val_mae: 11.5525 - val_mse: 225.6224\n",
      "Epoch 1974/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.5963 - mae: 0.5199 - mse: 0.5963 - val_loss: 239.3746 - val_mae: 11.7501 - val_mse: 239.3746\n",
      "Epoch 1975/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.6184 - mae: 0.5340 - mse: 0.6184 - val_loss: 221.1375 - val_mae: 11.5760 - val_mse: 221.1375\n",
      "Epoch 1976/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6935 - mae: 0.5979 - mse: 0.6935 - val_loss: 232.0262 - val_mae: 11.5949 - val_mse: 232.0262\n",
      "Epoch 1977/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.7499 - mae: 0.6101 - mse: 0.7499 - val_loss: 223.1595 - val_mae: 11.5276 - val_mse: 223.1595\n",
      "Epoch 1978/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6123 - mae: 0.5947 - mse: 0.6123 - val_loss: 226.2923 - val_mae: 11.4639 - val_mse: 226.2923\n",
      "Epoch 1979/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5019 - mae: 0.4792 - mse: 0.5019 - val_loss: 233.1687 - val_mae: 11.6962 - val_mse: 233.1687\n",
      "Epoch 1980/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.4803 - mae: 0.3773 - mse: 0.480 - 0s 79us/step - loss: 0.6140 - mae: 0.5179 - mse: 0.6140 - val_loss: 229.5304 - val_mae: 11.6418 - val_mse: 229.5304\n",
      "Epoch 1981/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4638 - mae: 0.4617 - mse: 0.4638 - val_loss: 230.8994 - val_mae: 11.5998 - val_mse: 230.8994\n",
      "Epoch 1982/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4657 - mae: 0.4195 - mse: 0.4657 - val_loss: 228.0663 - val_mae: 11.6418 - val_mse: 228.0663\n",
      "Epoch 1983/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4881 - mae: 0.4949 - mse: 0.4881 - val_loss: 232.3898 - val_mae: 11.5691 - val_mse: 232.3898\n",
      "Epoch 1984/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4479 - mae: 0.4473 - mse: 0.4479 - val_loss: 226.6277 - val_mae: 11.5450 - val_mse: 226.6277\n",
      "Epoch 1985/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4057 - mae: 0.3843 - mse: 0.4057 - val_loss: 233.4119 - val_mae: 11.6167 - val_mse: 233.4119\n",
      "Epoch 1986/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4014 - mae: 0.4178 - mse: 0.4014 - val_loss: 232.1967 - val_mae: 11.6912 - val_mse: 232.1967\n",
      "Epoch 1987/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4615 - mae: 0.4405 - mse: 0.4615 - val_loss: 225.7816 - val_mae: 11.5447 - val_mse: 225.7816\n",
      "Epoch 1988/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3590 - mae: 0.3629 - mse: 0.3590 - val_loss: 229.6674 - val_mae: 11.5708 - val_mse: 229.6674\n",
      "Epoch 1989/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3612 - mae: 0.3479 - mse: 0.3612 - val_loss: 229.4380 - val_mae: 11.6208 - val_mse: 229.4380\n",
      "Epoch 1990/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3376 - mae: 0.3351 - mse: 0.3376 - val_loss: 229.0220 - val_mae: 11.6084 - val_mse: 229.0220\n",
      "Epoch 1991/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2851 - mae: 0.2782 - mse: 0.2851 - val_loss: 234.5900 - val_mae: 11.6593 - val_mse: 234.5900\n",
      "Epoch 1992/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3119 - mae: 0.3041 - mse: 0.3119 - val_loss: 228.4078 - val_mae: 11.5435 - val_mse: 228.4078\n",
      "Epoch 1993/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3330 - mae: 0.3003 - mse: 0.3330 - val_loss: 227.5992 - val_mae: 11.5262 - val_mse: 227.5992\n",
      "Epoch 1994/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2868 - mae: 0.2766 - mse: 0.2868 - val_loss: 233.7242 - val_mae: 11.6492 - val_mse: 233.7242\n",
      "Epoch 1995/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3539 - mae: 0.2938 - mse: 0.3539 - val_loss: 231.4123 - val_mae: 11.6319 - val_mse: 231.4123\n",
      "Epoch 1996/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3164 - mae: 0.2971 - mse: 0.3164 - val_loss: 228.9492 - val_mae: 11.5796 - val_mse: 228.9492\n",
      "Epoch 1997/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4241 - mae: 0.3710 - mse: 0.4241 - val_loss: 233.6745 - val_mae: 11.6103 - val_mse: 233.6745\n",
      "Epoch 1998/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5835 - mae: 0.4525 - mse: 0.5835 - val_loss: 228.0040 - val_mae: 11.6187 - val_mse: 228.0040\n",
      "Epoch 1999/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6815 - mae: 0.6015 - mse: 0.6815 - val_loss: 232.5792 - val_mae: 11.6435 - val_mse: 232.5792\n",
      "Epoch 2000/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.6309 - mae: 0.5441 - mse: 0.6309 - val_loss: 231.0597 - val_mae: 11.6523 - val_mse: 231.0597\n",
      "Epoch 2001/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5894 - mae: 0.5427 - mse: 0.5894 - val_loss: 229.5465 - val_mae: 11.6107 - val_mse: 229.5465\n",
      "Epoch 2002/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5029 - mae: 0.4561 - mse: 0.5029 - val_loss: 234.5509 - val_mae: 11.6395 - val_mse: 234.5509\n",
      "Epoch 2003/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5326 - mae: 0.4881 - mse: 0.5326 - val_loss: 228.8068 - val_mae: 11.5805 - val_mse: 228.8068\n",
      "Epoch 2004/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4716 - mae: 0.4598 - mse: 0.4716 - val_loss: 231.6812 - val_mae: 11.6136 - val_mse: 231.6812\n",
      "Epoch 2005/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5206 - mae: 0.5126 - mse: 0.5206 - val_loss: 228.9611 - val_mae: 11.6926 - val_mse: 228.9611\n",
      "Epoch 2006/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5898 - mae: 0.5349 - mse: 0.5898 - val_loss: 236.2274 - val_mae: 11.6649 - val_mse: 236.2274\n",
      "Epoch 2007/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.6924 - mae: 0.6191 - mse: 0.6924 - val_loss: 228.5727 - val_mae: 11.6334 - val_mse: 228.5727\n",
      "Epoch 2008/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6697 - mae: 0.5863 - mse: 0.6697 - val_loss: 239.0590 - val_mae: 11.7264 - val_mse: 239.0590\n",
      "Epoch 2009/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5425 - mae: 0.4474 - mse: 0.5425 - val_loss: 225.1636 - val_mae: 11.4557 - val_mse: 225.1636\n",
      "Epoch 2010/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6476 - mae: 0.5342 - mse: 0.6476 - val_loss: 231.7461 - val_mae: 11.6806 - val_mse: 231.7461\n",
      "Epoch 2011/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4242 - mae: 0.4083 - mse: 0.4242 - val_loss: 234.1166 - val_mae: 11.7380 - val_mse: 234.1166\n",
      "Epoch 2012/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4252 - mae: 0.4155 - mse: 0.4252 - val_loss: 227.9109 - val_mae: 11.5682 - val_mse: 227.9109\n",
      "Epoch 2013/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4461 - mae: 0.4375 - mse: 0.4461 - val_loss: 234.3509 - val_mae: 11.6770 - val_mse: 234.3509\n",
      "Epoch 2014/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5162 - mae: 0.4503 - mse: 0.5162 - val_loss: 231.4290 - val_mae: 11.6965 - val_mse: 231.4290\n",
      "Epoch 2015/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4168 - mae: 0.4107 - mse: 0.4168 - val_loss: 231.6002 - val_mae: 11.6182 - val_mse: 231.6002\n",
      "Epoch 2016/3500\n",
      "126/126 [==============================] - 0s 158us/step - loss: 0.3969 - mae: 0.3775 - mse: 0.3969 - val_loss: 230.4048 - val_mae: 11.5928 - val_mse: 230.4048\n",
      "Epoch 2017/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.4413 - mae: 0.3655 - mse: 0.4413 - val_loss: 230.0418 - val_mae: 11.5767 - val_mse: 230.0418\n",
      "Epoch 2018/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4150 - mae: 0.3811 - mse: 0.4150 - val_loss: 231.4232 - val_mae: 11.6350 - val_mse: 231.4232\n",
      "Epoch 2019/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4337 - mae: 0.4400 - mse: 0.4337 - val_loss: 232.6653 - val_mae: 11.6996 - val_mse: 232.6653\n",
      "Epoch 2020/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4827 - mae: 0.4442 - mse: 0.4827 - val_loss: 232.3184 - val_mae: 11.5538 - val_mse: 232.3184\n",
      "Epoch 2021/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5800 - mae: 0.5312 - mse: 0.5800 - val_loss: 223.9874 - val_mae: 11.5102 - val_mse: 223.9874\n",
      "Epoch 2022/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4868 - mae: 0.4790 - mse: 0.4868 - val_loss: 235.3291 - val_mae: 11.6609 - val_mse: 235.3291\n",
      "Epoch 2023/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4424 - mae: 0.4309 - mse: 0.4424 - val_loss: 224.9888 - val_mae: 11.5141 - val_mse: 224.9888\n",
      "Epoch 2024/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4500 - mae: 0.4487 - mse: 0.4500 - val_loss: 230.6838 - val_mae: 11.6027 - val_mse: 230.6838\n",
      "Epoch 2025/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3732 - mae: 0.3577 - mse: 0.3732 - val_loss: 231.3304 - val_mae: 11.6012 - val_mse: 231.3304\n",
      "Epoch 2026/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4206 - mae: 0.3865 - mse: 0.4206 - val_loss: 228.0426 - val_mae: 11.5670 - val_mse: 228.0426\n",
      "Epoch 2027/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4128 - mae: 0.3672 - mse: 0.4128 - val_loss: 236.8431 - val_mae: 11.6961 - val_mse: 236.8431\n",
      "Epoch 2028/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.3778 - mae: 0.4664 - mse: 0.377 - 0s 87us/step - loss: 0.4322 - mae: 0.4279 - mse: 0.4322 - val_loss: 225.8684 - val_mae: 11.5299 - val_mse: 225.8684\n",
      "Epoch 2029/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5202 - mae: 0.5059 - mse: 0.5202 - val_loss: 232.6713 - val_mae: 11.6530 - val_mse: 232.6713\n",
      "Epoch 2030/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6894 - mae: 0.5821 - mse: 0.6894 - val_loss: 231.4054 - val_mae: 11.5413 - val_mse: 231.4054\n",
      "Epoch 2031/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.8872 - mae: 0.7043 - mse: 0.8872 - val_loss: 226.2428 - val_mae: 11.5551 - val_mse: 226.2428\n",
      "Epoch 2032/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.9875 - mae: 0.7898 - mse: 0.9875 - val_loss: 240.3665 - val_mae: 11.7371 - val_mse: 240.3665\n",
      "Epoch 2033/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.8904 - mae: 0.7715 - mse: 0.8904 - val_loss: 231.4246 - val_mae: 11.7577 - val_mse: 231.4246\n",
      "Epoch 2034/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.1379 - mae: 0.8220 - mse: 1.1379 - val_loss: 240.4507 - val_mae: 11.7491 - val_mse: 240.4507\n",
      "Epoch 2035/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.2163 - mae: 0.8497 - mse: 1.2163 - val_loss: 225.2698 - val_mae: 11.5620 - val_mse: 225.2698\n",
      "Epoch 2036/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.9944 - mae: 0.7759 - mse: 0.9944 - val_loss: 223.4057 - val_mae: 11.3709 - val_mse: 223.4057\n",
      "Epoch 2037/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.8753 - mae: 0.6721 - mse: 0.8753 - val_loss: 239.5421 - val_mae: 11.7251 - val_mse: 239.5421\n",
      "Epoch 2038/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.1863 - mae: 0.7812 - mse: 1.1863 - val_loss: 220.9338 - val_mae: 11.4857 - val_mse: 220.9338\n",
      "Epoch 2039/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.6482 - mae: 0.9519 - mse: 1.6482 - val_loss: 236.3305 - val_mae: 11.5749 - val_mse: 236.3305\n",
      "Epoch 2040/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.3430 - mae: 0.9492 - mse: 1.3430 - val_loss: 228.7595 - val_mae: 11.6860 - val_mse: 228.7595\n",
      "Epoch 2041/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.0037 - mae: 0.7602 - mse: 1.0037 - val_loss: 232.6950 - val_mae: 11.6490 - val_mse: 232.6950\n",
      "Epoch 2042/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.8218 - mae: 0.6019 - mse: 0.8218 - val_loss: 232.3064 - val_mae: 11.6538 - val_mse: 232.3064\n",
      "Epoch 2043/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.6596 - mae: 0.5540 - mse: 0.6596 - val_loss: 225.8671 - val_mae: 11.5029 - val_mse: 225.8671\n",
      "Epoch 2044/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.5579 - mae: 0.4972 - mse: 0.5579 - val_loss: 228.4504 - val_mae: 11.4589 - val_mse: 228.4504\n",
      "Epoch 2045/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4227 - mae: 0.4146 - mse: 0.4227 - val_loss: 228.7194 - val_mae: 11.6528 - val_mse: 228.7194\n",
      "Epoch 2046/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4329 - mae: 0.4263 - mse: 0.4329 - val_loss: 234.1713 - val_mae: 11.6784 - val_mse: 234.1713\n",
      "Epoch 2047/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3836 - mae: 0.3798 - mse: 0.3836 - val_loss: 228.7680 - val_mae: 11.5258 - val_mse: 228.7680\n",
      "Epoch 2048/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4380 - mae: 0.3975 - mse: 0.4380 - val_loss: 228.8644 - val_mae: 11.5932 - val_mse: 228.8644\n",
      "Epoch 2049/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3912 - mae: 0.3894 - mse: 0.3912 - val_loss: 231.6436 - val_mae: 11.5813 - val_mse: 231.6436\n",
      "Epoch 2050/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3546 - mae: 0.3554 - mse: 0.3546 - val_loss: 224.9501 - val_mae: 11.4920 - val_mse: 224.9501\n",
      "Epoch 2051/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3207 - mae: 0.3044 - mse: 0.3207 - val_loss: 233.5381 - val_mae: 11.6186 - val_mse: 233.5381\n",
      "Epoch 2052/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3680 - mae: 0.3741 - mse: 0.3680 - val_loss: 230.7313 - val_mae: 11.6409 - val_mse: 230.7313\n",
      "Epoch 2053/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2908 - mae: 0.2833 - mse: 0.2908 - val_loss: 231.2538 - val_mae: 11.5958 - val_mse: 231.2538\n",
      "Epoch 2054/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.2648 - mae: 0.2554 - mse: 0.2648 - val_loss: 229.8641 - val_mae: 11.5800 - val_mse: 229.8641\n",
      "Epoch 2055/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2907 - mae: 0.2632 - mse: 0.2907 - val_loss: 228.4363 - val_mae: 11.5502 - val_mse: 228.4363\n",
      "Epoch 2056/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3139 - mae: 0.2865 - mse: 0.3139 - val_loss: 229.7870 - val_mae: 11.5466 - val_mse: 229.7870\n",
      "Epoch 2057/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3813 - mae: 0.3815 - mse: 0.3813 - val_loss: 226.2052 - val_mae: 11.5752 - val_mse: 226.2052\n",
      "Epoch 2058/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3371 - mae: 0.3325 - mse: 0.3371 - val_loss: 231.8367 - val_mae: 11.5940 - val_mse: 231.8367\n",
      "Epoch 2059/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.2891 - mae: 0.2843 - mse: 0.2891 - val_loss: 227.2187 - val_mae: 11.5935 - val_mse: 227.2187\n",
      "Epoch 2060/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2969 - mae: 0.2802 - mse: 0.2969 - val_loss: 231.5820 - val_mae: 11.5842 - val_mse: 231.5820\n",
      "Epoch 2061/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3309 - mae: 0.3148 - mse: 0.3309 - val_loss: 225.3383 - val_mae: 11.5547 - val_mse: 225.3383\n",
      "Epoch 2062/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3545 - mae: 0.3701 - mse: 0.3545 - val_loss: 232.1851 - val_mae: 11.5655 - val_mse: 232.1851\n",
      "Epoch 2063/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.3923 - mae: 0.3672 - mse: 0.3923 - val_loss: 227.5043 - val_mae: 11.5430 - val_mse: 227.5043\n",
      "Epoch 2064/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3701 - mae: 0.3373 - mse: 0.3701 - val_loss: 228.7288 - val_mae: 11.5825 - val_mse: 228.7288\n",
      "Epoch 2065/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2691 - mae: 0.2737 - mse: 0.2691 - val_loss: 233.3764 - val_mae: 11.6218 - val_mse: 233.3764\n",
      "Epoch 2066/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3584 - mae: 0.3337 - mse: 0.3584 - val_loss: 227.2330 - val_mae: 11.5689 - val_mse: 227.2330\n",
      "Epoch 2067/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3348 - mae: 0.3275 - mse: 0.3348 - val_loss: 230.3803 - val_mae: 11.5901 - val_mse: 230.3803\n",
      "Epoch 2068/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4865 - mae: 0.3960 - mse: 0.4865 - val_loss: 229.4398 - val_mae: 11.6208 - val_mse: 229.4398\n",
      "Epoch 2069/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.3931 - mae: 0.3940 - mse: 0.3931 - val_loss: 228.0534 - val_mae: 11.5835 - val_mse: 228.0534\n",
      "Epoch 2070/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3882 - mae: 0.3485 - mse: 0.3882 - val_loss: 230.1646 - val_mae: 11.6119 - val_mse: 230.1646\n",
      "Epoch 2071/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3655 - mae: 0.3694 - mse: 0.3655 - val_loss: 228.7354 - val_mae: 11.5757 - val_mse: 228.7354\n",
      "Epoch 2072/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3330 - mae: 0.3165 - mse: 0.3330 - val_loss: 230.3972 - val_mae: 11.5692 - val_mse: 230.3972\n",
      "Epoch 2073/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3680 - mae: 0.3608 - mse: 0.3680 - val_loss: 226.9202 - val_mae: 11.6196 - val_mse: 226.9202\n",
      "Epoch 2074/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4455 - mae: 0.4173 - mse: 0.4455 - val_loss: 229.8033 - val_mae: 11.5063 - val_mse: 229.8033\n",
      "Epoch 2075/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4407 - mae: 0.4202 - mse: 0.4407 - val_loss: 225.6975 - val_mae: 11.5528 - val_mse: 225.6975\n",
      "Epoch 2076/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3962 - mae: 0.4063 - mse: 0.3962 - val_loss: 230.8167 - val_mae: 11.5712 - val_mse: 230.8167\n",
      "Epoch 2077/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3916 - mae: 0.3662 - mse: 0.3916 - val_loss: 231.5354 - val_mae: 11.6193 - val_mse: 231.5354\n",
      "Epoch 2078/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3440 - mae: 0.3464 - mse: 0.3440 - val_loss: 231.2104 - val_mae: 11.6474 - val_mse: 231.2104\n",
      "Epoch 2079/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3207 - mae: 0.3024 - mse: 0.3207 - val_loss: 229.7487 - val_mae: 11.5314 - val_mse: 229.7487\n",
      "Epoch 2080/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3598 - mae: 0.3276 - mse: 0.3598 - val_loss: 230.5517 - val_mae: 11.5981 - val_mse: 230.5517\n",
      "Epoch 2081/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3720 - mae: 0.3354 - mse: 0.3720 - val_loss: 230.6362 - val_mae: 11.5884 - val_mse: 230.6362\n",
      "Epoch 2082/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3281 - mae: 0.3368 - mse: 0.3281 - val_loss: 228.6755 - val_mae: 11.5629 - val_mse: 228.6755\n",
      "Epoch 2083/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3677 - mae: 0.3402 - mse: 0.3677 - val_loss: 229.5795 - val_mae: 11.5821 - val_mse: 229.5795\n",
      "Epoch 2084/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3525 - mae: 0.3253 - mse: 0.3525 - val_loss: 230.2706 - val_mae: 11.6230 - val_mse: 230.2706\n",
      "Epoch 2085/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3805 - mae: 0.3629 - mse: 0.3805 - val_loss: 230.1378 - val_mae: 11.5215 - val_mse: 230.1378\n",
      "Epoch 2086/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4478 - mae: 0.4011 - mse: 0.4478 - val_loss: 230.2449 - val_mae: 11.5738 - val_mse: 230.2449\n",
      "Epoch 2087/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3617 - mae: 0.3505 - mse: 0.3617 - val_loss: 226.6487 - val_mae: 11.5304 - val_mse: 226.6487\n",
      "Epoch 2088/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3729 - mae: 0.3572 - mse: 0.3729 - val_loss: 228.6787 - val_mae: 11.5162 - val_mse: 228.6787\n",
      "Epoch 2089/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3794 - mae: 0.3692 - mse: 0.3794 - val_loss: 232.9143 - val_mae: 11.7154 - val_mse: 232.9143\n",
      "Epoch 2090/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3551 - mae: 0.3735 - mse: 0.3551 - val_loss: 232.5828 - val_mae: 11.6492 - val_mse: 232.5828\n",
      "Epoch 2091/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3961 - mae: 0.4080 - mse: 0.3961 - val_loss: 229.9917 - val_mae: 11.5864 - val_mse: 229.9917\n",
      "Epoch 2092/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4042 - mae: 0.3863 - mse: 0.4042 - val_loss: 228.2917 - val_mae: 11.4835 - val_mse: 228.2917\n",
      "Epoch 2093/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3892 - mae: 0.3968 - mse: 0.3892 - val_loss: 229.2014 - val_mae: 11.5790 - val_mse: 229.2014\n",
      "Epoch 2094/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4661 - mae: 0.3897 - mse: 0.4661 - val_loss: 229.9307 - val_mae: 11.6360 - val_mse: 229.9307\n",
      "Epoch 2095/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4449 - mae: 0.4300 - mse: 0.4449 - val_loss: 230.5543 - val_mae: 11.5266 - val_mse: 230.5543\n",
      "Epoch 2096/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3659 - mae: 0.3859 - mse: 0.3659 - val_loss: 232.5878 - val_mae: 11.6610 - val_mse: 232.5878\n",
      "Epoch 2097/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4973 - mae: 0.4458 - mse: 0.4973 - val_loss: 230.1103 - val_mae: 11.5656 - val_mse: 230.1103\n",
      "Epoch 2098/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4836 - mae: 0.4186 - mse: 0.4836 - val_loss: 233.9603 - val_mae: 11.5541 - val_mse: 233.9603\n",
      "Epoch 2099/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6143 - mae: 0.5962 - mse: 0.6143 - val_loss: 224.0502 - val_mae: 11.5394 - val_mse: 224.0502\n",
      "Epoch 2100/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.7844 - mae: 0.6637 - mse: 0.7844 - val_loss: 233.6774 - val_mae: 11.5841 - val_mse: 233.6774\n",
      "Epoch 2101/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.8402 - mae: 0.7330 - mse: 0.8402 - val_loss: 219.3074 - val_mae: 11.4463 - val_mse: 219.3074\n",
      "Epoch 2102/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.0913 - mae: 0.8317 - mse: 1.0913 - val_loss: 235.3065 - val_mae: 11.6391 - val_mse: 235.3065\n",
      "Epoch 2103/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.9188 - mae: 0.7087 - mse: 0.9188 - val_loss: 236.1564 - val_mae: 11.6765 - val_mse: 236.1564\n",
      "Epoch 2104/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.8302 - mae: 0.6938 - mse: 0.8302 - val_loss: 229.5939 - val_mae: 11.7063 - val_mse: 229.5939\n",
      "Epoch 2105/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.7175 - mae: 0.6263 - mse: 0.7175 - val_loss: 234.3004 - val_mae: 11.6294 - val_mse: 234.3004\n",
      "Epoch 2106/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6830 - mae: 0.5795 - mse: 0.6830 - val_loss: 221.9415 - val_mae: 11.4543 - val_mse: 221.9415\n",
      "Epoch 2107/3500\n",
      "126/126 [==============================] - 0s 158us/step - loss: 0.5952 - mae: 0.5812 - mse: 0.5952 - val_loss: 232.9739 - val_mae: 11.6252 - val_mse: 232.9739\n",
      "Epoch 2108/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.6838 - mae: 0.5546 - mse: 0.6838 - val_loss: 233.4557 - val_mae: 11.7303 - val_mse: 233.4557\n",
      "Epoch 2109/3500\n",
      "126/126 [==============================] - 0s 142us/step - loss: 0.7626 - mae: 0.5839 - mse: 0.7626 - val_loss: 233.8063 - val_mae: 11.7658 - val_mse: 233.8063\n",
      "Epoch 2110/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4888 - mae: 0.4996 - mse: 0.4888 - val_loss: 235.4838 - val_mae: 11.5933 - val_mse: 235.4838\n",
      "Epoch 2111/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.7221 - mae: 0.5320 - mse: 0.7221 - val_loss: 222.0753 - val_mae: 11.4253 - val_mse: 222.0753\n",
      "Epoch 2112/3500\n",
      "126/126 [==============================] - 0s 142us/step - loss: 0.5697 - mae: 0.4964 - mse: 0.5697 - val_loss: 232.6025 - val_mae: 11.6229 - val_mse: 232.6025\n",
      "Epoch 2113/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.3524 - mae: 0.4099 - mse: 0.3524 - val_loss: 230.2165 - val_mae: 11.5516 - val_mse: 230.2165\n",
      "Epoch 2114/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4591 - mae: 0.4643 - mse: 0.4591 - val_loss: 229.2556 - val_mae: 11.5412 - val_mse: 229.2556\n",
      "Epoch 2115/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3676 - mae: 0.3631 - mse: 0.3676 - val_loss: 230.7361 - val_mae: 11.6131 - val_mse: 230.7361\n",
      "Epoch 2116/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3357 - mae: 0.3180 - mse: 0.3357 - val_loss: 230.9852 - val_mae: 11.6154 - val_mse: 230.9852\n",
      "Epoch 2117/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3415 - mae: 0.3346 - mse: 0.3415 - val_loss: 233.4939 - val_mae: 11.7067 - val_mse: 233.4939\n",
      "Epoch 2118/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3129 - mae: 0.3067 - mse: 0.3129 - val_loss: 228.6425 - val_mae: 11.5700 - val_mse: 228.6425\n",
      "Epoch 2119/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3427 - mae: 0.3234 - mse: 0.3427 - val_loss: 230.4664 - val_mae: 11.5769 - val_mse: 230.4664\n",
      "Epoch 2120/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2803 - mae: 0.2346 - mse: 0.2803 - val_loss: 231.1155 - val_mae: 11.6251 - val_mse: 231.1155\n",
      "Epoch 2121/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2659 - mae: 0.2446 - mse: 0.2659 - val_loss: 229.4115 - val_mae: 11.5570 - val_mse: 229.4115\n",
      "Epoch 2122/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2744 - mae: 0.2728 - mse: 0.2744 - val_loss: 232.3062 - val_mae: 11.6068 - val_mse: 232.3062\n",
      "Epoch 2123/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3418 - mae: 0.3079 - mse: 0.3418 - val_loss: 231.3115 - val_mae: 11.6533 - val_mse: 231.3115\n",
      "Epoch 2124/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3153 - mae: 0.3115 - mse: 0.3153 - val_loss: 229.7730 - val_mae: 11.5816 - val_mse: 229.7730\n",
      "Epoch 2125/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3343 - mae: 0.2990 - mse: 0.3343 - val_loss: 232.3360 - val_mae: 11.6261 - val_mse: 232.3360\n",
      "Epoch 2126/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3849 - mae: 0.3508 - mse: 0.3849 - val_loss: 230.2929 - val_mae: 11.5937 - val_mse: 230.2929\n",
      "Epoch 2127/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5276 - mae: 0.4438 - mse: 0.5276 - val_loss: 228.6722 - val_mae: 11.6228 - val_mse: 228.6722\n",
      "Epoch 2128/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5778 - mae: 0.5149 - mse: 0.5778 - val_loss: 231.9170 - val_mae: 11.5990 - val_mse: 231.9170\n",
      "Epoch 2129/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4680 - mae: 0.4701 - mse: 0.4680 - val_loss: 223.7160 - val_mae: 11.5200 - val_mse: 223.7160\n",
      "Epoch 2130/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5052 - mae: 0.4222 - mse: 0.5052 - val_loss: 237.0672 - val_mae: 11.6786 - val_mse: 237.0672\n",
      "Epoch 2131/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4131 - mae: 0.3885 - mse: 0.4131 - val_loss: 227.6226 - val_mae: 11.5122 - val_mse: 227.6226\n",
      "Epoch 2132/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6554 - mae: 0.5252 - mse: 0.6554 - val_loss: 226.9810 - val_mae: 11.5488 - val_mse: 226.9810\n",
      "Epoch 2133/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5632 - mae: 0.5334 - mse: 0.5632 - val_loss: 236.4797 - val_mae: 11.6544 - val_mse: 236.4797\n",
      "Epoch 2134/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.6509 - mae: 0.5344 - mse: 0.6509 - val_loss: 227.2803 - val_mae: 11.6326 - val_mse: 227.2803\n",
      "Epoch 2135/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6952 - mae: 0.5468 - mse: 0.6952 - val_loss: 233.2274 - val_mae: 11.6794 - val_mse: 233.2274\n",
      "Epoch 2136/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.6368 - mae: 0.5554 - mse: 0.6368 - val_loss: 226.4376 - val_mae: 11.4251 - val_mse: 226.4376\n",
      "Epoch 2137/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.7234 - mae: 0.6037 - mse: 0.7234 - val_loss: 228.7413 - val_mae: 11.6185 - val_mse: 228.7413\n",
      "Epoch 2138/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.8648 - mae: 0.6760 - mse: 0.8648 - val_loss: 237.3069 - val_mae: 11.7153 - val_mse: 237.3069\n",
      "Epoch 2139/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.7450 - mae: 0.6469 - mse: 0.7450 - val_loss: 225.8471 - val_mae: 11.5340 - val_mse: 225.8471\n",
      "Epoch 2140/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6074 - mae: 0.5207 - mse: 0.6074 - val_loss: 231.6415 - val_mae: 11.5544 - val_mse: 231.6415\n",
      "Epoch 2141/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4970 - mae: 0.4991 - mse: 0.4970 - val_loss: 229.2446 - val_mae: 11.4942 - val_mse: 229.2446\n",
      "Epoch 2142/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4491 - mae: 0.4424 - mse: 0.4491 - val_loss: 231.0803 - val_mae: 11.6678 - val_mse: 231.0803\n",
      "Epoch 2143/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4227 - mae: 0.4090 - mse: 0.4227 - val_loss: 233.7416 - val_mae: 11.6666 - val_mse: 233.7416\n",
      "Epoch 2144/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3886 - mae: 0.4187 - mse: 0.3886 - val_loss: 231.0969 - val_mae: 11.6486 - val_mse: 231.0969\n",
      "Epoch 2145/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4600 - mae: 0.4611 - mse: 0.4600 - val_loss: 233.2985 - val_mae: 11.6147 - val_mse: 233.2985\n",
      "Epoch 2146/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3272 - mae: 0.3390 - mse: 0.3272 - val_loss: 224.0866 - val_mae: 11.4586 - val_mse: 224.0866\n",
      "Epoch 2147/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3989 - mae: 0.3863 - mse: 0.3989 - val_loss: 233.6955 - val_mae: 11.5968 - val_mse: 233.6955\n",
      "Epoch 2148/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5203 - mae: 0.4873 - mse: 0.5203 - val_loss: 226.1564 - val_mae: 11.5436 - val_mse: 226.1564\n",
      "Epoch 2149/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5657 - mae: 0.5136 - mse: 0.5657 - val_loss: 230.8201 - val_mae: 11.5794 - val_mse: 230.8201\n",
      "Epoch 2150/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6655 - mae: 0.5650 - mse: 0.6655 - val_loss: 231.8263 - val_mae: 11.5651 - val_mse: 231.8263\n",
      "Epoch 2151/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.8090 - mae: 0.6370 - mse: 0.8090 - val_loss: 223.6157 - val_mae: 11.4550 - val_mse: 223.6157\n",
      "Epoch 2152/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5176 - mae: 0.5265 - mse: 0.5176 - val_loss: 239.1963 - val_mae: 11.6398 - val_mse: 239.1963\n",
      "Epoch 2153/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5414 - mae: 0.4734 - mse: 0.5414 - val_loss: 221.8542 - val_mae: 11.4882 - val_mse: 221.8542\n",
      "Epoch 2154/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5937 - mae: 0.5683 - mse: 0.5937 - val_loss: 231.9458 - val_mae: 11.5508 - val_mse: 231.9458\n",
      "Epoch 2155/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6503 - mae: 0.5614 - mse: 0.6503 - val_loss: 230.3460 - val_mae: 11.6317 - val_mse: 230.3460\n",
      "Epoch 2156/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5200 - mae: 0.4869 - mse: 0.5200 - val_loss: 227.3502 - val_mae: 11.5424 - val_mse: 227.3502\n",
      "Epoch 2157/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.6336 - mae: 0.5516 - mse: 0.6336 - val_loss: 236.5312 - val_mae: 11.5758 - val_mse: 236.5312\n",
      "Epoch 2158/3500\n",
      "126/126 [==============================] - 0s 182us/step - loss: 0.7241 - mae: 0.6188 - mse: 0.7241 - val_loss: 226.2689 - val_mae: 11.6190 - val_mse: 226.2689\n",
      "Epoch 2159/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.6193 - mae: 0.5683 - mse: 0.6193 - val_loss: 234.2854 - val_mae: 11.6297 - val_mse: 234.2854\n",
      "Epoch 2160/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6153 - mae: 0.5467 - mse: 0.6153 - val_loss: 225.5802 - val_mae: 11.5576 - val_mse: 225.5802\n",
      "Epoch 2161/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6683 - mae: 0.5713 - mse: 0.6683 - val_loss: 224.9979 - val_mae: 11.4083 - val_mse: 224.9979\n",
      "Epoch 2162/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6125 - mae: 0.5004 - mse: 0.6125 - val_loss: 230.1909 - val_mae: 11.6528 - val_mse: 230.1909\n",
      "Epoch 2163/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5563 - mae: 0.5500 - mse: 0.5563 - val_loss: 227.0894 - val_mae: 11.5754 - val_mse: 227.0894\n",
      "Epoch 2164/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4568 - mae: 0.4417 - mse: 0.4568 - val_loss: 230.5214 - val_mae: 11.5304 - val_mse: 230.5214\n",
      "Epoch 2165/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3889 - mae: 0.3953 - mse: 0.3889 - val_loss: 227.5886 - val_mae: 11.5906 - val_mse: 227.5886\n",
      "Epoch 2166/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3710 - mae: 0.3816 - mse: 0.3710 - val_loss: 230.0670 - val_mae: 11.5209 - val_mse: 230.0670\n",
      "Epoch 2167/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3601 - mae: 0.3598 - mse: 0.3601 - val_loss: 227.5338 - val_mae: 11.5290 - val_mse: 227.5338\n",
      "Epoch 2168/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3232 - mae: 0.3286 - mse: 0.3232 - val_loss: 229.9857 - val_mae: 11.5678 - val_mse: 229.9857\n",
      "Epoch 2169/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4055 - mae: 0.3942 - mse: 0.4055 - val_loss: 231.0223 - val_mae: 11.5685 - val_mse: 231.0223\n",
      "Epoch 2170/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4864 - mae: 0.4308 - mse: 0.4864 - val_loss: 234.0401 - val_mae: 11.6582 - val_mse: 234.0401\n",
      "Epoch 2171/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5558 - mae: 0.4787 - mse: 0.5558 - val_loss: 224.7846 - val_mae: 11.4743 - val_mse: 224.7846\n",
      "Epoch 2172/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4651 - mae: 0.4535 - mse: 0.4651 - val_loss: 234.1431 - val_mae: 11.5818 - val_mse: 234.1431\n",
      "Epoch 2173/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6992 - mae: 0.6100 - mse: 0.6992 - val_loss: 225.7467 - val_mae: 11.5244 - val_mse: 225.7467\n",
      "Epoch 2174/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.7668 - mae: 0.6284 - mse: 0.7668 - val_loss: 231.7401 - val_mae: 11.5602 - val_mse: 231.7401\n",
      "Epoch 2175/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.7752 - mae: 0.5966 - mse: 0.7752 - val_loss: 235.9318 - val_mae: 11.6747 - val_mse: 235.9318\n",
      "Epoch 2176/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4910 - mae: 0.4697 - mse: 0.4910 - val_loss: 223.4367 - val_mae: 11.4534 - val_mse: 223.4367\n",
      "Epoch 2177/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4561 - mae: 0.4237 - mse: 0.4561 - val_loss: 231.0344 - val_mae: 11.6124 - val_mse: 231.0344\n",
      "Epoch 2178/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4370 - mae: 0.4487 - mse: 0.4370 - val_loss: 228.4002 - val_mae: 11.5960 - val_mse: 228.4002\n",
      "Epoch 2179/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5645 - mae: 0.5279 - mse: 0.5645 - val_loss: 233.7235 - val_mae: 11.6704 - val_mse: 233.7235\n",
      "Epoch 2180/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4785 - mae: 0.4658 - mse: 0.4785 - val_loss: 231.3649 - val_mae: 11.6101 - val_mse: 231.3649\n",
      "Epoch 2181/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3766 - mae: 0.3866 - mse: 0.3766 - val_loss: 226.0199 - val_mae: 11.5158 - val_mse: 226.0199\n",
      "Epoch 2182/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4068 - mae: 0.3984 - mse: 0.4068 - val_loss: 236.8910 - val_mae: 11.7129 - val_mse: 236.8910\n",
      "Epoch 2183/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5682 - mae: 0.4813 - mse: 0.5682 - val_loss: 228.6860 - val_mae: 11.5914 - val_mse: 228.6860\n",
      "Epoch 2184/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.6663 - mae: 0.5613 - mse: 0.6663 - val_loss: 232.0477 - val_mae: 11.5976 - val_mse: 232.0477\n",
      "Epoch 2185/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5583 - mae: 0.5554 - mse: 0.5583 - val_loss: 231.4460 - val_mae: 11.7130 - val_mse: 231.4460\n",
      "Epoch 2186/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6520 - mae: 0.5877 - mse: 0.6520 - val_loss: 229.7465 - val_mae: 11.5391 - val_mse: 229.7465\n",
      "Epoch 2187/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.9091 - mae: 0.7333 - mse: 0.9091 - val_loss: 227.6859 - val_mae: 11.4684 - val_mse: 227.6859\n",
      "Epoch 2188/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.8338 - mae: 0.6713 - mse: 0.8338 - val_loss: 233.4530 - val_mae: 11.6809 - val_mse: 233.4530\n",
      "Epoch 2189/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.2886 - mae: 0.8712 - mse: 1.2886 - val_loss: 236.5819 - val_mae: 11.5724 - val_mse: 236.5819\n",
      "Epoch 2190/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 2.0957 - mae: 1.1158 - mse: 2.0957 - val_loss: 229.5471 - val_mae: 11.6772 - val_mse: 229.5471\n",
      "Epoch 2191/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.7086 - mae: 0.9609 - mse: 1.7086 - val_loss: 233.3529 - val_mae: 11.7049 - val_mse: 233.3529\n",
      "Epoch 2192/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.7920 - mae: 1.0355 - mse: 1.7920 - val_loss: 236.0242 - val_mae: 11.6796 - val_mse: 236.0242\n",
      "Epoch 2193/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.4713 - mae: 0.9673 - mse: 1.4713 - val_loss: 217.4509 - val_mae: 11.3223 - val_mse: 217.4509\n",
      "Epoch 2194/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.6418 - mae: 0.9756 - mse: 1.6418 - val_loss: 237.1491 - val_mae: 11.5999 - val_mse: 237.1491\n",
      "Epoch 2195/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.6161 - mae: 1.0261 - mse: 1.6161 - val_loss: 222.0165 - val_mae: 11.4734 - val_mse: 222.0165\n",
      "Epoch 2196/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.3841 - mae: 0.8963 - mse: 1.3841 - val_loss: 244.4406 - val_mae: 11.7168 - val_mse: 244.4406\n",
      "Epoch 2197/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.8700 - mae: 1.0064 - mse: 1.8700 - val_loss: 223.8456 - val_mae: 11.4916 - val_mse: 223.8456\n",
      "Epoch 2198/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 2.1542 - mae: 1.1253 - mse: 2.1542 - val_loss: 228.2576 - val_mae: 11.4947 - val_mse: 228.2576\n",
      "Epoch 2199/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 2.1377 - mae: 1.1783 - mse: 2.1377 - val_loss: 239.3007 - val_mae: 11.7119 - val_mse: 239.3007\n",
      "Epoch 2200/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.6991 - mae: 1.0289 - mse: 1.6991 - val_loss: 224.5499 - val_mae: 11.4324 - val_mse: 224.5499\n",
      "Epoch 2201/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.6069 - mae: 0.9644 - mse: 1.6069 - val_loss: 238.0271 - val_mae: 11.6389 - val_mse: 238.0271\n",
      "Epoch 2202/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 1.5262 - mae: 0.8974 - mse: 1.5262 - val_loss: 221.2403 - val_mae: 11.4141 - val_mse: 221.2403\n",
      "Epoch 2203/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 1.5614 - mae: 0.9595 - mse: 1.5614 - val_loss: 229.4821 - val_mae: 11.5503 - val_mse: 229.4821\n",
      "Epoch 2204/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 1.8802 - mae: 1.0779 - mse: 1.8802 - val_loss: 235.9013 - val_mae: 11.8276 - val_mse: 235.9013\n",
      "Epoch 2205/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 2.0185 - mae: 1.0962 - mse: 2.0185 - val_loss: 233.0483 - val_mae: 11.6544 - val_mse: 233.0483\n",
      "Epoch 2206/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 1.7885 - mae: 1.0562 - mse: 1.7885 - val_loss: 227.5300 - val_mae: 11.4309 - val_mse: 227.5300\n",
      "Epoch 2207/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.3385 - mae: 0.8880 - mse: 1.3385 - val_loss: 221.1510 - val_mae: 11.2986 - val_mse: 221.1510\n",
      "Epoch 2208/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 1.3579 - mae: 0.9271 - mse: 1.3579 - val_loss: 229.8999 - val_mae: 11.4364 - val_mse: 229.8999\n",
      "Epoch 2209/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 1.4816 - mae: 0.9748 - mse: 1.4816 - val_loss: 227.8375 - val_mae: 11.6727 - val_mse: 227.8375\n",
      "Epoch 2210/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.6650 - mae: 1.0253 - mse: 1.6650 - val_loss: 245.5012 - val_mae: 11.8147 - val_mse: 245.5012\n",
      "Epoch 2211/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.6922 - mae: 1.0041 - mse: 1.6922 - val_loss: 228.1325 - val_mae: 11.7399 - val_mse: 228.1325\n",
      "Epoch 2212/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 2.2325 - mae: 1.2177 - mse: 2.2325 - val_loss: 235.9109 - val_mae: 11.6939 - val_mse: 235.9109\n",
      "Epoch 2213/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 2.2261 - mae: 1.2132 - mse: 2.2261 - val_loss: 214.5063 - val_mae: 11.3314 - val_mse: 214.5063\n",
      "Epoch 2214/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 2.4397 - mae: 1.2158 - mse: 2.4397 - val_loss: 238.1775 - val_mae: 11.6408 - val_mse: 238.1775\n",
      "Epoch 2215/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 2.6989 - mae: 1.2237 - mse: 2.6989 - val_loss: 225.3746 - val_mae: 11.6109 - val_mse: 225.3746\n",
      "Epoch 2216/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 2.8468 - mae: 1.2194 - mse: 2.8468 - val_loss: 251.6416 - val_mae: 11.7766 - val_mse: 251.6416\n",
      "Epoch 2217/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 2.8528 - mae: 1.3071 - mse: 2.8528 - val_loss: 215.4958 - val_mae: 11.2640 - val_mse: 215.4958\n",
      "Epoch 2218/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.9966 - mae: 1.1008 - mse: 1.9966 - val_loss: 217.3075 - val_mae: 11.3396 - val_mse: 217.3075\n",
      "Epoch 2219/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.1710 - mae: 0.8295 - mse: 1.1710 - val_loss: 235.9098 - val_mae: 11.7256 - val_mse: 235.9098\n",
      "Epoch 2220/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.5657 - mae: 0.9141 - mse: 1.5657 - val_loss: 217.8119 - val_mae: 11.5134 - val_mse: 217.8119\n",
      "Epoch 2221/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.2488 - mae: 0.8145 - mse: 1.2488 - val_loss: 233.2089 - val_mae: 11.5954 - val_mse: 233.2089\n",
      "Epoch 2222/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.9701 - mae: 0.7307 - mse: 0.9701 - val_loss: 229.9333 - val_mae: 11.6533 - val_mse: 229.9333\n",
      "Epoch 2223/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.0160 - mae: 0.7608 - mse: 1.0160 - val_loss: 230.5824 - val_mae: 11.5973 - val_mse: 230.5824\n",
      "Epoch 2224/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.0440 - mae: 0.7546 - mse: 1.0440 - val_loss: 226.1306 - val_mae: 11.4939 - val_mse: 226.1306\n",
      "Epoch 2225/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.8534 - mae: 0.6802 - mse: 0.8534 - val_loss: 231.4102 - val_mae: 11.5865 - val_mse: 231.4102\n",
      "Epoch 2226/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.0392 - mae: 0.6785 - mse: 1.0392 - val_loss: 222.9680 - val_mae: 11.4242 - val_mse: 222.9680\n",
      "Epoch 2227/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.0869 - mae: 0.7546 - mse: 1.0869 - val_loss: 226.2331 - val_mae: 11.5331 - val_mse: 226.2331\n",
      "Epoch 2228/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.0202 - mae: 0.7511 - mse: 1.0202 - val_loss: 225.5241 - val_mae: 11.4174 - val_mse: 225.5241\n",
      "Epoch 2229/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.3141 - mae: 0.8422 - mse: 1.3141 - val_loss: 218.3561 - val_mae: 11.3271 - val_mse: 218.3561\n",
      "Epoch 2230/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.3077 - mae: 0.8631 - mse: 1.3077 - val_loss: 230.6595 - val_mae: 11.5802 - val_mse: 230.6595\n",
      "Epoch 2231/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.9835 - mae: 0.7629 - mse: 0.9835 - val_loss: 220.2289 - val_mae: 11.2538 - val_mse: 220.2289\n",
      "Epoch 2232/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.0283 - mae: 0.7810 - mse: 1.0283 - val_loss: 220.6333 - val_mae: 11.4053 - val_mse: 220.6333\n",
      "Epoch 2233/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.8637 - mae: 0.6185 - mse: 0.8637 - val_loss: 239.4012 - val_mae: 11.7126 - val_mse: 239.4012\n",
      "Epoch 2234/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.7218 - mae: 0.5719 - mse: 0.7218 - val_loss: 225.7313 - val_mae: 11.3927 - val_mse: 225.7313\n",
      "Epoch 2235/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6776 - mae: 0.5970 - mse: 0.6776 - val_loss: 219.6015 - val_mae: 11.3392 - val_mse: 219.6015\n",
      "Epoch 2236/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5543 - mae: 0.5274 - mse: 0.5543 - val_loss: 227.1688 - val_mae: 11.3909 - val_mse: 227.1688\n",
      "Epoch 2237/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4381 - mae: 0.4426 - mse: 0.4381 - val_loss: 222.6879 - val_mae: 11.4566 - val_mse: 222.6879\n",
      "Epoch 2238/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3890 - mae: 0.3987 - mse: 0.3890 - val_loss: 232.9269 - val_mae: 11.6450 - val_mse: 232.9269\n",
      "Epoch 2239/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4469 - mae: 0.4354 - mse: 0.4469 - val_loss: 224.7505 - val_mae: 11.4724 - val_mse: 224.7505\n",
      "Epoch 2240/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4059 - mae: 0.4124 - mse: 0.4059 - val_loss: 224.7346 - val_mae: 11.4182 - val_mse: 224.7346\n",
      "Epoch 2241/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3930 - mae: 0.3610 - mse: 0.3930 - val_loss: 225.9387 - val_mae: 11.5106 - val_mse: 225.9387\n",
      "Epoch 2242/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3686 - mae: 0.3684 - mse: 0.3686 - val_loss: 224.6188 - val_mae: 11.4353 - val_mse: 224.6188\n",
      "Epoch 2243/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3724 - mae: 0.3925 - mse: 0.3724 - val_loss: 223.6821 - val_mae: 11.4565 - val_mse: 223.6821\n",
      "Epoch 2244/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3835 - mae: 0.3720 - mse: 0.3835 - val_loss: 229.8567 - val_mae: 11.5288 - val_mse: 229.8567\n",
      "Epoch 2245/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3377 - mae: 0.3547 - mse: 0.3377 - val_loss: 223.8928 - val_mae: 11.4222 - val_mse: 223.8928\n",
      "Epoch 2246/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3478 - mae: 0.3640 - mse: 0.3478 - val_loss: 225.1684 - val_mae: 11.4308 - val_mse: 225.1684\n",
      "Epoch 2247/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3639 - mae: 0.3574 - mse: 0.3639 - val_loss: 224.5839 - val_mae: 11.4448 - val_mse: 224.5839\n",
      "Epoch 2248/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2841 - mae: 0.2542 - mse: 0.2841 - val_loss: 224.8198 - val_mae: 11.4542 - val_mse: 224.8198\n",
      "Epoch 2249/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2979 - mae: 0.2796 - mse: 0.2979 - val_loss: 227.0586 - val_mae: 11.5078 - val_mse: 227.0586\n",
      "Epoch 2250/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2912 - mae: 0.2667 - mse: 0.2912 - val_loss: 226.6152 - val_mae: 11.4784 - val_mse: 226.6152\n",
      "Epoch 2251/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2536 - mae: 0.2289 - mse: 0.2536 - val_loss: 226.1955 - val_mae: 11.4681 - val_mse: 226.1955\n",
      "Epoch 2252/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2517 - mae: 0.2295 - mse: 0.2517 - val_loss: 225.5442 - val_mae: 11.4540 - val_mse: 225.5442\n",
      "Epoch 2253/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2635 - mae: 0.2273 - mse: 0.2635 - val_loss: 226.4908 - val_mae: 11.4716 - val_mse: 226.4908\n",
      "Epoch 2254/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.2755 - mae: 0.2215 - mse: 0.2755 - val_loss: 224.8911 - val_mae: 11.4449 - val_mse: 224.8911\n",
      "Epoch 2255/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2577 - mae: 0.2018 - mse: 0.2577 - val_loss: 225.5253 - val_mae: 11.4541 - val_mse: 225.5253\n",
      "Epoch 2256/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2444 - mae: 0.1869 - mse: 0.2444 - val_loss: 225.4858 - val_mae: 11.4825 - val_mse: 225.4858\n",
      "Epoch 2257/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2709 - mae: 0.2307 - mse: 0.2709 - val_loss: 226.5987 - val_mae: 11.4656 - val_mse: 226.5987\n",
      "Epoch 2258/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.2684 - mae: 0.2509 - mse: 0.2684 - val_loss: 224.5354 - val_mae: 11.4893 - val_mse: 224.5354\n",
      "Epoch 2259/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2640 - mae: 0.2374 - mse: 0.2640 - val_loss: 226.9139 - val_mae: 11.4676 - val_mse: 226.9139\n",
      "Epoch 2260/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.2555 - mae: 0.2258 - mse: 0.2555 - val_loss: 225.5876 - val_mae: 11.4812 - val_mse: 225.5876\n",
      "Epoch 2261/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2485 - mae: 0.1989 - mse: 0.2485 - val_loss: 226.5654 - val_mae: 11.4913 - val_mse: 226.5654\n",
      "Epoch 2262/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2408 - mae: 0.1958 - mse: 0.2408 - val_loss: 225.1309 - val_mae: 11.4688 - val_mse: 225.1309\n",
      "Epoch 2263/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2444 - mae: 0.2151 - mse: 0.2444 - val_loss: 227.1019 - val_mae: 11.5020 - val_mse: 227.1019\n",
      "Epoch 2264/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2655 - mae: 0.2283 - mse: 0.2655 - val_loss: 226.4163 - val_mae: 11.4827 - val_mse: 226.4163\n",
      "Epoch 2265/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2770 - mae: 0.2455 - mse: 0.2770 - val_loss: 226.2949 - val_mae: 11.4661 - val_mse: 226.2949\n",
      "Epoch 2266/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.2390 - mae: 0.1926 - mse: 0.2390 - val_loss: 225.4529 - val_mae: 11.4672 - val_mse: 225.4529\n",
      "Epoch 2267/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.2430 - mae: 0.1821 - mse: 0.2430 - val_loss: 224.9424 - val_mae: 11.4606 - val_mse: 224.9424\n",
      "Epoch 2268/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2712 - mae: 0.2065 - mse: 0.2712 - val_loss: 227.3477 - val_mae: 11.5113 - val_mse: 227.3477\n",
      "Epoch 2269/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2613 - mae: 0.1975 - mse: 0.2613 - val_loss: 227.3516 - val_mae: 11.5111 - val_mse: 227.3516\n",
      "Epoch 2270/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2329 - mae: 0.1631 - mse: 0.2329 - val_loss: 224.3113 - val_mae: 11.4418 - val_mse: 224.3113\n",
      "Epoch 2271/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2393 - mae: 0.1753 - mse: 0.2393 - val_loss: 225.5926 - val_mae: 11.4529 - val_mse: 225.5926\n",
      "Epoch 2272/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2880 - mae: 0.2151 - mse: 0.2880 - val_loss: 225.1103 - val_mae: 11.4783 - val_mse: 225.1103\n",
      "Epoch 2273/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3477 - mae: 0.3020 - mse: 0.3477 - val_loss: 228.7177 - val_mae: 11.4946 - val_mse: 228.7177\n",
      "Epoch 2274/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3101 - mae: 0.3146 - mse: 0.3101 - val_loss: 224.1802 - val_mae: 11.5056 - val_mse: 224.1802\n",
      "Epoch 2275/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3568 - mae: 0.3499 - mse: 0.3568 - val_loss: 228.8081 - val_mae: 11.5210 - val_mse: 228.8081\n",
      "Epoch 2276/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4298 - mae: 0.4046 - mse: 0.4298 - val_loss: 226.2153 - val_mae: 11.5152 - val_mse: 226.2153\n",
      "Epoch 2277/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4500 - mae: 0.4274 - mse: 0.4500 - val_loss: 227.1697 - val_mae: 11.4665 - val_mse: 227.1697\n",
      "Epoch 2278/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4369 - mae: 0.4184 - mse: 0.4369 - val_loss: 221.4801 - val_mae: 11.4364 - val_mse: 221.4801\n",
      "Epoch 2279/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3518 - mae: 0.3498 - mse: 0.3518 - val_loss: 229.2589 - val_mae: 11.5282 - val_mse: 229.2589\n",
      "Epoch 2280/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3826 - mae: 0.3618 - mse: 0.3826 - val_loss: 227.3035 - val_mae: 11.5191 - val_mse: 227.3035\n",
      "Epoch 2281/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4168 - mae: 0.4046 - mse: 0.4168 - val_loss: 229.6298 - val_mae: 11.5517 - val_mse: 229.6298\n",
      "Epoch 2282/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3630 - mae: 0.3798 - mse: 0.3630 - val_loss: 229.1089 - val_mae: 11.5175 - val_mse: 229.1089\n",
      "Epoch 2283/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4161 - mae: 0.4256 - mse: 0.4161 - val_loss: 221.4477 - val_mae: 11.4121 - val_mse: 221.4477\n",
      "Epoch 2284/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.4918 - mae: 0.4929 - mse: 0.4918 - val_loss: 230.8600 - val_mae: 11.4981 - val_mse: 230.8600\n",
      "Epoch 2285/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4504 - mae: 0.4562 - mse: 0.4504 - val_loss: 218.4434 - val_mae: 11.4199 - val_mse: 218.4434\n",
      "Epoch 2286/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.6595 - mae: 0.6198 - mse: 0.6595 - val_loss: 227.7234 - val_mae: 11.5043 - val_mse: 227.7234\n",
      "Epoch 2287/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5999 - mae: 0.5356 - mse: 0.5999 - val_loss: 229.3430 - val_mae: 11.5718 - val_mse: 229.3430\n",
      "Epoch 2288/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.5161 - mae: 0.5238 - mse: 0.5161 - val_loss: 223.8280 - val_mae: 11.4536 - val_mse: 223.8280\n",
      "Epoch 2289/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6175 - mae: 0.5522 - mse: 0.6175 - val_loss: 230.0712 - val_mae: 11.5068 - val_mse: 230.0712\n",
      "Epoch 2290/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.5689 - mae: 0.5422 - mse: 0.5689 - val_loss: 222.8730 - val_mae: 11.4274 - val_mse: 222.8730\n",
      "Epoch 2291/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.3437 - mae: 0.4808 - mse: 0.343 - 0s 63us/step - loss: 0.4882 - mae: 0.4888 - mse: 0.4882 - val_loss: 225.5364 - val_mae: 11.4265 - val_mse: 225.5364\n",
      "Epoch 2292/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5023 - mae: 0.4941 - mse: 0.5023 - val_loss: 226.7773 - val_mae: 11.5903 - val_mse: 226.7773\n",
      "Epoch 2293/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5168 - mae: 0.4917 - mse: 0.5168 - val_loss: 230.9642 - val_mae: 11.5993 - val_mse: 230.9642\n",
      "Epoch 2294/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.5115 - mae: 0.4775 - mse: 0.5115 - val_loss: 223.6161 - val_mae: 11.5247 - val_mse: 223.6161\n",
      "Epoch 2295/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4576 - mae: 0.4447 - mse: 0.4576 - val_loss: 223.8238 - val_mae: 11.4108 - val_mse: 223.8238\n",
      "Epoch 2296/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4330 - mae: 0.4097 - mse: 0.4330 - val_loss: 223.7126 - val_mae: 11.4359 - val_mse: 223.7126\n",
      "Epoch 2297/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4014 - mae: 0.3457 - mse: 0.4014 - val_loss: 226.8995 - val_mae: 11.5212 - val_mse: 226.8995\n",
      "Epoch 2298/3500\n",
      "126/126 [==============================] - 0s 253us/step - loss: 0.3493 - mae: 0.3180 - mse: 0.3493 - val_loss: 229.3286 - val_mae: 11.4973 - val_mse: 229.3286\n",
      "Epoch 2299/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3542 - mae: 0.3420 - mse: 0.3542 - val_loss: 223.7292 - val_mae: 11.5102 - val_mse: 223.7292\n",
      "Epoch 2300/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3557 - mae: 0.3526 - mse: 0.3557 - val_loss: 231.5859 - val_mae: 11.5429 - val_mse: 231.5859\n",
      "Epoch 2301/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4024 - mae: 0.3588 - mse: 0.4024 - val_loss: 221.8577 - val_mae: 11.4661 - val_mse: 221.8577\n",
      "Epoch 2302/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4980 - mae: 0.4645 - mse: 0.4980 - val_loss: 227.7052 - val_mae: 11.4825 - val_mse: 227.7052\n",
      "Epoch 2303/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4126 - mae: 0.4102 - mse: 0.4126 - val_loss: 226.5887 - val_mae: 11.4909 - val_mse: 226.5887\n",
      "Epoch 2304/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3148 - mae: 0.3223 - mse: 0.3148 - val_loss: 226.9861 - val_mae: 11.5137 - val_mse: 226.9861\n",
      "Epoch 2305/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3789 - mae: 0.3867 - mse: 0.3789 - val_loss: 226.2363 - val_mae: 11.4547 - val_mse: 226.2363\n",
      "Epoch 2306/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3443 - mae: 0.3560 - mse: 0.3443 - val_loss: 226.7834 - val_mae: 11.5324 - val_mse: 226.7834\n",
      "Epoch 2307/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3960 - mae: 0.3705 - mse: 0.3960 - val_loss: 227.3998 - val_mae: 11.4765 - val_mse: 227.3998\n",
      "Epoch 2308/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3867 - mae: 0.4014 - mse: 0.3867 - val_loss: 223.7524 - val_mae: 11.4377 - val_mse: 223.7524\n",
      "Epoch 2309/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4058 - mae: 0.4155 - mse: 0.4058 - val_loss: 231.7277 - val_mae: 11.5721 - val_mse: 231.7277\n",
      "Epoch 2310/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3490 - mae: 0.3760 - mse: 0.3490 - val_loss: 223.4756 - val_mae: 11.4632 - val_mse: 223.4756\n",
      "Epoch 2311/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3521 - mae: 0.3720 - mse: 0.3521 - val_loss: 227.6618 - val_mae: 11.4238 - val_mse: 227.6618\n",
      "Epoch 2312/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5431 - mae: 0.5484 - mse: 0.5431 - val_loss: 223.2106 - val_mae: 11.4819 - val_mse: 223.2106\n",
      "Epoch 2313/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.8969 - mae: 0.6904 - mse: 0.8969 - val_loss: 232.8328 - val_mae: 11.6046 - val_mse: 232.8328\n",
      "Epoch 2314/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.9917 - mae: 0.7231 - mse: 0.9917 - val_loss: 223.0105 - val_mae: 11.5038 - val_mse: 223.0105\n",
      "Epoch 2315/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.9416 - mae: 0.7207 - mse: 0.9416 - val_loss: 228.7203 - val_mae: 11.4874 - val_mse: 228.7203\n",
      "Epoch 2316/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.2576 - mae: 0.8701 - mse: 1.2576 - val_loss: 222.9257 - val_mae: 11.3628 - val_mse: 222.9257\n",
      "Epoch 2317/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.4442 - mae: 0.9308 - mse: 1.4442 - val_loss: 219.9431 - val_mae: 11.3565 - val_mse: 219.9431\n",
      "Epoch 2318/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 1.3497 - mae: 0.8880 - mse: 1.3497 - val_loss: 237.2797 - val_mae: 11.7177 - val_mse: 237.2797\n",
      "Epoch 2319/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.9639 - mae: 1.0520 - mse: 1.9639 - val_loss: 224.8269 - val_mae: 11.6708 - val_mse: 224.8269\n",
      "Epoch 2320/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 2.6454 - mae: 1.1525 - mse: 2.6454 - val_loss: 240.1253 - val_mae: 11.6689 - val_mse: 240.1253\n",
      "Epoch 2321/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 3.4420 - mae: 1.3414 - mse: 3.4420 - val_loss: 225.2928 - val_mae: 11.6573 - val_mse: 225.2928\n",
      "Epoch 2322/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 2.6679 - mae: 1.3729 - mse: 2.6679 - val_loss: 229.0959 - val_mae: 11.3791 - val_mse: 229.0959\n",
      "Epoch 2323/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 1.8880 - mae: 1.0955 - mse: 1.8880 - val_loss: 226.0822 - val_mae: 11.5421 - val_mse: 226.0822\n",
      "Epoch 2324/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.6889 - mae: 1.0198 - mse: 1.6889 - val_loss: 229.9260 - val_mae: 11.4825 - val_mse: 229.9260\n",
      "Epoch 2325/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.6887 - mae: 0.9370 - mse: 1.6887 - val_loss: 214.2001 - val_mae: 11.2233 - val_mse: 214.2001\n",
      "Epoch 2326/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.3115 - mae: 0.7929 - mse: 1.3115 - val_loss: 223.9329 - val_mae: 11.3958 - val_mse: 223.9329\n",
      "Epoch 2327/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.7323 - mae: 0.6224 - mse: 0.7323 - val_loss: 220.9792 - val_mae: 11.3659 - val_mse: 220.9792\n",
      "Epoch 2328/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.6584 - mae: 0.5671 - mse: 0.6584 - val_loss: 220.9799 - val_mae: 11.4265 - val_mse: 220.9799\n",
      "Epoch 2329/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6137 - mae: 0.5381 - mse: 0.6137 - val_loss: 231.5447 - val_mae: 11.5365 - val_mse: 231.5447\n",
      "Epoch 2330/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5125 - mae: 0.4936 - mse: 0.5125 - val_loss: 222.3798 - val_mae: 11.4304 - val_mse: 222.3798\n",
      "Epoch 2331/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.5454 - mae: 0.4970 - mse: 0.5454 - val_loss: 227.1469 - val_mae: 11.3962 - val_mse: 227.1469\n",
      "Epoch 2332/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4416 - mae: 0.4441 - mse: 0.4416 - val_loss: 224.0709 - val_mae: 11.4711 - val_mse: 224.0709\n",
      "Epoch 2333/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4949 - mae: 0.4831 - mse: 0.4949 - val_loss: 224.0278 - val_mae: 11.4369 - val_mse: 224.0278\n",
      "Epoch 2334/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5516 - mae: 0.4620 - mse: 0.5516 - val_loss: 227.1005 - val_mae: 11.4769 - val_mse: 227.1005\n",
      "Epoch 2335/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5191 - mae: 0.4645 - mse: 0.5191 - val_loss: 220.9039 - val_mae: 11.3655 - val_mse: 220.9039\n",
      "Epoch 2336/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4855 - mae: 0.4670 - mse: 0.4855 - val_loss: 224.2417 - val_mae: 11.3632 - val_mse: 224.2417\n",
      "Epoch 2337/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4554 - mae: 0.4130 - mse: 0.4554 - val_loss: 224.0899 - val_mae: 11.4338 - val_mse: 224.0899\n",
      "Epoch 2338/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3987 - mae: 0.4042 - mse: 0.3987 - val_loss: 224.5186 - val_mae: 11.3745 - val_mse: 224.5186\n",
      "Epoch 2339/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4231 - mae: 0.4143 - mse: 0.4231 - val_loss: 221.4897 - val_mae: 11.3800 - val_mse: 221.4897\n",
      "Epoch 2340/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4952 - mae: 0.4467 - mse: 0.4952 - val_loss: 225.1407 - val_mae: 11.3840 - val_mse: 225.1407\n",
      "Epoch 2341/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5353 - mae: 0.4903 - mse: 0.5353 - val_loss: 223.9597 - val_mae: 11.4212 - val_mse: 223.9597\n",
      "Epoch 2342/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5157 - mae: 0.4926 - mse: 0.5157 - val_loss: 227.9795 - val_mae: 11.4743 - val_mse: 227.9795\n",
      "Epoch 2343/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6503 - mae: 0.5601 - mse: 0.6503 - val_loss: 218.1763 - val_mae: 11.3482 - val_mse: 218.1763\n",
      "Epoch 2344/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6104 - mae: 0.4995 - mse: 0.6104 - val_loss: 231.0633 - val_mae: 11.5062 - val_mse: 231.0633\n",
      "Epoch 2345/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.1635 - mae: 0.3461 - mse: 0.163 - 0s 79us/step - loss: 0.5759 - mae: 0.5296 - mse: 0.5759 - val_loss: 227.8490 - val_mae: 11.5444 - val_mse: 227.8490\n",
      "Epoch 2346/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.6224 - mae: 0.5498 - mse: 0.6224 - val_loss: 221.6331 - val_mae: 11.3583 - val_mse: 221.6331\n",
      "Epoch 2347/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7471 - mae: 0.6151 - mse: 0.7471 - val_loss: 230.2719 - val_mae: 11.4302 - val_mse: 230.2719\n",
      "Epoch 2348/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.7653 - mae: 0.6659 - mse: 0.7653 - val_loss: 214.6371 - val_mae: 11.3303 - val_mse: 214.6371\n",
      "Epoch 2349/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.8112 - mae: 0.7147 - mse: 0.8112 - val_loss: 231.0830 - val_mae: 11.4253 - val_mse: 231.0830\n",
      "Epoch 2350/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.9775 - mae: 0.7512 - mse: 0.9775 - val_loss: 216.3558 - val_mae: 11.3482 - val_mse: 216.3558\n",
      "Epoch 2351/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6230 - mae: 0.5879 - mse: 0.6230 - val_loss: 229.6487 - val_mae: 11.5185 - val_mse: 229.6487\n",
      "Epoch 2352/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.6244 - mae: 0.5514 - mse: 0.6244 - val_loss: 223.0670 - val_mae: 11.3737 - val_mse: 223.0670\n",
      "Epoch 2353/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5247 - mae: 0.5139 - mse: 0.5247 - val_loss: 220.5682 - val_mae: 11.2857 - val_mse: 220.5682\n",
      "Epoch 2354/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5982 - mae: 0.5406 - mse: 0.5982 - val_loss: 223.5748 - val_mae: 11.3425 - val_mse: 223.5748\n",
      "Epoch 2355/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6051 - mae: 0.4411 - mse: 0.6051 - val_loss: 224.8167 - val_mae: 11.4600 - val_mse: 224.8167\n",
      "Epoch 2356/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5582 - mae: 0.5099 - mse: 0.5582 - val_loss: 226.6706 - val_mae: 11.4490 - val_mse: 226.6706\n",
      "Epoch 2357/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.5520 - mae: 0.5449 - mse: 0.5520 - val_loss: 223.2961 - val_mae: 11.4287 - val_mse: 223.2961\n",
      "Epoch 2358/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5140 - mae: 0.4839 - mse: 0.5140 - val_loss: 227.4915 - val_mae: 11.3912 - val_mse: 227.4915\n",
      "Epoch 2359/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4876 - mae: 0.5038 - mse: 0.4876 - val_loss: 224.5539 - val_mae: 11.4868 - val_mse: 224.5539\n",
      "Epoch 2360/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4206 - mae: 0.4212 - mse: 0.4206 - val_loss: 228.7517 - val_mae: 11.5114 - val_mse: 228.7517\n",
      "Epoch 2361/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3426 - mae: 0.3128 - mse: 0.3426 - val_loss: 217.1370 - val_mae: 11.2913 - val_mse: 217.1370\n",
      "Epoch 2362/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4355 - mae: 0.4064 - mse: 0.4355 - val_loss: 225.0067 - val_mae: 11.4312 - val_mse: 225.0067\n",
      "Epoch 2363/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4436 - mae: 0.4118 - mse: 0.4436 - val_loss: 229.6622 - val_mae: 11.5677 - val_mse: 229.6622\n",
      "Epoch 2364/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4252 - mae: 0.3568 - mse: 0.4252 - val_loss: 220.2924 - val_mae: 11.2948 - val_mse: 220.2924\n",
      "Epoch 2365/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5264 - mae: 0.4574 - mse: 0.5264 - val_loss: 225.0443 - val_mae: 11.3967 - val_mse: 225.0443\n",
      "Epoch 2366/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4319 - mae: 0.3748 - mse: 0.4319 - val_loss: 228.0236 - val_mae: 11.5293 - val_mse: 228.0236\n",
      "Epoch 2367/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4055 - mae: 0.3744 - mse: 0.4055 - val_loss: 221.7102 - val_mae: 11.3330 - val_mse: 221.7102\n",
      "Epoch 2368/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4260 - mae: 0.4335 - mse: 0.4260 - val_loss: 221.8751 - val_mae: 11.4157 - val_mse: 221.8751\n",
      "Epoch 2369/3500\n",
      "126/126 [==============================] - 0s 88us/step - loss: 0.4827 - mae: 0.4295 - mse: 0.4827 - val_loss: 232.2442 - val_mae: 11.5432 - val_mse: 232.2442\n",
      "Epoch 2370/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3511 - mae: 0.3386 - mse: 0.3511 - val_loss: 220.1180 - val_mae: 11.2998 - val_mse: 220.1180\n",
      "Epoch 2371/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4094 - mae: 0.4066 - mse: 0.4094 - val_loss: 226.1399 - val_mae: 11.3774 - val_mse: 226.1399\n",
      "Epoch 2372/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3735 - mae: 0.3665 - mse: 0.3735 - val_loss: 225.7189 - val_mae: 11.4446 - val_mse: 225.7189\n",
      "Epoch 2373/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3860 - mae: 0.4016 - mse: 0.3860 - val_loss: 226.1401 - val_mae: 11.3865 - val_mse: 226.1401\n",
      "Epoch 2374/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3890 - mae: 0.4145 - mse: 0.3890 - val_loss: 225.2286 - val_mae: 11.4962 - val_mse: 225.2286\n",
      "Epoch 2375/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4342 - mae: 0.4484 - mse: 0.4342 - val_loss: 225.0789 - val_mae: 11.3760 - val_mse: 225.0789\n",
      "Epoch 2376/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3900 - mae: 0.4130 - mse: 0.3900 - val_loss: 219.6804 - val_mae: 11.3825 - val_mse: 219.6804\n",
      "Epoch 2377/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3583 - mae: 0.3515 - mse: 0.3583 - val_loss: 233.4469 - val_mae: 11.5653 - val_mse: 233.4469\n",
      "Epoch 2378/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4925 - mae: 0.4406 - mse: 0.4925 - val_loss: 226.9089 - val_mae: 11.4731 - val_mse: 226.9089\n",
      "Epoch 2379/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4339 - mae: 0.4238 - mse: 0.4339 - val_loss: 224.2185 - val_mae: 11.3745 - val_mse: 224.2185\n",
      "Epoch 2380/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5415 - mae: 0.4667 - mse: 0.5415 - val_loss: 225.6232 - val_mae: 11.3276 - val_mse: 225.6232\n",
      "Epoch 2381/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5556 - mae: 0.4562 - mse: 0.5556 - val_loss: 221.2155 - val_mae: 11.4130 - val_mse: 221.2155\n",
      "Epoch 2382/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6035 - mae: 0.4944 - mse: 0.6035 - val_loss: 233.4155 - val_mae: 11.5753 - val_mse: 233.4155\n",
      "Epoch 2383/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.8324 - mae: 0.6917 - mse: 0.8324 - val_loss: 224.6691 - val_mae: 11.5300 - val_mse: 224.6691\n",
      "Epoch 2384/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 1.0250 - mae: 0.7723 - mse: 1.0250 - val_loss: 229.2099 - val_mae: 11.4228 - val_mse: 229.2099\n",
      "Epoch 2385/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.8251 - mae: 0.6833 - mse: 0.8251 - val_loss: 216.4182 - val_mae: 11.2966 - val_mse: 216.4182\n",
      "Epoch 2386/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6168 - mae: 0.5403 - mse: 0.6168 - val_loss: 225.6535 - val_mae: 11.3582 - val_mse: 225.6535\n",
      "Epoch 2387/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5174 - mae: 0.4903 - mse: 0.5174 - val_loss: 225.1205 - val_mae: 11.3805 - val_mse: 225.1205\n",
      "Epoch 2388/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4264 - mae: 0.3843 - mse: 0.4264 - val_loss: 225.3150 - val_mae: 11.4728 - val_mse: 225.3150\n",
      "Epoch 2389/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4065 - mae: 0.4133 - mse: 0.4065 - val_loss: 226.1571 - val_mae: 11.3837 - val_mse: 226.1571\n",
      "Epoch 2390/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5117 - mae: 0.4684 - mse: 0.5117 - val_loss: 220.2823 - val_mae: 11.3460 - val_mse: 220.2823\n",
      "Epoch 2391/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4925 - mae: 0.4744 - mse: 0.4925 - val_loss: 226.6890 - val_mae: 11.4028 - val_mse: 226.6890\n",
      "Epoch 2392/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5734 - mae: 0.5122 - mse: 0.5734 - val_loss: 223.9188 - val_mae: 11.4467 - val_mse: 223.9188\n",
      "Epoch 2393/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5504 - mae: 0.5003 - mse: 0.5504 - val_loss: 224.8374 - val_mae: 11.3795 - val_mse: 224.8374\n",
      "Epoch 2394/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6696 - mae: 0.5921 - mse: 0.6696 - val_loss: 217.3882 - val_mae: 11.2664 - val_mse: 217.3882\n",
      "Epoch 2395/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.8407 - mae: 0.6190 - mse: 0.8407 - val_loss: 229.7880 - val_mae: 11.5897 - val_mse: 229.7880\n",
      "Epoch 2396/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.8639 - mae: 0.6365 - mse: 0.8639 - val_loss: 224.5753 - val_mae: 11.4181 - val_mse: 224.5753\n",
      "Epoch 2397/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.7746 - mae: 0.6635 - mse: 0.7746 - val_loss: 222.4948 - val_mae: 11.4515 - val_mse: 222.4948\n",
      "Epoch 2398/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.0117 - mae: 0.7911 - mse: 1.0117 - val_loss: 239.9431 - val_mae: 11.5873 - val_mse: 239.9431\n",
      "Epoch 2399/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.7705 - mae: 1.1790 - mse: 1.770 - 0s 87us/step - loss: 1.2278 - mae: 0.8742 - mse: 1.2278 - val_loss: 214.7979 - val_mae: 11.2671 - val_mse: 214.7979\n",
      "Epoch 2400/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.4338 - mae: 1.0120 - mse: 1.433 - 0s 142us/step - loss: 0.9859 - mae: 0.7455 - mse: 0.9859 - val_loss: 232.3651 - val_mae: 11.5328 - val_mse: 232.3651\n",
      "Epoch 2401/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.2286 - mae: 0.8307 - mse: 1.2286 - val_loss: 223.5070 - val_mae: 11.3367 - val_mse: 223.5070\n",
      "Epoch 2402/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.7195 - mae: 0.6303 - mse: 0.7195 - val_loss: 221.1622 - val_mae: 11.2762 - val_mse: 221.1622\n",
      "Epoch 2403/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.7050 - mae: 0.6264 - mse: 0.7050 - val_loss: 225.5051 - val_mae: 11.3480 - val_mse: 225.5051\n",
      "Epoch 2404/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5428 - mae: 0.5299 - mse: 0.5428 - val_loss: 218.1755 - val_mae: 11.2407 - val_mse: 218.1755\n",
      "Epoch 2405/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5090 - mae: 0.5262 - mse: 0.5090 - val_loss: 228.0026 - val_mae: 11.4929 - val_mse: 228.0026\n",
      "Epoch 2406/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5682 - mae: 0.5270 - mse: 0.5682 - val_loss: 224.4239 - val_mae: 11.4999 - val_mse: 224.4239\n",
      "Epoch 2407/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5440 - mae: 0.5143 - mse: 0.5440 - val_loss: 226.7168 - val_mae: 11.4828 - val_mse: 226.7168\n",
      "Epoch 2408/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4691 - mae: 0.4294 - mse: 0.4691 - val_loss: 225.5887 - val_mae: 11.4381 - val_mse: 225.5887\n",
      "Epoch 2409/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3484 - mae: 0.3417 - mse: 0.3484 - val_loss: 221.5650 - val_mae: 11.3572 - val_mse: 221.5650\n",
      "Epoch 2410/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3284 - mae: 0.3147 - mse: 0.3284 - val_loss: 226.2839 - val_mae: 11.3811 - val_mse: 226.2839\n",
      "Epoch 2411/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3461 - mae: 0.3396 - mse: 0.3461 - val_loss: 228.2051 - val_mae: 11.4510 - val_mse: 228.2051\n",
      "Epoch 2412/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4798 - mae: 0.4339 - mse: 0.4798 - val_loss: 223.8706 - val_mae: 11.4362 - val_mse: 223.8706\n",
      "Epoch 2413/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4133 - mae: 0.3963 - mse: 0.4133 - val_loss: 227.0545 - val_mae: 11.4060 - val_mse: 227.0545\n",
      "Epoch 2414/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3368 - mae: 0.3358 - mse: 0.3368 - val_loss: 224.6779 - val_mae: 11.5121 - val_mse: 224.6779\n",
      "Epoch 2415/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4232 - mae: 0.4212 - mse: 0.4232 - val_loss: 227.0730 - val_mae: 11.4289 - val_mse: 227.0730\n",
      "Epoch 2416/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3844 - mae: 0.3728 - mse: 0.3844 - val_loss: 225.9185 - val_mae: 11.4778 - val_mse: 225.9185\n",
      "Epoch 2417/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3685 - mae: 0.3623 - mse: 0.3685 - val_loss: 224.6244 - val_mae: 11.3932 - val_mse: 224.6244\n",
      "Epoch 2418/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3761 - mae: 0.3477 - mse: 0.3761 - val_loss: 225.5782 - val_mae: 11.3639 - val_mse: 225.5782\n",
      "Epoch 2419/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.2019 - mae: 0.3184 - mse: 0.201 - 0s 103us/step - loss: 0.3475 - mae: 0.3323 - mse: 0.3475 - val_loss: 224.5726 - val_mae: 11.4610 - val_mse: 224.5726\n",
      "Epoch 2420/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3055 - mae: 0.3130 - mse: 0.3055 - val_loss: 225.6913 - val_mae: 11.4060 - val_mse: 225.6913\n",
      "Epoch 2421/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2923 - mae: 0.2792 - mse: 0.2923 - val_loss: 223.9711 - val_mae: 11.4226 - val_mse: 223.9711\n",
      "Epoch 2422/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2844 - mae: 0.2988 - mse: 0.2844 - val_loss: 225.9923 - val_mae: 11.4343 - val_mse: 225.9923\n",
      "Epoch 2423/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.2708 - mae: 0.2864 - mse: 0.2708 - val_loss: 224.5757 - val_mae: 11.4261 - val_mse: 224.5757\n",
      "Epoch 2424/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2589 - mae: 0.2437 - mse: 0.2589 - val_loss: 227.1264 - val_mae: 11.4575 - val_mse: 227.1264\n",
      "Epoch 2425/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2481 - mae: 0.2264 - mse: 0.2481 - val_loss: 223.5204 - val_mae: 11.4069 - val_mse: 223.5204\n",
      "Epoch 2426/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2482 - mae: 0.2031 - mse: 0.2482 - val_loss: 225.8075 - val_mae: 11.4074 - val_mse: 225.8075\n",
      "Epoch 2427/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2567 - mae: 0.2116 - mse: 0.2567 - val_loss: 224.3818 - val_mae: 11.4143 - val_mse: 224.3818\n",
      "Epoch 2428/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2304 - mae: 0.1852 - mse: 0.2304 - val_loss: 225.2585 - val_mae: 11.4424 - val_mse: 225.2585\n",
      "Epoch 2429/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2833 - mae: 0.2115 - mse: 0.2833 - val_loss: 225.7067 - val_mae: 11.4066 - val_mse: 225.7067\n",
      "Epoch 2430/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3052 - mae: 0.2864 - mse: 0.3052 - val_loss: 223.1007 - val_mae: 11.4176 - val_mse: 223.1007\n",
      "Epoch 2431/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3408 - mae: 0.3333 - mse: 0.3408 - val_loss: 228.3694 - val_mae: 11.4262 - val_mse: 228.3694\n",
      "Epoch 2432/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3803 - mae: 0.3591 - mse: 0.3803 - val_loss: 221.7625 - val_mae: 11.3992 - val_mse: 221.7625\n",
      "Epoch 2433/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6244 - mae: 0.4903 - mse: 0.6244 - val_loss: 229.6116 - val_mae: 11.4929 - val_mse: 229.6116\n",
      "Epoch 2434/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5348 - mae: 0.5145 - mse: 0.5348 - val_loss: 224.9594 - val_mae: 11.4545 - val_mse: 224.9594\n",
      "Epoch 2435/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5136 - mae: 0.4748 - mse: 0.5136 - val_loss: 223.3604 - val_mae: 11.4062 - val_mse: 223.3604\n",
      "Epoch 2436/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4488 - mae: 0.4817 - mse: 0.4488 - val_loss: 224.9586 - val_mae: 11.3980 - val_mse: 224.9586\n",
      "Epoch 2437/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5787 - mae: 0.4979 - mse: 0.5787 - val_loss: 230.2914 - val_mae: 11.5125 - val_mse: 230.2914\n",
      "Epoch 2438/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.7392 - mae: 0.5434 - mse: 0.7392 - val_loss: 224.1583 - val_mae: 11.3756 - val_mse: 224.1583\n",
      "Epoch 2439/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6440 - mae: 0.5591 - mse: 0.6440 - val_loss: 225.1021 - val_mae: 11.4438 - val_mse: 225.1021\n",
      "Epoch 2440/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5203 - mae: 0.5160 - mse: 0.5203 - val_loss: 225.3986 - val_mae: 11.3440 - val_mse: 225.3986\n",
      "Epoch 2441/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5034 - mae: 0.5029 - mse: 0.5034 - val_loss: 221.9983 - val_mae: 11.4651 - val_mse: 221.9983\n",
      "Epoch 2442/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4764 - mae: 0.4744 - mse: 0.4764 - val_loss: 234.0027 - val_mae: 11.5978 - val_mse: 234.0027\n",
      "Epoch 2443/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4136 - mae: 0.4129 - mse: 0.4136 - val_loss: 223.1244 - val_mae: 11.3851 - val_mse: 223.1244\n",
      "Epoch 2444/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3326 - mae: 0.3142 - mse: 0.3326 - val_loss: 225.7511 - val_mae: 11.4668 - val_mse: 225.7511\n",
      "Epoch 2445/3500\n",
      "126/126 [==============================] - 0s 198us/step - loss: 0.3887 - mae: 0.3926 - mse: 0.3887 - val_loss: 222.4600 - val_mae: 11.3766 - val_mse: 222.4600\n",
      "Epoch 2446/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3324 - mae: 0.3513 - mse: 0.3324 - val_loss: 226.5768 - val_mae: 11.4301 - val_mse: 226.5768\n",
      "Epoch 2447/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3961 - mae: 0.3841 - mse: 0.3961 - val_loss: 227.0874 - val_mae: 11.4876 - val_mse: 227.0874\n",
      "Epoch 2448/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.5927 - mae: 0.4997 - mse: 0.592 - 0s 79us/step - loss: 0.3803 - mae: 0.3363 - mse: 0.3803 - val_loss: 224.1034 - val_mae: 11.3941 - val_mse: 224.1034\n",
      "Epoch 2449/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.2019 - mae: 0.3736 - mse: 0.201 - 0s 79us/step - loss: 0.3453 - mae: 0.3318 - mse: 0.3453 - val_loss: 227.7677 - val_mae: 11.4559 - val_mse: 227.7677\n",
      "Epoch 2450/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4618 - mae: 0.4454 - mse: 0.4618 - val_loss: 221.9276 - val_mae: 11.3792 - val_mse: 221.9276\n",
      "Epoch 2451/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3925 - mae: 0.3610 - mse: 0.3925 - val_loss: 229.2269 - val_mae: 11.5101 - val_mse: 229.2269\n",
      "Epoch 2452/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3351 - mae: 0.3511 - mse: 0.3351 - val_loss: 226.8220 - val_mae: 11.4594 - val_mse: 226.8220\n",
      "Epoch 2453/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3617 - mae: 0.3364 - mse: 0.3617 - val_loss: 220.3202 - val_mae: 11.2823 - val_mse: 220.3202\n",
      "Epoch 2454/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3849 - mae: 0.3394 - mse: 0.3849 - val_loss: 226.7884 - val_mae: 11.4525 - val_mse: 226.7884\n",
      "Epoch 2455/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3775 - mae: 0.3641 - mse: 0.3775 - val_loss: 223.6210 - val_mae: 11.3753 - val_mse: 223.6210\n",
      "Epoch 2456/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3507 - mae: 0.3449 - mse: 0.3507 - val_loss: 225.6177 - val_mae: 11.4459 - val_mse: 225.6177\n",
      "Epoch 2457/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3592 - mae: 0.3151 - mse: 0.3592 - val_loss: 227.5180 - val_mae: 11.4858 - val_mse: 227.5180\n",
      "Epoch 2458/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3851 - mae: 0.3592 - mse: 0.3851 - val_loss: 222.7022 - val_mae: 11.3507 - val_mse: 222.7022\n",
      "Epoch 2459/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3713 - mae: 0.3346 - mse: 0.3713 - val_loss: 224.4053 - val_mae: 11.3936 - val_mse: 224.4053\n",
      "Epoch 2460/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3371 - mae: 0.3411 - mse: 0.3371 - val_loss: 227.0689 - val_mae: 11.4259 - val_mse: 227.0689\n",
      "Epoch 2461/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3233 - mae: 0.3117 - mse: 0.3233 - val_loss: 225.3203 - val_mae: 11.4714 - val_mse: 225.3203\n",
      "Epoch 2462/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3558 - mae: 0.3423 - mse: 0.3558 - val_loss: 228.7570 - val_mae: 11.5153 - val_mse: 228.7570\n",
      "Epoch 2463/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3813 - mae: 0.3632 - mse: 0.3813 - val_loss: 226.4667 - val_mae: 11.3991 - val_mse: 226.4667\n",
      "Epoch 2464/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3605 - mae: 0.3664 - mse: 0.3605 - val_loss: 225.0365 - val_mae: 11.4597 - val_mse: 225.0365\n",
      "Epoch 2465/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4345 - mae: 0.4464 - mse: 0.4345 - val_loss: 226.7092 - val_mae: 11.3778 - val_mse: 226.7092\n",
      "Epoch 2466/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5147 - mae: 0.4748 - mse: 0.5147 - val_loss: 226.2424 - val_mae: 11.4607 - val_mse: 226.2424\n",
      "Epoch 2467/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4060 - mae: 0.3887 - mse: 0.4060 - val_loss: 229.1433 - val_mae: 11.5242 - val_mse: 229.1433\n",
      "Epoch 2468/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.3186 - mae: 0.3467 - mse: 0.3186 - val_loss: 222.7823 - val_mae: 11.3647 - val_mse: 222.7823\n",
      "Epoch 2469/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3571 - mae: 0.3566 - mse: 0.3571 - val_loss: 226.8270 - val_mae: 11.4320 - val_mse: 226.8270\n",
      "Epoch 2470/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4011 - mae: 0.4205 - mse: 0.4011 - val_loss: 223.7728 - val_mae: 11.3885 - val_mse: 223.7728\n",
      "Epoch 2471/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3625 - mae: 0.3764 - mse: 0.3625 - val_loss: 225.5700 - val_mae: 11.3947 - val_mse: 225.5700\n",
      "Epoch 2472/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3354 - mae: 0.3421 - mse: 0.3354 - val_loss: 229.2599 - val_mae: 11.5233 - val_mse: 229.2599\n",
      "Epoch 2473/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2992 - mae: 0.3032 - mse: 0.2992 - val_loss: 222.8631 - val_mae: 11.3486 - val_mse: 222.8631\n",
      "Epoch 2474/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3043 - mae: 0.3340 - mse: 0.3043 - val_loss: 228.9409 - val_mae: 11.4728 - val_mse: 228.9409\n",
      "Epoch 2475/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3512 - mae: 0.3325 - mse: 0.3512 - val_loss: 225.1162 - val_mae: 11.4151 - val_mse: 225.1162\n",
      "Epoch 2476/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3310 - mae: 0.3366 - mse: 0.3310 - val_loss: 222.2899 - val_mae: 11.3285 - val_mse: 222.2899\n",
      "Epoch 2477/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.2907 - mae: 0.2876 - mse: 0.2907 - val_loss: 228.2999 - val_mae: 11.4840 - val_mse: 228.2999\n",
      "Epoch 2478/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.2983 - mae: 0.2887 - mse: 0.2983 - val_loss: 225.1494 - val_mae: 11.4345 - val_mse: 225.1494\n",
      "Epoch 2479/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3158 - mae: 0.2975 - mse: 0.3158 - val_loss: 227.9513 - val_mae: 11.4387 - val_mse: 227.9513\n",
      "Epoch 2480/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3125 - mae: 0.3108 - mse: 0.3125 - val_loss: 222.8812 - val_mae: 11.3496 - val_mse: 222.8812\n",
      "Epoch 2481/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3381 - mae: 0.3288 - mse: 0.3381 - val_loss: 224.9442 - val_mae: 11.3623 - val_mse: 224.9442\n",
      "Epoch 2482/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3175 - mae: 0.2969 - mse: 0.3175 - val_loss: 225.5893 - val_mae: 11.4726 - val_mse: 225.5893\n",
      "Epoch 2483/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2697 - mae: 0.2643 - mse: 0.2697 - val_loss: 226.1456 - val_mae: 11.4220 - val_mse: 226.1456\n",
      "Epoch 2484/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3953 - mae: 0.3642 - mse: 0.3953 - val_loss: 227.5496 - val_mae: 11.4605 - val_mse: 227.5496\n",
      "Epoch 2485/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3665 - mae: 0.3710 - mse: 0.3665 - val_loss: 223.9633 - val_mae: 11.4799 - val_mse: 223.9633\n",
      "Epoch 2486/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3796 - mae: 0.4026 - mse: 0.3796 - val_loss: 228.6061 - val_mae: 11.4143 - val_mse: 228.6061\n",
      "Epoch 2487/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3994 - mae: 0.4034 - mse: 0.3994 - val_loss: 223.7898 - val_mae: 11.4288 - val_mse: 223.7898\n",
      "Epoch 2488/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3197 - mae: 0.3296 - mse: 0.3197 - val_loss: 226.3136 - val_mae: 11.4030 - val_mse: 226.3136\n",
      "Epoch 2489/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3323 - mae: 0.3146 - mse: 0.3323 - val_loss: 226.3719 - val_mae: 11.4601 - val_mse: 226.3719\n",
      "Epoch 2490/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3870 - mae: 0.3421 - mse: 0.3870 - val_loss: 223.1369 - val_mae: 11.3734 - val_mse: 223.1369\n",
      "Epoch 2491/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3499 - mae: 0.3611 - mse: 0.3499 - val_loss: 227.2073 - val_mae: 11.3732 - val_mse: 227.2073\n",
      "Epoch 2492/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.3439 - mae: 0.3635 - mse: 0.3439 - val_loss: 223.1601 - val_mae: 11.4345 - val_mse: 223.1601\n",
      "Epoch 2493/3500\n",
      "126/126 [==============================] - 0s 190us/step - loss: 0.4183 - mae: 0.4337 - mse: 0.4183 - val_loss: 227.2000 - val_mae: 11.4010 - val_mse: 227.2000\n",
      "Epoch 2494/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4497 - mae: 0.4297 - mse: 0.4497 - val_loss: 226.0449 - val_mae: 11.3955 - val_mse: 226.0449\n",
      "Epoch 2495/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3928 - mae: 0.4172 - mse: 0.3928 - val_loss: 223.4448 - val_mae: 11.4029 - val_mse: 223.4448\n",
      "Epoch 2496/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4305 - mae: 0.4380 - mse: 0.4305 - val_loss: 227.6064 - val_mae: 11.3991 - val_mse: 227.6064\n",
      "Epoch 2497/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3708 - mae: 0.3924 - mse: 0.3708 - val_loss: 220.0567 - val_mae: 11.3358 - val_mse: 220.0567\n",
      "Epoch 2498/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3761 - mae: 0.3723 - mse: 0.3761 - val_loss: 233.2121 - val_mae: 11.5220 - val_mse: 233.2121\n",
      "Epoch 2499/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3929 - mae: 0.4289 - mse: 0.3929 - val_loss: 226.0082 - val_mae: 11.4296 - val_mse: 226.0082\n",
      "Epoch 2500/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3334 - mae: 0.3509 - mse: 0.3334 - val_loss: 224.0771 - val_mae: 11.3658 - val_mse: 224.0771\n",
      "Epoch 2501/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3230 - mae: 0.2949 - mse: 0.3230 - val_loss: 226.5796 - val_mae: 11.4059 - val_mse: 226.5796\n",
      "Epoch 2502/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3099 - mae: 0.2922 - mse: 0.3099 - val_loss: 225.0740 - val_mae: 11.3893 - val_mse: 225.0740\n",
      "Epoch 2503/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2840 - mae: 0.2822 - mse: 0.2840 - val_loss: 227.1759 - val_mae: 11.4236 - val_mse: 227.1759\n",
      "Epoch 2504/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2801 - mae: 0.2861 - mse: 0.2801 - val_loss: 225.5224 - val_mae: 11.4513 - val_mse: 225.5224\n",
      "Epoch 2505/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2618 - mae: 0.2566 - mse: 0.2618 - val_loss: 226.7390 - val_mae: 11.4349 - val_mse: 226.7390\n",
      "Epoch 2506/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.0351 - mae: 0.1449 - mse: 0.035 - 0s 63us/step - loss: 0.2577 - mae: 0.2303 - mse: 0.2577 - val_loss: 225.1521 - val_mae: 11.4372 - val_mse: 225.1521\n",
      "Epoch 2507/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2667 - mae: 0.2504 - mse: 0.2667 - val_loss: 225.4934 - val_mae: 11.4053 - val_mse: 225.4934\n",
      "Epoch 2508/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3148 - mae: 0.2881 - mse: 0.3148 - val_loss: 226.7325 - val_mae: 11.4426 - val_mse: 226.7325\n",
      "Epoch 2509/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3078 - mae: 0.2790 - mse: 0.3078 - val_loss: 224.2365 - val_mae: 11.3993 - val_mse: 224.2365\n",
      "Epoch 2510/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.2874 - mae: 0.2818 - mse: 0.2874 - val_loss: 225.4504 - val_mae: 11.3922 - val_mse: 225.4504\n",
      "Epoch 2511/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3118 - mae: 0.3008 - mse: 0.3118 - val_loss: 229.0379 - val_mae: 11.5193 - val_mse: 229.0379\n",
      "Epoch 2512/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2959 - mae: 0.2979 - mse: 0.2959 - val_loss: 222.5013 - val_mae: 11.3556 - val_mse: 222.5013\n",
      "Epoch 2513/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3859 - mae: 0.3682 - mse: 0.3859 - val_loss: 229.2624 - val_mae: 11.4542 - val_mse: 229.2624\n",
      "Epoch 2514/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5075 - mae: 0.5004 - mse: 0.5075 - val_loss: 224.3560 - val_mae: 11.4537 - val_mse: 224.3560\n",
      "Epoch 2515/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4718 - mae: 0.4667 - mse: 0.4718 - val_loss: 226.8992 - val_mae: 11.3910 - val_mse: 226.8992\n",
      "Epoch 2516/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6388 - mae: 0.5900 - mse: 0.6388 - val_loss: 223.8976 - val_mae: 11.4825 - val_mse: 223.8976\n",
      "Epoch 2517/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5003 - mae: 0.5003 - mse: 0.5003 - val_loss: 225.9441 - val_mae: 11.4321 - val_mse: 225.9441\n",
      "Epoch 2518/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6023 - mae: 0.5566 - mse: 0.6023 - val_loss: 221.8416 - val_mae: 11.3541 - val_mse: 221.8416\n",
      "Epoch 2519/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6202 - mae: 0.5692 - mse: 0.6202 - val_loss: 226.8721 - val_mae: 11.4437 - val_mse: 226.8721\n",
      "Epoch 2520/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6294 - mae: 0.5754 - mse: 0.6294 - val_loss: 227.9719 - val_mae: 11.3977 - val_mse: 227.9719\n",
      "Epoch 2521/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6583 - mae: 0.5960 - mse: 0.6583 - val_loss: 223.8704 - val_mae: 11.4737 - val_mse: 223.8704\n",
      "Epoch 2522/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6021 - mae: 0.5665 - mse: 0.6021 - val_loss: 239.0813 - val_mae: 11.6488 - val_mse: 239.0813\n",
      "Epoch 2523/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5486 - mae: 0.5065 - mse: 0.5486 - val_loss: 225.5735 - val_mae: 11.4014 - val_mse: 225.5735\n",
      "Epoch 2524/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5744 - mae: 0.5209 - mse: 0.5744 - val_loss: 226.1867 - val_mae: 11.3700 - val_mse: 226.1867\n",
      "Epoch 2525/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6057 - mae: 0.5234 - mse: 0.6057 - val_loss: 233.1496 - val_mae: 11.5248 - val_mse: 233.1496\n",
      "Epoch 2526/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.7235 - mae: 0.6054 - mse: 0.7235 - val_loss: 221.1415 - val_mae: 11.3903 - val_mse: 221.1415\n",
      "Epoch 2527/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.9294 - mae: 0.7130 - mse: 0.9294 - val_loss: 233.3213 - val_mae: 11.4619 - val_mse: 233.3213\n",
      "Epoch 2528/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.7773 - mae: 0.6991 - mse: 0.7773 - val_loss: 215.5893 - val_mae: 11.2138 - val_mse: 215.5893\n",
      "Epoch 2529/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.7541 - mae: 0.6427 - mse: 0.7541 - val_loss: 228.6350 - val_mae: 11.4323 - val_mse: 228.6350\n",
      "Epoch 2530/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6353 - mae: 0.5609 - mse: 0.6353 - val_loss: 230.3925 - val_mae: 11.6010 - val_mse: 230.3925\n",
      "Epoch 2531/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5569 - mae: 0.5012 - mse: 0.5569 - val_loss: 222.7085 - val_mae: 11.3518 - val_mse: 222.7085\n",
      "Epoch 2532/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.5098 - mae: 0.4886 - mse: 0.5098 - val_loss: 223.4941 - val_mae: 11.3436 - val_mse: 223.4941\n",
      "Epoch 2533/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5925 - mae: 0.5599 - mse: 0.5925 - val_loss: 220.2868 - val_mae: 11.3461 - val_mse: 220.2868\n",
      "Epoch 2534/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5874 - mae: 0.5042 - mse: 0.5874 - val_loss: 229.4725 - val_mae: 11.4355 - val_mse: 229.4725\n",
      "Epoch 2535/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4837 - mae: 0.4603 - mse: 0.4837 - val_loss: 226.6408 - val_mae: 11.5370 - val_mse: 226.6408\n",
      "Epoch 2536/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4365 - mae: 0.4227 - mse: 0.4365 - val_loss: 230.5931 - val_mae: 11.4655 - val_mse: 230.5931\n",
      "Epoch 2537/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4762 - mae: 0.4601 - mse: 0.4762 - val_loss: 223.4447 - val_mae: 11.4057 - val_mse: 223.4447\n",
      "Epoch 2538/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4790 - mae: 0.4912 - mse: 0.4790 - val_loss: 230.9142 - val_mae: 11.4714 - val_mse: 230.9142\n",
      "Epoch 2539/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4215 - mae: 0.4313 - mse: 0.4215 - val_loss: 226.0028 - val_mae: 11.4123 - val_mse: 226.0028\n",
      "Epoch 2540/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3600 - mae: 0.3963 - mse: 0.3600 - val_loss: 226.2638 - val_mae: 11.3842 - val_mse: 226.2638\n",
      "Epoch 2541/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4587 - mae: 0.4769 - mse: 0.4587 - val_loss: 226.8332 - val_mae: 11.4573 - val_mse: 226.8332\n",
      "Epoch 2542/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.4960 - mae: 0.4758 - mse: 0.4960 - val_loss: 224.1008 - val_mae: 11.4535 - val_mse: 224.1008\n",
      "Epoch 2543/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.5327 - mae: 0.4889 - mse: 0.5327 - val_loss: 230.2590 - val_mae: 11.4855 - val_mse: 230.2590\n",
      "Epoch 2544/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.7268 - mae: 0.5845 - mse: 0.7268 - val_loss: 225.6545 - val_mae: 11.4391 - val_mse: 225.6545\n",
      "Epoch 2545/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5484 - mae: 0.5304 - mse: 0.5484 - val_loss: 229.7376 - val_mae: 11.4669 - val_mse: 229.7376\n",
      "Epoch 2546/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4334 - mae: 0.4340 - mse: 0.4334 - val_loss: 230.6376 - val_mae: 11.5034 - val_mse: 230.6376\n",
      "Epoch 2547/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3577 - mae: 0.3580 - mse: 0.3577 - val_loss: 227.7425 - val_mae: 11.5319 - val_mse: 227.7425\n",
      "Epoch 2548/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3626 - mae: 0.3575 - mse: 0.3626 - val_loss: 228.8490 - val_mae: 11.5061 - val_mse: 228.8490\n",
      "Epoch 2549/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5061 - mae: 0.4623 - mse: 0.5061 - val_loss: 227.3958 - val_mae: 11.4886 - val_mse: 227.3958\n",
      "Epoch 2550/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6047 - mae: 0.5197 - mse: 0.6047 - val_loss: 223.1075 - val_mae: 11.3816 - val_mse: 223.1075\n",
      "Epoch 2551/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5149 - mae: 0.4747 - mse: 0.5149 - val_loss: 231.2099 - val_mae: 11.4479 - val_mse: 231.2099\n",
      "Epoch 2552/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4242 - mae: 0.4270 - mse: 0.4242 - val_loss: 222.9909 - val_mae: 11.3837 - val_mse: 222.9909\n",
      "Epoch 2553/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4730 - mae: 0.4479 - mse: 0.4730 - val_loss: 228.6763 - val_mae: 11.3988 - val_mse: 228.6763\n",
      "Epoch 2554/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4449 - mae: 0.4420 - mse: 0.4449 - val_loss: 227.9050 - val_mae: 11.4882 - val_mse: 227.9050\n",
      "Epoch 2555/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5610 - mae: 0.5748 - mse: 0.5610 - val_loss: 229.4099 - val_mae: 11.4736 - val_mse: 229.4099\n",
      "Epoch 2556/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5562 - mae: 0.5005 - mse: 0.5562 - val_loss: 226.8619 - val_mae: 11.4898 - val_mse: 226.8619\n",
      "Epoch 2557/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5001 - mae: 0.4767 - mse: 0.5001 - val_loss: 229.3009 - val_mae: 11.4759 - val_mse: 229.3009\n",
      "Epoch 2558/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.5063 - mae: 0.4797 - mse: 0.5063 - val_loss: 228.7644 - val_mae: 11.4371 - val_mse: 228.7644\n",
      "Epoch 2559/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5034 - mae: 0.4735 - mse: 0.5034 - val_loss: 224.2139 - val_mae: 11.3789 - val_mse: 224.2139\n",
      "Epoch 2560/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.3748 - mae: 0.3816 - mse: 0.3748 - val_loss: 230.0034 - val_mae: 11.4306 - val_mse: 230.0034\n",
      "Epoch 2561/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3167 - mae: 0.3354 - mse: 0.3167 - val_loss: 227.9146 - val_mae: 11.5026 - val_mse: 227.9146\n",
      "Epoch 2562/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3504 - mae: 0.3706 - mse: 0.3504 - val_loss: 226.7925 - val_mae: 11.3669 - val_mse: 226.7925\n",
      "Epoch 2563/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4259 - mae: 0.4235 - mse: 0.4259 - val_loss: 226.8184 - val_mae: 11.4538 - val_mse: 226.8184\n",
      "Epoch 2564/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3790 - mae: 0.4006 - mse: 0.3790 - val_loss: 228.4901 - val_mae: 11.4413 - val_mse: 228.4901\n",
      "Epoch 2565/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4388 - mae: 0.4391 - mse: 0.4388 - val_loss: 222.1131 - val_mae: 11.3381 - val_mse: 222.1131\n",
      "Epoch 2566/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4414 - mae: 0.4447 - mse: 0.4414 - val_loss: 232.3503 - val_mae: 11.5746 - val_mse: 232.3503\n",
      "Epoch 2567/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4264 - mae: 0.4396 - mse: 0.4264 - val_loss: 221.9037 - val_mae: 11.3358 - val_mse: 221.9037\n",
      "Epoch 2568/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4217 - mae: 0.4039 - mse: 0.4217 - val_loss: 228.8861 - val_mae: 11.4489 - val_mse: 228.8861\n",
      "Epoch 2569/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.6542 - mae: 0.5632 - mse: 0.6542 - val_loss: 229.6197 - val_mae: 11.4765 - val_mse: 229.6197\n",
      "Epoch 2570/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5546 - mae: 0.5162 - mse: 0.5546 - val_loss: 223.8996 - val_mae: 11.3961 - val_mse: 223.8996\n",
      "Epoch 2571/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4442 - mae: 0.4542 - mse: 0.4442 - val_loss: 231.6071 - val_mae: 11.4670 - val_mse: 231.6071\n",
      "Epoch 2572/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4535 - mae: 0.4407 - mse: 0.4535 - val_loss: 217.8253 - val_mae: 11.2662 - val_mse: 217.8253\n",
      "Epoch 2573/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4871 - mae: 0.4667 - mse: 0.4871 - val_loss: 229.1838 - val_mae: 11.4456 - val_mse: 229.1838\n",
      "Epoch 2574/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4568 - mae: 0.4174 - mse: 0.4568 - val_loss: 226.5305 - val_mae: 11.4552 - val_mse: 226.5305\n",
      "Epoch 2575/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5693 - mae: 0.4597 - mse: 0.5693 - val_loss: 226.8380 - val_mae: 11.4405 - val_mse: 226.8380\n",
      "Epoch 2576/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.4661 - mae: 0.4474 - mse: 0.4661 - val_loss: 232.1638 - val_mae: 11.4911 - val_mse: 232.1638\n",
      "Epoch 2577/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.7616 - mae: 0.5789 - mse: 0.7616 - val_loss: 219.1548 - val_mae: 11.3214 - val_mse: 219.1548\n",
      "Epoch 2578/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7823 - mae: 0.6240 - mse: 0.7823 - val_loss: 231.8371 - val_mae: 11.4426 - val_mse: 231.8371\n",
      "Epoch 2579/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.9269 - mae: 0.7233 - mse: 0.9269 - val_loss: 224.2958 - val_mae: 11.5274 - val_mse: 224.2958\n",
      "Epoch 2580/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.7935 - mae: 0.6965 - mse: 0.7935 - val_loss: 229.1921 - val_mae: 11.4732 - val_mse: 229.1921\n",
      "Epoch 2581/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.8182 - mae: 0.6499 - mse: 0.8182 - val_loss: 230.0306 - val_mae: 11.4506 - val_mse: 230.0306\n",
      "Epoch 2582/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5502 - mae: 0.5320 - mse: 0.5502 - val_loss: 217.1002 - val_mae: 11.2165 - val_mse: 217.1002\n",
      "Epoch 2583/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6693 - mae: 0.5588 - mse: 0.6693 - val_loss: 229.5191 - val_mae: 11.4380 - val_mse: 229.5191\n",
      "Epoch 2584/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.8836 - mae: 0.6431 - mse: 0.8836 - val_loss: 222.1158 - val_mae: 11.4231 - val_mse: 222.1158\n",
      "Epoch 2585/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.0996 - mae: 0.7400 - mse: 1.0996 - val_loss: 235.1228 - val_mae: 11.5700 - val_mse: 235.1228\n",
      "Epoch 2586/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 1.5516 - mae: 0.9861 - mse: 1.5516 - val_loss: 232.3481 - val_mae: 11.6016 - val_mse: 232.3481\n",
      "Epoch 2587/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.1996 - mae: 0.8323 - mse: 1.1996 - val_loss: 219.8882 - val_mae: 11.2481 - val_mse: 219.8882\n",
      "Epoch 2588/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.0205 - mae: 0.7910 - mse: 1.0205 - val_loss: 233.1751 - val_mae: 11.4181 - val_mse: 233.1751\n",
      "Epoch 2589/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.4554 - mae: 0.8630 - mse: 1.4554 - val_loss: 221.5905 - val_mae: 11.4966 - val_mse: 221.5905\n",
      "Epoch 2590/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 2.2383 - mae: 1.1716 - mse: 2.2383 - val_loss: 233.4740 - val_mae: 11.5584 - val_mse: 233.4740\n",
      "Epoch 2591/3500\n",
      "126/126 [==============================] - 0s 150us/step - loss: 2.1094 - mae: 1.1360 - mse: 2.1094 - val_loss: 233.7678 - val_mae: 11.6927 - val_mse: 233.7678\n",
      "Epoch 2592/3500\n",
      "126/126 [==============================] - 0s 134us/step - loss: 1.5091 - mae: 0.9329 - mse: 1.5091 - val_loss: 225.0865 - val_mae: 11.3395 - val_mse: 225.0865\n",
      "Epoch 2593/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.4177 - mae: 0.8365 - mse: 1.4177 - val_loss: 233.0723 - val_mae: 11.4343 - val_mse: 233.0723\n",
      "Epoch 2594/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 2.0759 - mae: 1.0300 - mse: 2.0759 - val_loss: 230.5601 - val_mae: 11.6355 - val_mse: 230.5601\n",
      "Epoch 2595/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.9063 - mae: 1.0570 - mse: 1.9063 - val_loss: 235.2296 - val_mae: 11.6389 - val_mse: 235.2296\n",
      "Epoch 2596/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 2.0370 - mae: 1.1468 - mse: 2.0370 - val_loss: 229.4291 - val_mae: 11.7372 - val_mse: 229.4291\n",
      "Epoch 2597/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.7642 - mae: 1.0547 - mse: 1.7642 - val_loss: 223.2779 - val_mae: 11.2738 - val_mse: 223.2779\n",
      "Epoch 2598/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 1.5934 - mae: 0.9777 - mse: 1.5934 - val_loss: 220.9323 - val_mae: 11.4689 - val_mse: 220.9323\n",
      "Epoch 2599/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.1520 - mae: 0.8201 - mse: 1.1520 - val_loss: 233.0987 - val_mae: 11.6666 - val_mse: 233.0987\n",
      "Epoch 2600/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.0355 - mae: 0.7750 - mse: 1.0355 - val_loss: 223.2426 - val_mae: 11.3691 - val_mse: 223.2426\n",
      "Epoch 2601/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.9017 - mae: 0.7177 - mse: 0.9017 - val_loss: 221.1248 - val_mae: 11.4249 - val_mse: 221.1248\n",
      "Epoch 2602/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.8046 - mae: 0.6562 - mse: 0.8046 - val_loss: 235.0391 - val_mae: 11.5295 - val_mse: 235.0391\n",
      "Epoch 2603/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.8439 - mae: 0.6467 - mse: 0.8439 - val_loss: 219.9052 - val_mae: 11.3126 - val_mse: 219.9052\n",
      "Epoch 2604/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5640 - mae: 0.5349 - mse: 0.5640 - val_loss: 231.7133 - val_mae: 11.5576 - val_mse: 231.7133\n",
      "Epoch 2605/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6009 - mae: 0.5487 - mse: 0.6009 - val_loss: 224.0540 - val_mae: 11.3839 - val_mse: 224.0540\n",
      "Epoch 2606/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.7550 - mae: 0.6263 - mse: 0.7550 - val_loss: 224.2331 - val_mae: 11.4563 - val_mse: 224.2331\n",
      "Epoch 2607/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.0064 - mae: 0.7597 - mse: 1.0064 - val_loss: 235.8544 - val_mae: 11.6127 - val_mse: 235.8544\n",
      "Epoch 2608/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.8594 - mae: 0.7897 - mse: 0.859 - 0s 71us/step - loss: 0.8238 - mae: 0.7272 - mse: 0.8238 - val_loss: 221.1020 - val_mae: 11.4196 - val_mse: 221.1020\n",
      "Epoch 2609/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 1.0593 - mae: 0.7851 - mse: 1.0593 - val_loss: 239.3517 - val_mae: 11.7083 - val_mse: 239.3517\n",
      "Epoch 2610/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 1.1720 - mae: 0.8307 - mse: 1.1720 - val_loss: 218.3512 - val_mae: 11.3436 - val_mse: 218.3512\n",
      "Epoch 2611/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.0717 - mae: 0.8053 - mse: 1.0717 - val_loss: 228.6335 - val_mae: 11.3669 - val_mse: 228.6335\n",
      "Epoch 2612/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.9378 - mae: 0.6766 - mse: 0.9378 - val_loss: 227.4698 - val_mae: 11.5041 - val_mse: 227.4698\n",
      "Epoch 2613/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6211 - mae: 0.5714 - mse: 0.6211 - val_loss: 224.1069 - val_mae: 11.4621 - val_mse: 224.1069\n",
      "Epoch 2614/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5269 - mae: 0.5089 - mse: 0.5269 - val_loss: 229.8550 - val_mae: 11.4012 - val_mse: 229.8550\n",
      "Epoch 2615/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5835 - mae: 0.5554 - mse: 0.5835 - val_loss: 223.5438 - val_mae: 11.4516 - val_mse: 223.5438\n",
      "Epoch 2616/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4089 - mae: 0.4210 - mse: 0.4089 - val_loss: 232.8176 - val_mae: 11.5357 - val_mse: 232.8176\n",
      "Epoch 2617/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5766 - mae: 0.5155 - mse: 0.5766 - val_loss: 222.5456 - val_mae: 11.3593 - val_mse: 222.5456\n",
      "Epoch 2618/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4815 - mae: 0.4679 - mse: 0.4815 - val_loss: 228.6972 - val_mae: 11.4218 - val_mse: 228.6972\n",
      "Epoch 2619/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4172 - mae: 0.4055 - mse: 0.4172 - val_loss: 229.1539 - val_mae: 11.4922 - val_mse: 229.1539\n",
      "Epoch 2620/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3377 - mae: 0.3637 - mse: 0.3377 - val_loss: 225.4838 - val_mae: 11.4588 - val_mse: 225.4838\n",
      "Epoch 2621/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3709 - mae: 0.3421 - mse: 0.3709 - val_loss: 223.5454 - val_mae: 11.3801 - val_mse: 223.5454\n",
      "Epoch 2622/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4431 - mae: 0.3517 - mse: 0.4431 - val_loss: 230.3282 - val_mae: 11.4743 - val_mse: 230.3282\n",
      "Epoch 2623/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3257 - mae: 0.3231 - mse: 0.3257 - val_loss: 226.4030 - val_mae: 11.4511 - val_mse: 226.4030\n",
      "Epoch 2624/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.5448 - mae: 0.4820 - mse: 0.5448 - val_loss: 222.7531 - val_mae: 11.3538 - val_mse: 222.7531\n",
      "Epoch 2625/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.4824 - mae: 0.4156 - mse: 0.4824 - val_loss: 227.9161 - val_mae: 11.4249 - val_mse: 227.9161\n",
      "Epoch 2626/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.4111 - mae: 0.4180 - mse: 0.4111 - val_loss: 225.1947 - val_mae: 11.4942 - val_mse: 225.1947\n",
      "Epoch 2627/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4466 - mae: 0.4602 - mse: 0.4466 - val_loss: 229.4647 - val_mae: 11.4583 - val_mse: 229.4647\n",
      "Epoch 2628/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5747 - mae: 0.4995 - mse: 0.5747 - val_loss: 223.1144 - val_mae: 11.4396 - val_mse: 223.1144\n",
      "Epoch 2629/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4748 - mae: 0.4860 - mse: 0.4748 - val_loss: 229.0184 - val_mae: 11.5356 - val_mse: 229.0184\n",
      "Epoch 2630/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.8808 - mae: 0.6732 - mse: 0.8808 - val_loss: 221.9873 - val_mae: 11.4044 - val_mse: 221.9873\n",
      "Epoch 2631/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.7705 - mae: 0.6401 - mse: 0.7705 - val_loss: 236.4958 - val_mae: 11.5397 - val_mse: 236.4958\n",
      "Epoch 2632/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.7594 - mae: 0.8395 - mse: 1.7594 - val_loss: 219.4332 - val_mae: 11.2588 - val_mse: 219.4332\n",
      "Epoch 2633/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 2.4065 - mae: 1.0567 - mse: 2.4065 - val_loss: 227.2052 - val_mae: 11.4186 - val_mse: 227.2052\n",
      "Epoch 2634/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 2.2694 - mae: 1.1185 - mse: 2.2694 - val_loss: 238.2130 - val_mae: 11.6735 - val_mse: 238.2130\n",
      "Epoch 2635/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.9871 - mae: 1.1409 - mse: 1.9871 - val_loss: 210.4517 - val_mae: 11.1288 - val_mse: 210.4517\n",
      "Epoch 2636/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.2235 - mae: 0.8689 - mse: 1.223 - 0s 222us/step - loss: 1.9918 - mae: 1.0816 - mse: 1.9918 - val_loss: 229.5195 - val_mae: 11.4169 - val_mse: 229.5195\n",
      "Epoch 2637/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 1.0549 - mae: 0.7038 - mse: 1.054 - 0s 87us/step - loss: 1.2244 - mae: 0.7963 - mse: 1.2244 - val_loss: 220.5502 - val_mae: 11.3401 - val_mse: 220.5502\n",
      "Epoch 2638/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 1.1123 - mae: 0.7963 - mse: 1.1123 - val_loss: 223.1399 - val_mae: 11.2723 - val_mse: 223.1399\n",
      "Epoch 2639/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 1.0949 - mae: 0.8216 - mse: 1.0949 - val_loss: 219.6422 - val_mae: 11.2263 - val_mse: 219.6422\n",
      "Epoch 2640/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.3332 - mae: 0.8608 - mse: 1.3332 - val_loss: 224.5327 - val_mae: 11.2410 - val_mse: 224.5327\n",
      "Epoch 2641/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.3769 - mae: 0.9336 - mse: 1.3769 - val_loss: 221.9480 - val_mae: 11.4197 - val_mse: 221.9480\n",
      "Epoch 2642/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.7902 - mae: 0.6541 - mse: 0.7902 - val_loss: 223.1175 - val_mae: 11.3023 - val_mse: 223.1175\n",
      "Epoch 2643/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.6511 - mae: 0.5800 - mse: 0.6511 - val_loss: 218.7014 - val_mae: 11.1704 - val_mse: 218.7014\n",
      "Epoch 2644/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.6106 - mae: 0.5514 - mse: 0.6106 - val_loss: 223.5892 - val_mae: 11.3542 - val_mse: 223.5892\n",
      "Epoch 2645/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.6862 - mae: 0.6175 - mse: 0.6862 - val_loss: 228.4228 - val_mae: 11.3713 - val_mse: 228.4228\n",
      "Epoch 2646/3500\n",
      "126/126 [==============================] - 0s 56us/step - loss: 0.6296 - mae: 0.5693 - mse: 0.6296 - val_loss: 215.8793 - val_mae: 11.2645 - val_mse: 215.8793\n",
      "Epoch 2647/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6838 - mae: 0.5857 - mse: 0.6838 - val_loss: 228.9283 - val_mae: 11.3756 - val_mse: 228.9283\n",
      "Epoch 2648/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.6673 - mae: 0.5743 - mse: 0.6673 - val_loss: 224.6773 - val_mae: 11.3501 - val_mse: 224.6773\n",
      "Epoch 2649/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.7194 - mae: 0.6012 - mse: 0.7194 - val_loss: 225.1260 - val_mae: 11.3533 - val_mse: 225.1260\n",
      "Epoch 2650/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5115 - mae: 0.5141 - mse: 0.5115 - val_loss: 221.9780 - val_mae: 11.3048 - val_mse: 221.9780\n",
      "Epoch 2651/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.6275 - mae: 0.5542 - mse: 0.6275 - val_loss: 225.3793 - val_mae: 11.3029 - val_mse: 225.3793\n",
      "Epoch 2652/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4324 - mae: 0.4527 - mse: 0.4324 - val_loss: 220.5043 - val_mae: 11.3341 - val_mse: 220.5043\n",
      "Epoch 2653/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3960 - mae: 0.4291 - mse: 0.3960 - val_loss: 224.9400 - val_mae: 11.3415 - val_mse: 224.9400\n",
      "Epoch 2654/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4074 - mae: 0.3968 - mse: 0.4074 - val_loss: 222.0536 - val_mae: 11.3303 - val_mse: 222.0536\n",
      "Epoch 2655/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3525 - mae: 0.3486 - mse: 0.3525 - val_loss: 226.8372 - val_mae: 11.4036 - val_mse: 226.8372\n",
      "Epoch 2656/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3134 - mae: 0.3182 - mse: 0.3134 - val_loss: 223.3866 - val_mae: 11.3040 - val_mse: 223.3866\n",
      "Epoch 2657/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2921 - mae: 0.2818 - mse: 0.2921 - val_loss: 223.2259 - val_mae: 11.3036 - val_mse: 223.2259\n",
      "Epoch 2658/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3116 - mae: 0.3259 - mse: 0.3116 - val_loss: 224.5443 - val_mae: 11.3258 - val_mse: 224.5443\n",
      "Epoch 2659/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.2975 - mae: 0.3062 - mse: 0.2975 - val_loss: 225.3382 - val_mae: 11.3997 - val_mse: 225.3382\n",
      "Epoch 2660/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3380 - mae: 0.3294 - mse: 0.3380 - val_loss: 226.6568 - val_mae: 11.3709 - val_mse: 226.6568\n",
      "Epoch 2661/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3143 - mae: 0.3185 - mse: 0.3143 - val_loss: 221.4098 - val_mae: 11.2910 - val_mse: 221.4098\n",
      "Epoch 2662/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3217 - mae: 0.3281 - mse: 0.3217 - val_loss: 228.1646 - val_mae: 11.3537 - val_mse: 228.1646\n",
      "Epoch 2663/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3311 - mae: 0.3329 - mse: 0.3311 - val_loss: 220.8856 - val_mae: 11.3136 - val_mse: 220.8856\n",
      "Epoch 2664/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3506 - mae: 0.3466 - mse: 0.3506 - val_loss: 228.0227 - val_mae: 11.3426 - val_mse: 228.0227\n",
      "Epoch 2665/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3194 - mae: 0.3286 - mse: 0.3194 - val_loss: 222.9852 - val_mae: 11.3403 - val_mse: 222.9852\n",
      "Epoch 2666/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3124 - mae: 0.2960 - mse: 0.3124 - val_loss: 225.9733 - val_mae: 11.3571 - val_mse: 225.9733\n",
      "Epoch 2667/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.2762 - mae: 0.2795 - mse: 0.2762 - val_loss: 224.9738 - val_mae: 11.3690 - val_mse: 224.9738\n",
      "Epoch 2668/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2597 - mae: 0.2742 - mse: 0.2597 - val_loss: 220.8946 - val_mae: 11.2841 - val_mse: 220.8946\n",
      "Epoch 2669/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2601 - mae: 0.2314 - mse: 0.2601 - val_loss: 226.7090 - val_mae: 11.3674 - val_mse: 226.7090\n",
      "Epoch 2670/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2693 - mae: 0.2377 - mse: 0.2693 - val_loss: 223.0524 - val_mae: 11.3534 - val_mse: 223.0524\n",
      "Epoch 2671/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3089 - mae: 0.2733 - mse: 0.3089 - val_loss: 222.3061 - val_mae: 11.2916 - val_mse: 222.3061\n",
      "Epoch 2672/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2864 - mae: 0.2884 - mse: 0.2864 - val_loss: 226.0763 - val_mae: 11.3460 - val_mse: 226.0763\n",
      "Epoch 2673/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3106 - mae: 0.3078 - mse: 0.3106 - val_loss: 221.4586 - val_mae: 11.3317 - val_mse: 221.4586\n",
      "Epoch 2674/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3121 - mae: 0.3373 - mse: 0.3121 - val_loss: 225.4765 - val_mae: 11.3078 - val_mse: 225.4765\n",
      "Epoch 2675/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3237 - mae: 0.2877 - mse: 0.3237 - val_loss: 223.1093 - val_mae: 11.3378 - val_mse: 223.1093\n",
      "Epoch 2676/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2513 - mae: 0.2451 - mse: 0.2513 - val_loss: 224.3755 - val_mae: 11.3289 - val_mse: 224.3755\n",
      "Epoch 2677/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2682 - mae: 0.2288 - mse: 0.2682 - val_loss: 223.2688 - val_mae: 11.3217 - val_mse: 223.2688\n",
      "Epoch 2678/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2657 - mae: 0.2457 - mse: 0.2657 - val_loss: 226.6606 - val_mae: 11.4037 - val_mse: 226.6606\n",
      "Epoch 2679/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.2675 - mae: 0.2552 - mse: 0.2675 - val_loss: 224.1725 - val_mae: 11.3313 - val_mse: 224.1725\n",
      "Epoch 2680/3500\n",
      "126/126 [==============================] - 0s 356us/step - loss: 0.2807 - mae: 0.2641 - mse: 0.2807 - val_loss: 223.9853 - val_mae: 11.3067 - val_mse: 223.9853\n",
      "Epoch 2681/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2857 - mae: 0.2550 - mse: 0.2857 - val_loss: 224.0021 - val_mae: 11.3501 - val_mse: 224.0021\n",
      "Epoch 2682/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2947 - mae: 0.2875 - mse: 0.2947 - val_loss: 222.6673 - val_mae: 11.3217 - val_mse: 222.6673\n",
      "Epoch 2683/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2794 - mae: 0.2474 - mse: 0.2794 - val_loss: 224.6892 - val_mae: 11.3130 - val_mse: 224.6892\n",
      "Epoch 2684/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.2592 - mae: 0.2255 - mse: 0.2592 - val_loss: 223.4507 - val_mae: 11.3410 - val_mse: 223.4507\n",
      "Epoch 2685/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.2588 - mae: 0.2477 - mse: 0.2588 - val_loss: 224.3289 - val_mae: 11.3212 - val_mse: 224.3289\n",
      "Epoch 2686/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.2522 - mae: 0.2414 - mse: 0.2522 - val_loss: 223.0946 - val_mae: 11.3353 - val_mse: 223.0946\n",
      "Epoch 2687/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2769 - mae: 0.2766 - mse: 0.2769 - val_loss: 224.6174 - val_mae: 11.3408 - val_mse: 224.6174\n",
      "Epoch 2688/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2824 - mae: 0.2308 - mse: 0.2824 - val_loss: 224.0171 - val_mae: 11.3117 - val_mse: 224.0171\n",
      "Epoch 2689/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2329 - mae: 0.2107 - mse: 0.2329 - val_loss: 224.9478 - val_mae: 11.3255 - val_mse: 224.9478\n",
      "Epoch 2690/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2572 - mae: 0.2276 - mse: 0.2572 - val_loss: 224.6403 - val_mae: 11.3189 - val_mse: 224.6403\n",
      "Epoch 2691/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2333 - mae: 0.2189 - mse: 0.2333 - val_loss: 224.0536 - val_mae: 11.3430 - val_mse: 224.0536\n",
      "Epoch 2692/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2728 - mae: 0.2244 - mse: 0.2728 - val_loss: 226.0704 - val_mae: 11.3506 - val_mse: 226.0704\n",
      "Epoch 2693/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2402 - mae: 0.2064 - mse: 0.2402 - val_loss: 222.6548 - val_mae: 11.3204 - val_mse: 222.6548\n",
      "Epoch 2694/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2447 - mae: 0.2132 - mse: 0.2447 - val_loss: 225.2700 - val_mae: 11.3535 - val_mse: 225.2700\n",
      "Epoch 2695/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2691 - mae: 0.2145 - mse: 0.2691 - val_loss: 226.0824 - val_mae: 11.3601 - val_mse: 226.0824\n",
      "Epoch 2696/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2592 - mae: 0.2226 - mse: 0.2592 - val_loss: 222.4688 - val_mae: 11.3317 - val_mse: 222.4688\n",
      "Epoch 2697/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2388 - mae: 0.2351 - mse: 0.2388 - val_loss: 224.9698 - val_mae: 11.3361 - val_mse: 224.9698\n",
      "Epoch 2698/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2516 - mae: 0.2185 - mse: 0.2516 - val_loss: 225.1018 - val_mae: 11.3416 - val_mse: 225.1018\n",
      "Epoch 2699/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2254 - mae: 0.1982 - mse: 0.2254 - val_loss: 224.2085 - val_mae: 11.3638 - val_mse: 224.2085\n",
      "Epoch 2700/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.2159 - mae: 0.1757 - mse: 0.2159 - val_loss: 225.3345 - val_mae: 11.3383 - val_mse: 225.3345\n",
      "Epoch 2701/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2262 - mae: 0.1682 - mse: 0.2262 - val_loss: 223.5993 - val_mae: 11.3381 - val_mse: 223.5993\n",
      "Epoch 2702/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2433 - mae: 0.2173 - mse: 0.2433 - val_loss: 225.7201 - val_mae: 11.3465 - val_mse: 225.7201\n",
      "Epoch 2703/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2380 - mae: 0.2054 - mse: 0.2380 - val_loss: 222.6365 - val_mae: 11.3344 - val_mse: 222.6365\n",
      "Epoch 2704/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.2536 - mae: 0.2129 - mse: 0.2536 - val_loss: 226.6795 - val_mae: 11.3713 - val_mse: 226.6795\n",
      "Epoch 2705/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.3888 - mae: 0.3043 - mse: 0.388 - 0s 71us/step - loss: 0.2492 - mae: 0.2113 - mse: 0.2492 - val_loss: 223.2624 - val_mae: 11.3376 - val_mse: 223.2624\n",
      "Epoch 2706/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2830 - mae: 0.2323 - mse: 0.2830 - val_loss: 224.0317 - val_mae: 11.3362 - val_mse: 224.0317\n",
      "Epoch 2707/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2536 - mae: 0.2239 - mse: 0.2536 - val_loss: 227.0441 - val_mae: 11.4067 - val_mse: 227.0441\n",
      "Epoch 2708/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2235 - mae: 0.1769 - mse: 0.2235 - val_loss: 225.0989 - val_mae: 11.3584 - val_mse: 225.0989\n",
      "Epoch 2709/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.2334 - mae: 0.2067 - mse: 0.2334 - val_loss: 224.0527 - val_mae: 11.3235 - val_mse: 224.0527\n",
      "Epoch 2710/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2550 - mae: 0.2302 - mse: 0.2550 - val_loss: 225.3759 - val_mae: 11.3480 - val_mse: 225.3759\n",
      "Epoch 2711/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3487 - mae: 0.3519 - mse: 0.3487 - val_loss: 227.2125 - val_mae: 11.3299 - val_mse: 227.2125\n",
      "Epoch 2712/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5256 - mae: 0.5378 - mse: 0.5256 - val_loss: 220.1408 - val_mae: 11.3242 - val_mse: 220.1408\n",
      "Epoch 2713/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5207 - mae: 0.5158 - mse: 0.5207 - val_loss: 227.3507 - val_mae: 11.3503 - val_mse: 227.3507\n",
      "Epoch 2714/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4620 - mae: 0.4703 - mse: 0.4620 - val_loss: 224.0313 - val_mae: 11.3758 - val_mse: 224.0313\n",
      "Epoch 2715/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4764 - mae: 0.4441 - mse: 0.4764 - val_loss: 223.8958 - val_mae: 11.3136 - val_mse: 223.8958\n",
      "Epoch 2716/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3662 - mae: 0.3807 - mse: 0.3662 - val_loss: 227.4051 - val_mae: 11.3682 - val_mse: 227.4051\n",
      "Epoch 2717/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4748 - mae: 0.4322 - mse: 0.4748 - val_loss: 220.1326 - val_mae: 11.2549 - val_mse: 220.1326\n",
      "Epoch 2718/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4477 - mae: 0.4637 - mse: 0.4477 - val_loss: 228.3416 - val_mae: 11.4096 - val_mse: 228.3416\n",
      "Epoch 2719/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3498 - mae: 0.3532 - mse: 0.3498 - val_loss: 224.9373 - val_mae: 11.3628 - val_mse: 224.9373\n",
      "Epoch 2720/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3699 - mae: 0.3452 - mse: 0.3699 - val_loss: 224.3421 - val_mae: 11.2647 - val_mse: 224.3421\n",
      "Epoch 2721/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3464 - mae: 0.3588 - mse: 0.3464 - val_loss: 222.7045 - val_mae: 11.3285 - val_mse: 222.7045\n",
      "Epoch 2722/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3626 - mae: 0.3615 - mse: 0.3626 - val_loss: 222.5534 - val_mae: 11.2290 - val_mse: 222.5534\n",
      "Epoch 2723/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3075 - mae: 0.3288 - mse: 0.3075 - val_loss: 224.8611 - val_mae: 11.3558 - val_mse: 224.8611\n",
      "Epoch 2724/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2719 - mae: 0.2670 - mse: 0.2719 - val_loss: 226.0500 - val_mae: 11.3912 - val_mse: 226.0500\n",
      "Epoch 2725/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2846 - mae: 0.2508 - mse: 0.2846 - val_loss: 224.0660 - val_mae: 11.2851 - val_mse: 224.0660\n",
      "Epoch 2726/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2701 - mae: 0.2700 - mse: 0.2701 - val_loss: 222.7599 - val_mae: 11.3116 - val_mse: 222.7599\n",
      "Epoch 2727/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.2608 - mae: 0.2396 - mse: 0.2608 - val_loss: 225.8479 - val_mae: 11.3414 - val_mse: 225.8479\n",
      "Epoch 2728/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2557 - mae: 0.2234 - mse: 0.2557 - val_loss: 225.2714 - val_mae: 11.3507 - val_mse: 225.2714\n",
      "Epoch 2729/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2503 - mae: 0.2384 - mse: 0.2503 - val_loss: 223.5651 - val_mae: 11.3279 - val_mse: 223.5651\n",
      "Epoch 2730/3500\n",
      "126/126 [==============================] - 0s 174us/step - loss: 0.2818 - mae: 0.2891 - mse: 0.2818 - val_loss: 222.5399 - val_mae: 11.2849 - val_mse: 222.5399\n",
      "Epoch 2731/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3172 - mae: 0.3087 - mse: 0.3172 - val_loss: 225.9231 - val_mae: 11.3664 - val_mse: 225.9231\n",
      "Epoch 2732/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3367 - mae: 0.3333 - mse: 0.3367 - val_loss: 222.5151 - val_mae: 11.3009 - val_mse: 222.5151\n",
      "Epoch 2733/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.3193 - mae: 0.3006 - mse: 0.3193 - val_loss: 224.2621 - val_mae: 11.3086 - val_mse: 224.2621\n",
      "Epoch 2734/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.2996 - mae: 0.3099 - mse: 0.2996 - val_loss: 225.7984 - val_mae: 11.3467 - val_mse: 225.7984\n",
      "Epoch 2735/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3149 - mae: 0.3026 - mse: 0.3149 - val_loss: 220.9703 - val_mae: 11.2806 - val_mse: 220.9703\n",
      "Epoch 2736/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3427 - mae: 0.3281 - mse: 0.3427 - val_loss: 227.5779 - val_mae: 11.3357 - val_mse: 227.5779\n",
      "Epoch 2737/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3414 - mae: 0.3758 - mse: 0.3414 - val_loss: 220.2573 - val_mae: 11.3196 - val_mse: 220.2573\n",
      "Epoch 2738/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3866 - mae: 0.3973 - mse: 0.3866 - val_loss: 230.0979 - val_mae: 11.3892 - val_mse: 230.0979\n",
      "Epoch 2739/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3869 - mae: 0.4090 - mse: 0.3869 - val_loss: 222.4936 - val_mae: 11.3346 - val_mse: 222.4936\n",
      "Epoch 2740/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3656 - mae: 0.3377 - mse: 0.3656 - val_loss: 222.7155 - val_mae: 11.2799 - val_mse: 222.7155\n",
      "Epoch 2741/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3342 - mae: 0.3321 - mse: 0.3342 - val_loss: 224.4787 - val_mae: 11.3684 - val_mse: 224.4787\n",
      "Epoch 2742/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3563 - mae: 0.3535 - mse: 0.3563 - val_loss: 223.3631 - val_mae: 11.3408 - val_mse: 223.3631\n",
      "Epoch 2743/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4021 - mae: 0.3875 - mse: 0.4021 - val_loss: 226.8901 - val_mae: 11.3615 - val_mse: 226.8901\n",
      "Epoch 2744/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4528 - mae: 0.4594 - mse: 0.4528 - val_loss: 222.9035 - val_mae: 11.3518 - val_mse: 222.9035\n",
      "Epoch 2745/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4742 - mae: 0.4559 - mse: 0.4742 - val_loss: 225.1600 - val_mae: 11.2821 - val_mse: 225.1600\n",
      "Epoch 2746/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5369 - mae: 0.5093 - mse: 0.5369 - val_loss: 225.0646 - val_mae: 11.3370 - val_mse: 225.0646\n",
      "Epoch 2747/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5389 - mae: 0.5360 - mse: 0.5389 - val_loss: 223.9122 - val_mae: 11.3523 - val_mse: 223.9122\n",
      "Epoch 2748/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4928 - mae: 0.4606 - mse: 0.4928 - val_loss: 231.0992 - val_mae: 11.4449 - val_mse: 231.0992\n",
      "Epoch 2749/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6811 - mae: 0.5856 - mse: 0.6811 - val_loss: 222.3211 - val_mae: 11.3376 - val_mse: 222.3211\n",
      "Epoch 2750/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5281 - mae: 0.4766 - mse: 0.5281 - val_loss: 225.7714 - val_mae: 11.3770 - val_mse: 225.7714\n",
      "Epoch 2751/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4193 - mae: 0.4143 - mse: 0.4193 - val_loss: 229.5556 - val_mae: 11.4798 - val_mse: 229.5556\n",
      "Epoch 2752/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4125 - mae: 0.4054 - mse: 0.4125 - val_loss: 221.9472 - val_mae: 11.3325 - val_mse: 221.9472\n",
      "Epoch 2753/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3881 - mae: 0.3903 - mse: 0.3881 - val_loss: 228.0925 - val_mae: 11.3199 - val_mse: 228.0925\n",
      "Epoch 2754/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4077 - mae: 0.4277 - mse: 0.4077 - val_loss: 221.5736 - val_mae: 11.3196 - val_mse: 221.5736\n",
      "Epoch 2755/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.6417 - mae: 0.5182 - mse: 0.6417 - val_loss: 226.6289 - val_mae: 11.4073 - val_mse: 226.6289\n",
      "Epoch 2756/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4225 - mae: 0.4209 - mse: 0.4225 - val_loss: 227.4442 - val_mae: 11.3415 - val_mse: 227.4442\n",
      "Epoch 2757/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3336 - mae: 0.3132 - mse: 0.3336 - val_loss: 221.5820 - val_mae: 11.3056 - val_mse: 221.5820\n",
      "Epoch 2758/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3178 - mae: 0.2867 - mse: 0.3178 - val_loss: 223.9597 - val_mae: 11.3313 - val_mse: 223.9597\n",
      "Epoch 2759/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2979 - mae: 0.2792 - mse: 0.2979 - val_loss: 222.2112 - val_mae: 11.3306 - val_mse: 222.2112\n",
      "Epoch 2760/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2408 - mae: 0.2316 - mse: 0.2408 - val_loss: 226.1514 - val_mae: 11.3784 - val_mse: 226.1514\n",
      "Epoch 2761/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2884 - mae: 0.2463 - mse: 0.2884 - val_loss: 225.1163 - val_mae: 11.3515 - val_mse: 225.1163\n",
      "Epoch 2762/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3744 - mae: 0.3487 - mse: 0.3744 - val_loss: 222.0359 - val_mae: 11.2611 - val_mse: 222.0359\n",
      "Epoch 2763/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3800 - mae: 0.3585 - mse: 0.3800 - val_loss: 225.1485 - val_mae: 11.3699 - val_mse: 225.1485\n",
      "Epoch 2764/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3433 - mae: 0.3609 - mse: 0.3433 - val_loss: 228.1050 - val_mae: 11.3971 - val_mse: 228.1050\n",
      "Epoch 2765/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3649 - mae: 0.4017 - mse: 0.3649 - val_loss: 223.5827 - val_mae: 11.3414 - val_mse: 223.5827\n",
      "Epoch 2766/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3559 - mae: 0.3946 - mse: 0.3559 - val_loss: 224.0240 - val_mae: 11.3232 - val_mse: 224.0240\n",
      "Epoch 2767/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3777 - mae: 0.3665 - mse: 0.3777 - val_loss: 225.4771 - val_mae: 11.3923 - val_mse: 225.4771\n",
      "Epoch 2768/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2747 - mae: 0.2726 - mse: 0.2747 - val_loss: 229.4961 - val_mae: 11.4094 - val_mse: 229.4961\n",
      "Epoch 2769/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2922 - mae: 0.2829 - mse: 0.2922 - val_loss: 227.3091 - val_mae: 11.4083 - val_mse: 227.3091\n",
      "Epoch 2770/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2644 - mae: 0.2347 - mse: 0.2644 - val_loss: 224.9445 - val_mae: 11.3310 - val_mse: 224.9445\n",
      "Epoch 2771/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2605 - mae: 0.2558 - mse: 0.2605 - val_loss: 222.4010 - val_mae: 11.3170 - val_mse: 222.4010\n",
      "Epoch 2772/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2579 - mae: 0.2663 - mse: 0.2579 - val_loss: 224.0524 - val_mae: 11.3413 - val_mse: 224.0524\n",
      "Epoch 2773/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.2581 - mae: 0.2208 - mse: 0.2581 - val_loss: 226.0549 - val_mae: 11.3512 - val_mse: 226.0549\n",
      "Epoch 2774/3500\n",
      "126/126 [==============================] - 0s 166us/step - loss: 0.2534 - mae: 0.2085 - mse: 0.2534 - val_loss: 223.3007 - val_mae: 11.2911 - val_mse: 223.3007\n",
      "Epoch 2775/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.2257 - mae: 0.2081 - mse: 0.2257 - val_loss: 223.6433 - val_mae: 11.3100 - val_mse: 223.6433\n",
      "Epoch 2776/3500\n",
      "126/126 [==============================] - 0s 190us/step - loss: 0.2457 - mae: 0.2159 - mse: 0.2457 - val_loss: 226.0381 - val_mae: 11.3519 - val_mse: 226.0381\n",
      "Epoch 2777/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2782 - mae: 0.2705 - mse: 0.2782 - val_loss: 225.6058 - val_mae: 11.3668 - val_mse: 225.6058\n",
      "Epoch 2778/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3705 - mae: 0.3469 - mse: 0.3705 - val_loss: 223.7359 - val_mae: 11.3631 - val_mse: 223.7359\n",
      "Epoch 2779/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3296 - mae: 0.3335 - mse: 0.3296 - val_loss: 223.8662 - val_mae: 11.2712 - val_mse: 223.8662\n",
      "Epoch 2780/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3296 - mae: 0.3325 - mse: 0.3296 - val_loss: 224.7977 - val_mae: 11.3595 - val_mse: 224.7977\n",
      "Epoch 2781/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3681 - mae: 0.3423 - mse: 0.3681 - val_loss: 224.3325 - val_mae: 11.3121 - val_mse: 224.3325\n",
      "Epoch 2782/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3819 - mae: 0.3273 - mse: 0.3819 - val_loss: 223.3946 - val_mae: 11.3115 - val_mse: 223.3946\n",
      "Epoch 2783/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3681 - mae: 0.3832 - mse: 0.3681 - val_loss: 225.5164 - val_mae: 11.3401 - val_mse: 225.5164\n",
      "Epoch 2784/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4585 - mae: 0.4283 - mse: 0.4585 - val_loss: 226.1993 - val_mae: 11.3100 - val_mse: 226.1993\n",
      "Epoch 2785/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5253 - mae: 0.4153 - mse: 0.5253 - val_loss: 221.4589 - val_mae: 11.2958 - val_mse: 221.4589\n",
      "Epoch 2786/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4807 - mae: 0.4676 - mse: 0.4807 - val_loss: 226.5108 - val_mae: 11.3671 - val_mse: 226.5108\n",
      "Epoch 2787/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4108 - mae: 0.4321 - mse: 0.4108 - val_loss: 223.1025 - val_mae: 11.3301 - val_mse: 223.1025\n",
      "Epoch 2788/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3751 - mae: 0.3753 - mse: 0.3751 - val_loss: 228.0111 - val_mae: 11.3647 - val_mse: 228.0111\n",
      "Epoch 2789/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4735 - mae: 0.4486 - mse: 0.4735 - val_loss: 223.9280 - val_mae: 11.2900 - val_mse: 223.9280\n",
      "Epoch 2790/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4176 - mae: 0.4527 - mse: 0.4176 - val_loss: 221.3882 - val_mae: 11.3205 - val_mse: 221.3882\n",
      "Epoch 2791/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4217 - mae: 0.4378 - mse: 0.4217 - val_loss: 230.0577 - val_mae: 11.4134 - val_mse: 230.0577\n",
      "Epoch 2792/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5768 - mae: 0.5450 - mse: 0.5768 - val_loss: 220.0573 - val_mae: 11.2561 - val_mse: 220.0573\n",
      "Epoch 2793/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4918 - mae: 0.4497 - mse: 0.4918 - val_loss: 224.0483 - val_mae: 11.2675 - val_mse: 224.0483\n",
      "Epoch 2794/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4599 - mae: 0.4548 - mse: 0.4599 - val_loss: 228.4562 - val_mae: 11.3478 - val_mse: 228.4562\n",
      "Epoch 2795/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4516 - mae: 0.4644 - mse: 0.4516 - val_loss: 227.3753 - val_mae: 11.4664 - val_mse: 227.3753\n",
      "Epoch 2796/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5845 - mae: 0.5510 - mse: 0.5845 - val_loss: 227.6979 - val_mae: 11.3480 - val_mse: 227.6979\n",
      "Epoch 2797/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.5044 - mae: 0.4958 - mse: 0.5044 - val_loss: 222.2043 - val_mae: 11.3325 - val_mse: 222.2043\n",
      "Epoch 2798/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4176 - mae: 0.4093 - mse: 0.4176 - val_loss: 227.5611 - val_mae: 11.4340 - val_mse: 227.5611\n",
      "Epoch 2799/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4107 - mae: 0.4127 - mse: 0.4107 - val_loss: 224.4855 - val_mae: 11.3419 - val_mse: 224.4855\n",
      "Epoch 2800/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4537 - mae: 0.4297 - mse: 0.4537 - val_loss: 228.5312 - val_mae: 11.4571 - val_mse: 228.5312\n",
      "Epoch 2801/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3930 - mae: 0.4126 - mse: 0.3930 - val_loss: 227.1261 - val_mae: 11.2895 - val_mse: 227.1261\n",
      "Epoch 2802/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3267 - mae: 0.3577 - mse: 0.3267 - val_loss: 223.3960 - val_mae: 11.3279 - val_mse: 223.3960\n",
      "Epoch 2803/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3538 - mae: 0.3917 - mse: 0.3538 - val_loss: 227.5874 - val_mae: 11.3895 - val_mse: 227.5874\n",
      "Epoch 2804/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3346 - mae: 0.3406 - mse: 0.3346 - val_loss: 222.9873 - val_mae: 11.3518 - val_mse: 222.9873\n",
      "Epoch 2805/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3368 - mae: 0.3561 - mse: 0.3368 - val_loss: 227.1590 - val_mae: 11.3476 - val_mse: 227.1590\n",
      "Epoch 2806/3500\n",
      "126/126 [==============================] - 0s 102us/step - loss: 0.2884 - mae: 0.3014 - mse: 0.2884 - val_loss: 225.6497 - val_mae: 11.3923 - val_mse: 225.6497\n",
      "Epoch 2807/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2685 - mae: 0.2736 - mse: 0.2685 - val_loss: 224.7711 - val_mae: 11.3557 - val_mse: 224.7711\n",
      "Epoch 2808/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2580 - mae: 0.2319 - mse: 0.2580 - val_loss: 225.1230 - val_mae: 11.3390 - val_mse: 225.1230\n",
      "Epoch 2809/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2646 - mae: 0.2542 - mse: 0.2646 - val_loss: 223.4000 - val_mae: 11.3196 - val_mse: 223.4000\n",
      "Epoch 2810/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2820 - mae: 0.2439 - mse: 0.2820 - val_loss: 222.7431 - val_mae: 11.2894 - val_mse: 222.7431\n",
      "Epoch 2811/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2771 - mae: 0.2388 - mse: 0.2771 - val_loss: 226.3475 - val_mae: 11.3752 - val_mse: 226.3475\n",
      "Epoch 2812/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2674 - mae: 0.2098 - mse: 0.2674 - val_loss: 225.0604 - val_mae: 11.3479 - val_mse: 225.0604\n",
      "Epoch 2813/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2888 - mae: 0.2554 - mse: 0.2888 - val_loss: 225.2876 - val_mae: 11.3232 - val_mse: 225.2876\n",
      "Epoch 2814/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2765 - mae: 0.2985 - mse: 0.2765 - val_loss: 222.2696 - val_mae: 11.3192 - val_mse: 222.2696\n",
      "Epoch 2815/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3151 - mae: 0.3058 - mse: 0.3151 - val_loss: 225.2686 - val_mae: 11.3410 - val_mse: 225.2686\n",
      "Epoch 2816/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.3982 - mae: 0.3955 - mse: 0.3982 - val_loss: 228.6691 - val_mae: 11.4320 - val_mse: 228.6691\n",
      "Epoch 2817/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4507 - mae: 0.4268 - mse: 0.4507 - val_loss: 218.8903 - val_mae: 11.2433 - val_mse: 218.8903\n",
      "Epoch 2818/3500\n",
      "126/126 [==============================] - 0s 253us/step - loss: 0.5849 - mae: 0.5561 - mse: 0.5849 - val_loss: 229.3127 - val_mae: 11.3808 - val_mse: 229.3127\n",
      "Epoch 2819/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 1.0806 - mae: 0.8227 - mse: 1.0806 - val_loss: 228.1385 - val_mae: 11.4056 - val_mse: 228.1385\n",
      "Epoch 2820/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 1.1499 - mae: 0.8161 - mse: 1.1499 - val_loss: 226.1962 - val_mae: 11.4980 - val_mse: 226.1962\n",
      "Epoch 2821/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 1.0926 - mae: 0.8113 - mse: 1.0926 - val_loss: 232.3588 - val_mae: 11.4506 - val_mse: 232.3588\n",
      "Epoch 2822/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.0452 - mae: 0.7839 - mse: 1.0452 - val_loss: 220.6824 - val_mae: 11.3230 - val_mse: 220.6824\n",
      "Epoch 2823/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.3758 - mae: 0.8744 - mse: 1.3758 - val_loss: 231.2340 - val_mae: 11.5387 - val_mse: 231.2340\n",
      "Epoch 2824/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.5679 - mae: 0.8432 - mse: 1.5679 - val_loss: 235.3488 - val_mae: 11.4551 - val_mse: 235.3488\n",
      "Epoch 2825/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.2709 - mae: 0.9104 - mse: 1.2709 - val_loss: 214.6432 - val_mae: 11.3176 - val_mse: 214.6432\n",
      "Epoch 2826/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.5504 - mae: 0.9672 - mse: 1.5504 - val_loss: 238.2218 - val_mae: 11.6466 - val_mse: 238.2218\n",
      "Epoch 2827/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.7308 - mae: 1.0151 - mse: 1.7308 - val_loss: 209.6066 - val_mae: 11.2153 - val_mse: 209.6066\n",
      "Epoch 2828/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 2.4742 - mae: 1.2403 - mse: 2.4742 - val_loss: 228.3040 - val_mae: 11.2826 - val_mse: 228.3040\n",
      "Epoch 2829/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.7154 - mae: 0.9600 - mse: 1.7154 - val_loss: 231.5803 - val_mae: 11.5771 - val_mse: 231.5803\n",
      "Epoch 2830/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.6569 - mae: 0.9635 - mse: 1.6569 - val_loss: 214.8193 - val_mae: 11.0743 - val_mse: 214.8193\n",
      "Epoch 2831/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.8767 - mae: 1.0807 - mse: 1.8767 - val_loss: 229.8076 - val_mae: 11.3829 - val_mse: 229.8076\n",
      "Epoch 2832/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 1.7033 - mae: 0.9610 - mse: 1.7033 - val_loss: 223.4486 - val_mae: 11.4968 - val_mse: 223.4486\n",
      "Epoch 2833/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.9398 - mae: 0.7474 - mse: 0.9398 - val_loss: 224.2763 - val_mae: 11.3040 - val_mse: 224.2763\n",
      "Epoch 2834/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6542 - mae: 0.5923 - mse: 0.6542 - val_loss: 227.4050 - val_mae: 11.4012 - val_mse: 227.4050\n",
      "Epoch 2835/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.9773 - mae: 0.7083 - mse: 0.9773 - val_loss: 219.1464 - val_mae: 11.1108 - val_mse: 219.1464\n",
      "Epoch 2836/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.8109 - mae: 0.6158 - mse: 0.8109 - val_loss: 222.4758 - val_mae: 11.2962 - val_mse: 222.4758\n",
      "Epoch 2837/3500\n",
      "126/126 [==============================] - 0s 166us/step - loss: 0.5005 - mae: 0.4687 - mse: 0.5005 - val_loss: 223.8534 - val_mae: 11.3756 - val_mse: 223.8534\n",
      "Epoch 2838/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.6694 - mae: 0.5591 - mse: 0.6694 - val_loss: 226.0670 - val_mae: 11.3670 - val_mse: 226.0670\n",
      "Epoch 2839/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6312 - mae: 0.5645 - mse: 0.6312 - val_loss: 221.0493 - val_mae: 11.3223 - val_mse: 221.0493\n",
      "Epoch 2840/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.6884 - mae: 0.5934 - mse: 0.6884 - val_loss: 228.6877 - val_mae: 11.3400 - val_mse: 228.6877\n",
      "Epoch 2841/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.6041 - mae: 0.5700 - mse: 0.6041 - val_loss: 220.2050 - val_mae: 11.2570 - val_mse: 220.2050\n",
      "Epoch 2842/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5456 - mae: 0.5004 - mse: 0.5456 - val_loss: 225.9438 - val_mae: 11.2727 - val_mse: 225.9438\n",
      "Epoch 2843/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4590 - mae: 0.4540 - mse: 0.4590 - val_loss: 217.3148 - val_mae: 11.2274 - val_mse: 217.3148\n",
      "Epoch 2844/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.5274 - mae: 0.4860 - mse: 0.5274 - val_loss: 232.0219 - val_mae: 11.4148 - val_mse: 232.0219\n",
      "Epoch 2845/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.5624 - mae: 0.4992 - mse: 0.5624 - val_loss: 223.1391 - val_mae: 11.3327 - val_mse: 223.1391\n",
      "Epoch 2846/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.4190 - mae: 0.4124 - mse: 0.4190 - val_loss: 223.4966 - val_mae: 11.2442 - val_mse: 223.4966\n",
      "Epoch 2847/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4667 - mae: 0.4164 - mse: 0.4667 - val_loss: 224.6451 - val_mae: 11.2818 - val_mse: 224.6451\n",
      "Epoch 2848/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.3554 - mae: 0.3874 - mse: 0.3554 - val_loss: 221.3312 - val_mae: 11.2368 - val_mse: 221.3312\n",
      "Epoch 2849/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.3891 - mae: 0.3794 - mse: 0.3891 - val_loss: 229.4846 - val_mae: 11.3450 - val_mse: 229.4846\n",
      "Epoch 2850/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4446 - mae: 0.3972 - mse: 0.4446 - val_loss: 222.3808 - val_mae: 11.3114 - val_mse: 222.3808\n",
      "Epoch 2851/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4462 - mae: 0.4056 - mse: 0.4462 - val_loss: 224.9165 - val_mae: 11.2937 - val_mse: 224.9165\n",
      "Epoch 2852/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3402 - mae: 0.3490 - mse: 0.3402 - val_loss: 222.9977 - val_mae: 11.3135 - val_mse: 222.9977\n",
      "Epoch 2853/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.4462 - mae: 0.4320 - mse: 0.4462 - val_loss: 225.4221 - val_mae: 11.2823 - val_mse: 225.4221\n",
      "Epoch 2854/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5072 - mae: 0.4338 - mse: 0.5072 - val_loss: 230.2771 - val_mae: 11.3796 - val_mse: 230.2771\n",
      "Epoch 2855/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.5172 - mae: 0.4755 - mse: 0.5172 - val_loss: 224.1556 - val_mae: 11.3566 - val_mse: 224.1556\n",
      "Epoch 2856/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4851 - mae: 0.5063 - mse: 0.4851 - val_loss: 230.5063 - val_mae: 11.3597 - val_mse: 230.5063\n",
      "Epoch 2857/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5956 - mae: 0.5827 - mse: 0.5956 - val_loss: 222.2455 - val_mae: 11.3448 - val_mse: 222.2455\n",
      "Epoch 2858/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5748 - mae: 0.5378 - mse: 0.5748 - val_loss: 229.7630 - val_mae: 11.3277 - val_mse: 229.7630\n",
      "Epoch 2859/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5214 - mae: 0.4911 - mse: 0.5214 - val_loss: 217.0515 - val_mae: 11.1650 - val_mse: 217.0515\n",
      "Epoch 2860/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5500 - mae: 0.4808 - mse: 0.5500 - val_loss: 229.5445 - val_mae: 11.3753 - val_mse: 229.5445\n",
      "Epoch 2861/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3996 - mae: 0.4201 - mse: 0.3996 - val_loss: 223.5764 - val_mae: 11.3047 - val_mse: 223.5764\n",
      "Epoch 2862/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3713 - mae: 0.3684 - mse: 0.3713 - val_loss: 221.6425 - val_mae: 11.2943 - val_mse: 221.6425\n",
      "Epoch 2863/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3246 - mae: 0.3009 - mse: 0.3246 - val_loss: 227.3379 - val_mae: 11.3601 - val_mse: 227.3379\n",
      "Epoch 2864/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3504 - mae: 0.3214 - mse: 0.3504 - val_loss: 223.5605 - val_mae: 11.2962 - val_mse: 223.5605\n",
      "Epoch 2865/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3320 - mae: 0.3258 - mse: 0.3320 - val_loss: 224.7907 - val_mae: 11.3146 - val_mse: 224.7907\n",
      "Epoch 2866/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3050 - mae: 0.3192 - mse: 0.3050 - val_loss: 224.2943 - val_mae: 11.2738 - val_mse: 224.2943\n",
      "Epoch 2867/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2753 - mae: 0.2842 - mse: 0.2753 - val_loss: 223.5948 - val_mae: 11.2882 - val_mse: 223.5948\n",
      "Epoch 2868/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3312 - mae: 0.3165 - mse: 0.3312 - val_loss: 224.2737 - val_mae: 11.3298 - val_mse: 224.2737\n",
      "Epoch 2869/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3090 - mae: 0.2814 - mse: 0.3090 - val_loss: 226.2499 - val_mae: 11.3257 - val_mse: 226.2499\n",
      "Epoch 2870/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2782 - mae: 0.2754 - mse: 0.2782 - val_loss: 221.9919 - val_mae: 11.3117 - val_mse: 221.9919\n",
      "Epoch 2871/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2921 - mae: 0.3049 - mse: 0.2921 - val_loss: 225.4812 - val_mae: 11.2972 - val_mse: 225.4812\n",
      "Epoch 2872/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2728 - mae: 0.2572 - mse: 0.2728 - val_loss: 224.5835 - val_mae: 11.3371 - val_mse: 224.5835\n",
      "Epoch 2873/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.2353 - mae: 0.2321 - mse: 0.2353 - val_loss: 225.7533 - val_mae: 11.2802 - val_mse: 225.7533\n",
      "Epoch 2874/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.2445 - mae: 0.2531 - mse: 0.2445 - val_loss: 221.1424 - val_mae: 11.2234 - val_mse: 221.1424\n",
      "Epoch 2875/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2879 - mae: 0.2594 - mse: 0.2879 - val_loss: 225.9895 - val_mae: 11.3557 - val_mse: 225.9895\n",
      "Epoch 2876/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.3353 - mae: 0.3311 - mse: 0.3353 - val_loss: 227.9128 - val_mae: 11.3321 - val_mse: 227.9128\n",
      "Epoch 2877/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4045 - mae: 0.4576 - mse: 0.4045 - val_loss: 221.1812 - val_mae: 11.3015 - val_mse: 221.1812\n",
      "Epoch 2878/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.6077 - mae: 0.5776 - mse: 0.6077 - val_loss: 225.1901 - val_mae: 11.2651 - val_mse: 225.1901\n",
      "Epoch 2879/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.7713 - mae: 0.6868 - mse: 0.7713 - val_loss: 230.5348 - val_mae: 11.4983 - val_mse: 230.5348\n",
      "Epoch 2880/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.6394 - mae: 0.5924 - mse: 0.6394 - val_loss: 218.7966 - val_mae: 11.2255 - val_mse: 218.7966\n",
      "Epoch 2881/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6310 - mae: 0.5537 - mse: 0.6310 - val_loss: 226.7717 - val_mae: 11.2870 - val_mse: 226.7717\n",
      "Epoch 2882/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.7315 - mae: 0.6514 - mse: 0.7315 - val_loss: 226.3257 - val_mae: 11.4174 - val_mse: 226.3257\n",
      "Epoch 2883/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.9116 - mae: 0.7308 - mse: 0.9116 - val_loss: 227.2029 - val_mae: 11.2893 - val_mse: 227.2029\n",
      "Epoch 2884/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 1.1352 - mae: 0.8258 - mse: 1.1352 - val_loss: 223.6684 - val_mae: 11.2200 - val_mse: 223.6684\n",
      "Epoch 2885/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.1689 - mae: 0.8568 - mse: 1.1689 - val_loss: 220.9583 - val_mae: 11.1842 - val_mse: 220.9583\n",
      "Epoch 2886/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 1.1388 - mae: 0.8173 - mse: 1.1388 - val_loss: 222.4587 - val_mae: 11.2610 - val_mse: 222.4587\n",
      "Epoch 2887/3500\n",
      "126/126 [==============================] - 0s 277us/step - loss: 0.8778 - mae: 0.7060 - mse: 0.8778 - val_loss: 217.8690 - val_mae: 11.1287 - val_mse: 217.8690\n",
      "Epoch 2888/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.8232 - mae: 0.6685 - mse: 0.8232 - val_loss: 232.2102 - val_mae: 11.4656 - val_mse: 232.2102\n",
      "Epoch 2889/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 1.1064 - mae: 0.8216 - mse: 1.1064 - val_loss: 227.2941 - val_mae: 11.4183 - val_mse: 227.2941\n",
      "Epoch 2890/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 1.0980 - mae: 0.7903 - mse: 1.0980 - val_loss: 218.7592 - val_mae: 11.1131 - val_mse: 218.7592\n",
      "Epoch 2891/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.9500 - mae: 0.7038 - mse: 0.9500 - val_loss: 222.0611 - val_mae: 11.2666 - val_mse: 222.0611\n",
      "Epoch 2892/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.6605 - mae: 0.6090 - mse: 0.6605 - val_loss: 227.2382 - val_mae: 11.4154 - val_mse: 227.2382\n",
      "Epoch 2893/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.6834 - mae: 0.6058 - mse: 0.6834 - val_loss: 224.0185 - val_mae: 11.2849 - val_mse: 224.0185\n",
      "Epoch 2894/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5152 - mae: 0.4978 - mse: 0.5152 - val_loss: 216.1315 - val_mae: 11.1011 - val_mse: 216.1315\n",
      "Epoch 2895/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4843 - mae: 0.4475 - mse: 0.4843 - val_loss: 224.2383 - val_mae: 11.2173 - val_mse: 224.2383\n",
      "Epoch 2896/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5184 - mae: 0.5104 - mse: 0.5184 - val_loss: 225.1400 - val_mae: 11.3075 - val_mse: 225.1400\n",
      "Epoch 2897/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.5598 - mae: 0.5250 - mse: 0.5598 - val_loss: 224.6488 - val_mae: 11.2925 - val_mse: 224.6488\n",
      "Epoch 2898/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5571 - mae: 0.5058 - mse: 0.5571 - val_loss: 224.0863 - val_mae: 11.3429 - val_mse: 224.0863\n",
      "Epoch 2899/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.7282 - mae: 0.6084 - mse: 0.7282 - val_loss: 224.8667 - val_mae: 11.3221 - val_mse: 224.8667\n",
      "Epoch 2900/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7540 - mae: 0.6340 - mse: 0.7540 - val_loss: 233.0800 - val_mae: 11.3418 - val_mse: 233.0800\n",
      "Epoch 2901/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.8230 - mae: 0.6591 - mse: 0.8230 - val_loss: 221.2771 - val_mae: 11.2988 - val_mse: 221.2771\n",
      "Epoch 2902/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.7408 - mae: 0.9263 - mse: 1.7408 - val_loss: 228.5236 - val_mae: 11.2611 - val_mse: 228.5236\n",
      "Epoch 2903/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.9083 - mae: 0.9254 - mse: 1.9083 - val_loss: 231.2668 - val_mae: 11.5299 - val_mse: 231.2668\n",
      "Epoch 2904/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.6030 - mae: 1.0012 - mse: 1.6030 - val_loss: 227.0891 - val_mae: 11.5279 - val_mse: 227.0891\n",
      "Epoch 2905/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.5920 - mae: 0.9753 - mse: 1.5920 - val_loss: 226.5696 - val_mae: 11.3349 - val_mse: 226.5696\n",
      "Epoch 2906/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.0485 - mae: 0.8085 - mse: 1.0485 - val_loss: 212.6546 - val_mae: 10.9771 - val_mse: 212.6546\n",
      "Epoch 2907/3500\n",
      "126/126 [==============================] - 0s 142us/step - loss: 1.0613 - mae: 0.7910 - mse: 1.0613 - val_loss: 221.1282 - val_mae: 11.2386 - val_mse: 221.1282\n",
      "Epoch 2908/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 1.2102 - mae: 0.8841 - mse: 1.2102 - val_loss: 217.3674 - val_mae: 11.2177 - val_mse: 217.3674\n",
      "Epoch 2909/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.8444 - mae: 0.9840 - mse: 1.8444 - val_loss: 225.3519 - val_mae: 11.2465 - val_mse: 225.3519\n",
      "Epoch 2910/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 2.2540 - mae: 1.1579 - mse: 2.2540 - val_loss: 223.8588 - val_mae: 11.3441 - val_mse: 223.8588\n",
      "Epoch 2911/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.9826 - mae: 1.0833 - mse: 1.9826 - val_loss: 222.8153 - val_mae: 11.3256 - val_mse: 222.8153\n",
      "Epoch 2912/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.5304 - mae: 0.9484 - mse: 1.5304 - val_loss: 233.0277 - val_mae: 11.4785 - val_mse: 233.0277\n",
      "Epoch 2913/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 1.2162 - mae: 0.8434 - mse: 1.2162 - val_loss: 220.6324 - val_mae: 11.2085 - val_mse: 220.6324\n",
      "Epoch 2914/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 1.1225 - mae: 0.7853 - mse: 1.1225 - val_loss: 217.4254 - val_mae: 11.0424 - val_mse: 217.4254\n",
      "Epoch 2915/3500\n",
      "126/126 [==============================] - 0s 269us/step - loss: 1.1430 - mae: 0.8778 - mse: 1.1430 - val_loss: 224.5155 - val_mae: 11.3656 - val_mse: 224.5155\n",
      "Epoch 2916/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.1794 - mae: 0.8189 - mse: 1.1794 - val_loss: 227.9714 - val_mae: 11.3198 - val_mse: 227.9714\n",
      "Epoch 2917/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.9282 - mae: 0.7746 - mse: 0.9282 - val_loss: 229.2740 - val_mae: 11.5206 - val_mse: 229.2740\n",
      "Epoch 2918/3500\n",
      "126/126 [==============================] - 0s 230us/step - loss: 0.9358 - mae: 0.7278 - mse: 0.9358 - val_loss: 220.3470 - val_mae: 11.0546 - val_mse: 220.3470\n",
      "Epoch 2919/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.1799 - mae: 0.8707 - mse: 1.1799 - val_loss: 217.9261 - val_mae: 11.1708 - val_mse: 217.9261\n",
      "Epoch 2920/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.8792 - mae: 0.7326 - mse: 0.8792 - val_loss: 228.7915 - val_mae: 11.4886 - val_mse: 228.7915\n",
      "Epoch 2921/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.6679 - mae: 0.5886 - mse: 0.6679 - val_loss: 219.6040 - val_mae: 11.1914 - val_mse: 219.6040\n",
      "Epoch 2922/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.6445 - mae: 0.5502 - mse: 0.6445 - val_loss: 223.9778 - val_mae: 11.2909 - val_mse: 223.9778\n",
      "Epoch 2923/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5184 - mae: 0.5311 - mse: 0.5184 - val_loss: 223.4065 - val_mae: 11.2756 - val_mse: 223.4065\n",
      "Epoch 2924/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.6199 - mae: 0.5155 - mse: 0.6199 - val_loss: 217.1362 - val_mae: 11.1374 - val_mse: 217.1362\n",
      "Epoch 2925/3500\n",
      "126/126 [==============================] - 0s 142us/step - loss: 0.4450 - mae: 0.4702 - mse: 0.4450 - val_loss: 225.4597 - val_mae: 11.2502 - val_mse: 225.4597\n",
      "Epoch 2926/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.4109 - mae: 0.4134 - mse: 0.4109 - val_loss: 220.7130 - val_mae: 11.2070 - val_mse: 220.7130\n",
      "Epoch 2927/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3151 - mae: 0.3385 - mse: 0.3151 - val_loss: 223.3021 - val_mae: 11.2422 - val_mse: 223.3021\n",
      "Epoch 2928/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3623 - mae: 0.3749 - mse: 0.3623 - val_loss: 222.1503 - val_mae: 11.2332 - val_mse: 222.1503\n",
      "Epoch 2929/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.3380 - mae: 0.3568 - mse: 0.3380 - val_loss: 220.7108 - val_mae: 11.2360 - val_mse: 220.7108\n",
      "Epoch 2930/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3302 - mae: 0.3271 - mse: 0.3302 - val_loss: 221.3810 - val_mae: 11.1739 - val_mse: 221.3810\n",
      "Epoch 2931/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3496 - mae: 0.3224 - mse: 0.3496 - val_loss: 221.4803 - val_mae: 11.2034 - val_mse: 221.4803\n",
      "Epoch 2932/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2822 - mae: 0.2981 - mse: 0.2822 - val_loss: 220.7499 - val_mae: 11.2350 - val_mse: 220.7499\n",
      "Epoch 2933/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3183 - mae: 0.3301 - mse: 0.3183 - val_loss: 221.5291 - val_mae: 11.1693 - val_mse: 221.5291\n",
      "Epoch 2934/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3758 - mae: 0.3618 - mse: 0.3758 - val_loss: 220.4123 - val_mae: 11.1904 - val_mse: 220.4123\n",
      "Epoch 2935/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.3256 - mae: 0.3268 - mse: 0.3256 - val_loss: 221.4337 - val_mae: 11.2347 - val_mse: 221.4337\n",
      "Epoch 2936/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.3954 - mae: 0.3145 - mse: 0.3954 - val_loss: 223.3027 - val_mae: 11.2345 - val_mse: 223.3027\n",
      "Epoch 2937/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3050 - mae: 0.3298 - mse: 0.3050 - val_loss: 216.8476 - val_mae: 11.1894 - val_mse: 216.8476\n",
      "Epoch 2938/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3389 - mae: 0.3475 - mse: 0.3389 - val_loss: 224.0060 - val_mae: 11.2486 - val_mse: 224.0060\n",
      "Epoch 2939/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3792 - mae: 0.3978 - mse: 0.3792 - val_loss: 222.8284 - val_mae: 11.3248 - val_mse: 222.8284\n",
      "Epoch 2940/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4571 - mae: 0.4297 - mse: 0.4571 - val_loss: 222.3568 - val_mae: 11.2604 - val_mse: 222.3568\n",
      "Epoch 2941/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4299 - mae: 0.3870 - mse: 0.4299 - val_loss: 218.7579 - val_mae: 11.2239 - val_mse: 218.7579\n",
      "Epoch 2942/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4321 - mae: 0.4206 - mse: 0.4321 - val_loss: 222.1013 - val_mae: 11.2518 - val_mse: 222.1013\n",
      "Epoch 2943/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3493 - mae: 0.3784 - mse: 0.3493 - val_loss: 222.3899 - val_mae: 11.2208 - val_mse: 222.3899\n",
      "Epoch 2944/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3373 - mae: 0.3325 - mse: 0.3373 - val_loss: 222.9542 - val_mae: 11.2596 - val_mse: 222.9542\n",
      "Epoch 2945/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4348 - mae: 0.4144 - mse: 0.4348 - val_loss: 221.5486 - val_mae: 11.1927 - val_mse: 221.5486\n",
      "Epoch 2946/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4332 - mae: 0.4150 - mse: 0.4332 - val_loss: 221.4006 - val_mae: 11.2085 - val_mse: 221.4006\n",
      "Epoch 2947/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3936 - mae: 0.3693 - mse: 0.3936 - val_loss: 219.6352 - val_mae: 11.2022 - val_mse: 219.6352\n",
      "Epoch 2948/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3665 - mae: 0.3765 - mse: 0.3665 - val_loss: 222.9404 - val_mae: 11.2185 - val_mse: 222.9404\n",
      "Epoch 2949/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4836 - mae: 0.4307 - mse: 0.4836 - val_loss: 219.8323 - val_mae: 11.2104 - val_mse: 219.8323\n",
      "Epoch 2950/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.5232 - mae: 0.4826 - mse: 0.5232 - val_loss: 222.0448 - val_mae: 11.2148 - val_mse: 222.0448\n",
      "Epoch 2951/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.7314 - mae: 0.6108 - mse: 0.7314 - val_loss: 226.8660 - val_mae: 11.3308 - val_mse: 226.8660\n",
      "Epoch 2952/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.5355 - mae: 0.5243 - mse: 0.5355 - val_loss: 225.8657 - val_mae: 11.3021 - val_mse: 225.8657\n",
      "Epoch 2953/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5106 - mae: 0.4634 - mse: 0.5106 - val_loss: 213.3137 - val_mae: 11.0733 - val_mse: 213.3137\n",
      "Epoch 2954/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.4802 - mae: 0.4840 - mse: 0.4802 - val_loss: 225.8569 - val_mae: 11.2640 - val_mse: 225.8569\n",
      "Epoch 2955/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.5102 - mae: 0.5002 - mse: 0.5102 - val_loss: 217.6607 - val_mae: 11.1934 - val_mse: 217.6607\n",
      "Epoch 2956/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4181 - mae: 0.4449 - mse: 0.4181 - val_loss: 221.5731 - val_mae: 11.2112 - val_mse: 221.5731\n",
      "Epoch 2957/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3564 - mae: 0.3405 - mse: 0.3564 - val_loss: 225.5946 - val_mae: 11.3232 - val_mse: 225.5946\n",
      "Epoch 2958/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.3295 - mae: 0.3715 - mse: 0.3295 - val_loss: 217.9765 - val_mae: 11.1563 - val_mse: 217.9765\n",
      "Epoch 2959/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4570 - mae: 0.4164 - mse: 0.4570 - val_loss: 226.2431 - val_mae: 11.3203 - val_mse: 226.2431\n",
      "Epoch 2960/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4484 - mae: 0.4500 - mse: 0.4484 - val_loss: 219.7516 - val_mae: 11.2503 - val_mse: 219.7516\n",
      "Epoch 2961/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3593 - mae: 0.3871 - mse: 0.3593 - val_loss: 221.3143 - val_mae: 11.1993 - val_mse: 221.3143\n",
      "Epoch 2962/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3715 - mae: 0.4081 - mse: 0.3715 - val_loss: 222.8389 - val_mae: 11.2923 - val_mse: 222.8389\n",
      "Epoch 2963/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3539 - mae: 0.3898 - mse: 0.3539 - val_loss: 222.3819 - val_mae: 11.2266 - val_mse: 222.3819\n",
      "Epoch 2964/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.3733 - mae: 0.3354 - mse: 0.3733 - val_loss: 222.1084 - val_mae: 11.2451 - val_mse: 222.1084\n",
      "Epoch 2965/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3691 - mae: 0.3850 - mse: 0.3691 - val_loss: 222.4267 - val_mae: 11.2873 - val_mse: 222.4267\n",
      "Epoch 2966/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4452 - mae: 0.4406 - mse: 0.4452 - val_loss: 223.9728 - val_mae: 11.1908 - val_mse: 223.9728\n",
      "Epoch 2967/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3915 - mae: 0.4232 - mse: 0.3915 - val_loss: 220.4726 - val_mae: 11.2343 - val_mse: 220.4726\n",
      "Epoch 2968/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3476 - mae: 0.3610 - mse: 0.3476 - val_loss: 222.0684 - val_mae: 11.2345 - val_mse: 222.0684\n",
      "Epoch 2969/3500\n",
      "126/126 [==============================] - 0s 158us/step - loss: 0.3177 - mae: 0.3099 - mse: 0.3177 - val_loss: 220.8043 - val_mae: 11.1893 - val_mse: 220.8043\n",
      "Epoch 2970/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.4149 - mae: 0.3432 - mse: 0.4149 - val_loss: 220.8040 - val_mae: 11.2184 - val_mse: 220.8040\n",
      "Epoch 2971/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.4056 - mae: 0.3926 - mse: 0.4056 - val_loss: 225.0179 - val_mae: 11.2477 - val_mse: 225.0179\n",
      "Epoch 2972/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4068 - mae: 0.3776 - mse: 0.4068 - val_loss: 216.4893 - val_mae: 11.0980 - val_mse: 216.4893\n",
      "Epoch 2973/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4058 - mae: 0.3812 - mse: 0.4058 - val_loss: 221.4687 - val_mae: 11.1579 - val_mse: 221.4687\n",
      "Epoch 2974/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3509 - mae: 0.3294 - mse: 0.3509 - val_loss: 224.2393 - val_mae: 11.2751 - val_mse: 224.2393\n",
      "Epoch 2975/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4184 - mae: 0.4154 - mse: 0.4184 - val_loss: 220.2326 - val_mae: 11.1747 - val_mse: 220.2326\n",
      "Epoch 2976/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.4155 - mae: 0.4335 - mse: 0.4155 - val_loss: 219.8662 - val_mae: 11.2182 - val_mse: 219.8662\n",
      "Epoch 2977/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4525 - mae: 0.4669 - mse: 0.4525 - val_loss: 222.5692 - val_mae: 11.2056 - val_mse: 222.5692\n",
      "Epoch 2978/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.4444 - mae: 0.4567 - mse: 0.4444 - val_loss: 218.3676 - val_mae: 11.1465 - val_mse: 218.3676\n",
      "Epoch 2979/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3931 - mae: 0.4220 - mse: 0.3931 - val_loss: 223.3257 - val_mae: 11.2098 - val_mse: 223.3257\n",
      "Epoch 2980/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.3515 - mae: 0.3486 - mse: 0.3515 - val_loss: 222.2308 - val_mae: 11.2229 - val_mse: 222.2308\n",
      "Epoch 2981/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.3694 - mae: 0.3830 - mse: 0.3694 - val_loss: 219.3849 - val_mae: 11.2070 - val_mse: 219.3849\n",
      "Epoch 2982/3500\n",
      "126/126 [==============================] - 0s 143us/step - loss: 0.4105 - mae: 0.4193 - mse: 0.4105 - val_loss: 222.7423 - val_mae: 11.2017 - val_mse: 222.7423\n",
      "Epoch 2983/3500\n",
      "126/126 [==============================] - 0s 166us/step - loss: 0.4137 - mae: 0.4147 - mse: 0.4137 - val_loss: 219.6770 - val_mae: 11.2041 - val_mse: 219.6770\n",
      "Epoch 2984/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.3856 - mae: 0.3844 - mse: 0.3856 - val_loss: 222.1946 - val_mae: 11.2299 - val_mse: 222.1946\n",
      "Epoch 2985/3500\n",
      "126/126 [==============================] - 0s 142us/step - loss: 0.3235 - mae: 0.3405 - mse: 0.3235 - val_loss: 222.1357 - val_mae: 11.2194 - val_mse: 222.1357\n",
      "Epoch 2986/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.3302 - mae: 0.3640 - mse: 0.3302 - val_loss: 217.6889 - val_mae: 11.1960 - val_mse: 217.6889\n",
      "Epoch 2987/3500\n",
      "126/126 [==============================] - 0s 237us/step - loss: 0.3337 - mae: 0.3370 - mse: 0.3337 - val_loss: 223.3965 - val_mae: 11.2244 - val_mse: 223.3965\n",
      "Epoch 2988/3500\n",
      "126/126 [==============================] - 0s 198us/step - loss: 0.3070 - mae: 0.3393 - mse: 0.3070 - val_loss: 217.3541 - val_mae: 11.1458 - val_mse: 217.3541\n",
      "Epoch 2989/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.4040 - mae: 0.4215 - mse: 0.4040 - val_loss: 226.1392 - val_mae: 11.2533 - val_mse: 226.1392\n",
      "Epoch 2990/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5105 - mae: 0.4870 - mse: 0.5105 - val_loss: 222.2024 - val_mae: 11.2037 - val_mse: 222.2024\n",
      "Epoch 2991/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4434 - mae: 0.4296 - mse: 0.4434 - val_loss: 219.6551 - val_mae: 11.2171 - val_mse: 219.6551\n",
      "Epoch 2992/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.5183 - mae: 0.4633 - mse: 0.5183 - val_loss: 222.7737 - val_mae: 11.2413 - val_mse: 222.7737\n",
      "Epoch 2993/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4722 - mae: 0.4791 - mse: 0.4722 - val_loss: 220.4551 - val_mae: 11.1743 - val_mse: 220.4551\n",
      "Epoch 2994/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.3368 - mae: 0.3591 - mse: 0.3368 - val_loss: 221.5021 - val_mae: 11.2002 - val_mse: 221.5021\n",
      "Epoch 2995/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.4028 - mae: 0.3823 - mse: 0.4028 - val_loss: 219.0608 - val_mae: 11.1472 - val_mse: 219.0608\n",
      "Epoch 2996/3500\n",
      "126/126 [==============================] - 0s 134us/step - loss: 0.3444 - mae: 0.3828 - mse: 0.3444 - val_loss: 223.2773 - val_mae: 11.2544 - val_mse: 223.2773\n",
      "Epoch 2997/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.4420 - mae: 0.4456 - mse: 0.4420 - val_loss: 222.4576 - val_mae: 11.2234 - val_mse: 222.4576\n",
      "Epoch 2998/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.3375 - mae: 0.3564 - mse: 0.3375 - val_loss: 218.8597 - val_mae: 11.1202 - val_mse: 218.8597\n",
      "Epoch 2999/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.4023 - mae: 0.4279 - mse: 0.4023 - val_loss: 221.8787 - val_mae: 11.2069 - val_mse: 221.8787\n",
      "Epoch 3000/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.4439 - mae: 0.4552 - mse: 0.4439 - val_loss: 222.5744 - val_mae: 11.2378 - val_mse: 222.5744\n",
      "Epoch 3001/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4260 - mae: 0.4358 - mse: 0.4260 - val_loss: 222.5891 - val_mae: 11.1463 - val_mse: 222.5891\n",
      "Epoch 3002/3500\n",
      "126/126 [==============================] - 0s 142us/step - loss: 0.4500 - mae: 0.4560 - mse: 0.4500 - val_loss: 219.9589 - val_mae: 11.2108 - val_mse: 219.9589\n",
      "Epoch 3003/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4098 - mae: 0.4299 - mse: 0.4098 - val_loss: 221.2789 - val_mae: 11.1880 - val_mse: 221.2789\n",
      "Epoch 3004/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.5298 - mae: 0.4948 - mse: 0.5298 - val_loss: 223.2988 - val_mae: 11.2082 - val_mse: 223.2988\n",
      "Epoch 3005/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.7945 - mae: 0.6800 - mse: 0.7945 - val_loss: 216.7896 - val_mae: 11.1885 - val_mse: 216.7896\n",
      "Epoch 3006/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.7135 - mae: 0.6111 - mse: 0.7135 - val_loss: 226.7151 - val_mae: 11.3312 - val_mse: 226.7151\n",
      "Epoch 3007/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.7142 - mae: 0.6004 - mse: 0.7142 - val_loss: 223.3128 - val_mae: 11.3153 - val_mse: 223.3128\n",
      "Epoch 3008/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.5260 - mae: 0.5179 - mse: 0.5260 - val_loss: 216.6835 - val_mae: 11.0724 - val_mse: 216.6835\n",
      "Epoch 3009/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4683 - mae: 0.4868 - mse: 0.4683 - val_loss: 228.2308 - val_mae: 11.3695 - val_mse: 228.2308\n",
      "Epoch 3010/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4573 - mae: 0.4617 - mse: 0.4573 - val_loss: 217.6396 - val_mae: 11.1744 - val_mse: 217.6396\n",
      "Epoch 3011/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.6246 - mae: 0.5994 - mse: 0.6246 - val_loss: 226.7516 - val_mae: 11.2518 - val_mse: 226.7516\n",
      "Epoch 3012/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.7798 - mae: 0.6678 - mse: 0.7798 - val_loss: 221.2235 - val_mae: 11.2955 - val_mse: 221.2235\n",
      "Epoch 3013/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.5586 - mae: 0.5516 - mse: 0.5586 - val_loss: 216.9325 - val_mae: 11.0976 - val_mse: 216.9325\n",
      "Epoch 3014/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5431 - mae: 0.5274 - mse: 0.5431 - val_loss: 220.9092 - val_mae: 11.1957 - val_mse: 220.9092\n",
      "Epoch 3015/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4909 - mae: 0.4874 - mse: 0.4909 - val_loss: 219.4661 - val_mae: 11.0769 - val_mse: 219.4661\n",
      "Epoch 3016/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4228 - mae: 0.4258 - mse: 0.4228 - val_loss: 223.3746 - val_mae: 11.2054 - val_mse: 223.3746\n",
      "Epoch 3017/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4595 - mae: 0.4851 - mse: 0.4595 - val_loss: 223.1917 - val_mae: 11.2615 - val_mse: 223.1917\n",
      "Epoch 3018/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5275 - mae: 0.5098 - mse: 0.5275 - val_loss: 225.1627 - val_mae: 11.1799 - val_mse: 225.1627\n",
      "Epoch 3019/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6417 - mae: 0.5978 - mse: 0.6417 - val_loss: 216.0164 - val_mae: 11.1448 - val_mse: 216.0164\n",
      "Epoch 3020/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.9247 - mae: 0.7052 - mse: 0.9247 - val_loss: 228.8074 - val_mae: 11.3004 - val_mse: 228.8074\n",
      "Epoch 3021/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.8652 - mae: 0.9396 - mse: 1.8652 - val_loss: 217.8899 - val_mae: 11.1430 - val_mse: 217.8899\n",
      "Epoch 3022/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.5610 - mae: 0.9258 - mse: 1.5610 - val_loss: 223.6145 - val_mae: 11.3731 - val_mse: 223.6145\n",
      "Epoch 3023/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.8807 - mae: 0.9889 - mse: 1.8807 - val_loss: 227.4372 - val_mae: 11.3400 - val_mse: 227.4372\n",
      "Epoch 3024/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 1.7142 - mae: 0.8591 - mse: 1.7142 - val_loss: 222.0712 - val_mae: 11.2031 - val_mse: 222.0712\n",
      "Epoch 3025/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.5811 - mae: 0.9203 - mse: 1.5811 - val_loss: 225.6801 - val_mae: 11.2898 - val_mse: 225.6801\n",
      "Epoch 3026/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.3153 - mae: 0.8540 - mse: 1.3153 - val_loss: 223.6108 - val_mae: 11.2112 - val_mse: 223.6108\n",
      "Epoch 3027/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.4196 - mae: 0.7609 - mse: 1.4196 - val_loss: 226.2658 - val_mae: 11.3114 - val_mse: 226.2658\n",
      "Epoch 3028/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 2.5320 - mae: 1.0550 - mse: 2.5320 - val_loss: 224.0634 - val_mae: 11.2689 - val_mse: 224.0634\n",
      "Epoch 3029/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 1.8608 - mae: 1.0230 - mse: 1.8608 - val_loss: 214.3525 - val_mae: 11.0694 - val_mse: 214.3525\n",
      "Epoch 3030/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 2.1238 - mae: 1.1728 - mse: 2.1238 - val_loss: 214.5507 - val_mae: 11.2281 - val_mse: 214.5507\n",
      "Epoch 3031/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.4665 - mae: 0.8904 - mse: 1.4665 - val_loss: 248.5641 - val_mae: 11.7604 - val_mse: 248.5641\n",
      "Epoch 3032/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 1.2485 - mae: 0.7844 - mse: 1.2485 - val_loss: 213.6857 - val_mae: 11.0902 - val_mse: 213.6857\n",
      "Epoch 3033/3500\n",
      "126/126 [==============================] - 0s 182us/step - loss: 1.0719 - mae: 0.8072 - mse: 1.0719 - val_loss: 218.0536 - val_mae: 11.1100 - val_mse: 218.0536\n",
      "Epoch 3034/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.9949 - mae: 0.7250 - mse: 0.9949 - val_loss: 215.5965 - val_mae: 11.0770 - val_mse: 215.5965\n",
      "Epoch 3035/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.8944 - mae: 0.6939 - mse: 0.8944 - val_loss: 212.4059 - val_mae: 11.0717 - val_mse: 212.4059\n",
      "Epoch 3036/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.7744 - mae: 0.6466 - mse: 0.7744 - val_loss: 224.1057 - val_mae: 11.3000 - val_mse: 224.1057\n",
      "Epoch 3037/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.5729 - mae: 0.5613 - mse: 0.5729 - val_loss: 218.0643 - val_mae: 11.1454 - val_mse: 218.0643\n",
      "Epoch 3038/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.4150 - mae: 0.4649 - mse: 0.415 - 0s 95us/step - loss: 0.5163 - mae: 0.5023 - mse: 0.5163 - val_loss: 219.1566 - val_mae: 11.1789 - val_mse: 219.1566\n",
      "Epoch 3039/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4364 - mae: 0.4147 - mse: 0.4364 - val_loss: 218.4832 - val_mae: 11.1270 - val_mse: 218.4832\n",
      "Epoch 3040/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4780 - mae: 0.4836 - mse: 0.4780 - val_loss: 219.1255 - val_mae: 11.1650 - val_mse: 219.1255\n",
      "Epoch 3041/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4880 - mae: 0.4649 - mse: 0.4880 - val_loss: 219.9033 - val_mae: 11.2585 - val_mse: 219.9033\n",
      "Epoch 3042/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.3526 - mae: 0.3817 - mse: 0.3526 - val_loss: 221.2151 - val_mae: 11.1663 - val_mse: 221.2151\n",
      "Epoch 3043/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4716 - mae: 0.4486 - mse: 0.4716 - val_loss: 216.5018 - val_mae: 11.2021 - val_mse: 216.5018\n",
      "Epoch 3044/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5985 - mae: 0.5424 - mse: 0.5985 - val_loss: 224.4397 - val_mae: 11.2962 - val_mse: 224.4397\n",
      "Epoch 3045/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.4867 - mae: 0.4637 - mse: 0.4867 - val_loss: 217.9241 - val_mae: 11.1788 - val_mse: 217.9241\n",
      "Epoch 3046/3500\n",
      "126/126 [==============================] - 0s 158us/step - loss: 0.5634 - mae: 0.4763 - mse: 0.5634 - val_loss: 220.4630 - val_mae: 11.2365 - val_mse: 220.4630\n",
      "Epoch 3047/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4682 - mae: 0.4806 - mse: 0.4682 - val_loss: 221.6497 - val_mae: 11.2183 - val_mse: 221.6497\n",
      "Epoch 3048/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4884 - mae: 0.4409 - mse: 0.4884 - val_loss: 218.3895 - val_mae: 11.1294 - val_mse: 218.3895\n",
      "Epoch 3049/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.5022 - mae: 0.4378 - mse: 0.5022 - val_loss: 220.5461 - val_mae: 11.1669 - val_mse: 220.5461\n",
      "Epoch 3050/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.7479 - mae: 0.5861 - mse: 0.7479 - val_loss: 218.4712 - val_mae: 11.2142 - val_mse: 218.4712\n",
      "Epoch 3051/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.6665 - mae: 0.5547 - mse: 0.6665 - val_loss: 221.3784 - val_mae: 11.1916 - val_mse: 221.3784\n",
      "Epoch 3052/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6022 - mae: 0.5365 - mse: 0.6022 - val_loss: 208.6841 - val_mae: 11.0657 - val_mse: 208.6841\n",
      "Epoch 3053/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.7830 - mae: 0.6358 - mse: 0.7830 - val_loss: 228.7493 - val_mae: 11.3147 - val_mse: 228.7493\n",
      "Epoch 3054/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.6459 - mae: 0.5874 - mse: 0.6459 - val_loss: 223.0357 - val_mae: 11.3182 - val_mse: 223.0357\n",
      "Epoch 3055/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5479 - mae: 0.4924 - mse: 0.5479 - val_loss: 220.4464 - val_mae: 11.2227 - val_mse: 220.4464\n",
      "Epoch 3056/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5437 - mae: 0.4729 - mse: 0.5437 - val_loss: 224.0975 - val_mae: 11.1873 - val_mse: 224.0975\n",
      "Epoch 3057/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.5084 - mae: 0.5023 - mse: 0.5084 - val_loss: 217.7226 - val_mae: 11.1726 - val_mse: 217.7226\n",
      "Epoch 3058/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.3543 - mae: 0.3973 - mse: 0.3543 - val_loss: 226.2096 - val_mae: 11.2576 - val_mse: 226.2096\n",
      "Epoch 3059/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4312 - mae: 0.4222 - mse: 0.4312 - val_loss: 216.3246 - val_mae: 11.1240 - val_mse: 216.3246\n",
      "Epoch 3060/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3799 - mae: 0.3816 - mse: 0.3799 - val_loss: 217.3753 - val_mae: 11.0938 - val_mse: 217.3753\n",
      "Epoch 3061/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.3545 - mae: 0.3249 - mse: 0.3545 - val_loss: 223.0118 - val_mae: 11.2341 - val_mse: 223.0118\n",
      "Epoch 3062/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3595 - mae: 0.3731 - mse: 0.3595 - val_loss: 218.7001 - val_mae: 11.1572 - val_mse: 218.7001\n",
      "Epoch 3063/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3110 - mae: 0.3183 - mse: 0.3110 - val_loss: 219.6559 - val_mae: 11.1330 - val_mse: 219.6559\n",
      "Epoch 3064/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2832 - mae: 0.3040 - mse: 0.2832 - val_loss: 217.2835 - val_mae: 11.1301 - val_mse: 217.2835\n",
      "Epoch 3065/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2879 - mae: 0.2881 - mse: 0.2879 - val_loss: 220.1785 - val_mae: 11.1413 - val_mse: 220.1785\n",
      "Epoch 3066/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2565 - mae: 0.2582 - mse: 0.2565 - val_loss: 219.9344 - val_mae: 11.1982 - val_mse: 219.9344\n",
      "Epoch 3067/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2788 - mae: 0.2405 - mse: 0.2788 - val_loss: 221.2462 - val_mae: 11.1732 - val_mse: 221.2462\n",
      "Epoch 3068/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2706 - mae: 0.2886 - mse: 0.2706 - val_loss: 218.2595 - val_mae: 11.1703 - val_mse: 218.2595\n",
      "Epoch 3069/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3344 - mae: 0.3394 - mse: 0.3344 - val_loss: 217.0057 - val_mae: 11.1168 - val_mse: 217.0057\n",
      "Epoch 3070/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3174 - mae: 0.3006 - mse: 0.3174 - val_loss: 222.1650 - val_mae: 11.1823 - val_mse: 222.1650\n",
      "Epoch 3071/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3053 - mae: 0.3194 - mse: 0.3053 - val_loss: 218.1755 - val_mae: 11.2335 - val_mse: 218.1755\n",
      "Epoch 3072/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3103 - mae: 0.3176 - mse: 0.3103 - val_loss: 222.8595 - val_mae: 11.1904 - val_mse: 222.8595\n",
      "Epoch 3073/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2724 - mae: 0.2931 - mse: 0.2724 - val_loss: 218.4693 - val_mae: 11.1744 - val_mse: 218.4693\n",
      "Epoch 3074/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2827 - mae: 0.3046 - mse: 0.2827 - val_loss: 219.6486 - val_mae: 11.1489 - val_mse: 219.6486\n",
      "Epoch 3075/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2735 - mae: 0.2794 - mse: 0.2735 - val_loss: 220.4360 - val_mae: 11.1728 - val_mse: 220.4360\n",
      "Epoch 3076/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.2557 - mae: 0.2483 - mse: 0.2557 - val_loss: 219.0152 - val_mae: 11.1754 - val_mse: 219.0152\n",
      "Epoch 3077/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.2279 - mae: 0.2031 - mse: 0.2279 - val_loss: 219.3524 - val_mae: 11.1672 - val_mse: 219.3524\n",
      "Epoch 3078/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2485 - mae: 0.2182 - mse: 0.2485 - val_loss: 221.0060 - val_mae: 11.2038 - val_mse: 221.0060\n",
      "Epoch 3079/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3118 - mae: 0.3017 - mse: 0.3118 - val_loss: 219.6115 - val_mae: 11.1862 - val_mse: 219.6115\n",
      "Epoch 3080/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3347 - mae: 0.3298 - mse: 0.3347 - val_loss: 222.9968 - val_mae: 11.1980 - val_mse: 222.9968\n",
      "Epoch 3081/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3282 - mae: 0.3334 - mse: 0.3282 - val_loss: 220.1702 - val_mae: 11.2171 - val_mse: 220.1702\n",
      "Epoch 3082/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3179 - mae: 0.3511 - mse: 0.3179 - val_loss: 217.6197 - val_mae: 11.1104 - val_mse: 217.6197\n",
      "Epoch 3083/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3825 - mae: 0.3908 - mse: 0.3825 - val_loss: 221.5485 - val_mae: 11.1555 - val_mse: 221.5485\n",
      "Epoch 3084/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3668 - mae: 0.4034 - mse: 0.3668 - val_loss: 219.1429 - val_mae: 11.2202 - val_mse: 219.1429\n",
      "Epoch 3085/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4123 - mae: 0.4355 - mse: 0.4123 - val_loss: 221.0310 - val_mae: 11.1670 - val_mse: 221.0310\n",
      "Epoch 3086/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3555 - mae: 0.3972 - mse: 0.3555 - val_loss: 219.4066 - val_mae: 11.2298 - val_mse: 219.4066\n",
      "Epoch 3087/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4063 - mae: 0.4415 - mse: 0.4063 - val_loss: 226.3009 - val_mae: 11.2714 - val_mse: 226.3009\n",
      "Epoch 3088/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4631 - mae: 0.4494 - mse: 0.4631 - val_loss: 220.1588 - val_mae: 11.1847 - val_mse: 220.1588\n",
      "Epoch 3089/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3390 - mae: 0.3875 - mse: 0.3390 - val_loss: 220.4905 - val_mae: 11.1980 - val_mse: 220.4905\n",
      "Epoch 3090/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3724 - mae: 0.3718 - mse: 0.3724 - val_loss: 221.4364 - val_mae: 11.1770 - val_mse: 221.4364\n",
      "Epoch 3091/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4406 - mae: 0.4693 - mse: 0.4406 - val_loss: 218.8035 - val_mae: 11.2284 - val_mse: 218.8035\n",
      "Epoch 3092/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4741 - mae: 0.4658 - mse: 0.4741 - val_loss: 228.6022 - val_mae: 11.3369 - val_mse: 228.6022\n",
      "Epoch 3093/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6194 - mae: 0.5491 - mse: 0.6194 - val_loss: 216.0318 - val_mae: 11.0825 - val_mse: 216.0318\n",
      "Epoch 3094/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5628 - mae: 0.5351 - mse: 0.5628 - val_loss: 219.5323 - val_mae: 11.1437 - val_mse: 219.5323\n",
      "Epoch 3095/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5113 - mae: 0.4571 - mse: 0.5113 - val_loss: 225.0907 - val_mae: 11.2714 - val_mse: 225.0907\n",
      "Epoch 3096/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3640 - mae: 0.3918 - mse: 0.3640 - val_loss: 216.1000 - val_mae: 11.1413 - val_mse: 216.1000\n",
      "Epoch 3097/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4838 - mae: 0.4470 - mse: 0.4838 - val_loss: 224.8889 - val_mae: 11.2543 - val_mse: 224.8889\n",
      "Epoch 3098/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6178 - mae: 0.5590 - mse: 0.6178 - val_loss: 215.9486 - val_mae: 11.0980 - val_mse: 215.9486\n",
      "Epoch 3099/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6322 - mae: 0.5772 - mse: 0.6322 - val_loss: 216.9743 - val_mae: 11.1141 - val_mse: 216.9743\n",
      "Epoch 3100/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5291 - mae: 0.5046 - mse: 0.5291 - val_loss: 225.0864 - val_mae: 11.2727 - val_mse: 225.0864\n",
      "Epoch 3101/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4948 - mae: 0.4674 - mse: 0.4948 - val_loss: 219.3282 - val_mae: 11.1421 - val_mse: 219.3282\n",
      "Epoch 3102/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4438 - mae: 0.4433 - mse: 0.4438 - val_loss: 220.6435 - val_mae: 11.1679 - val_mse: 220.6435\n",
      "Epoch 3103/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7264 - mae: 0.6080 - mse: 0.7264 - val_loss: 218.8503 - val_mae: 11.1520 - val_mse: 218.8503\n",
      "Epoch 3104/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.7065 - mae: 0.6108 - mse: 0.7065 - val_loss: 220.9052 - val_mae: 11.1874 - val_mse: 220.9052\n",
      "Epoch 3105/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5720 - mae: 0.5320 - mse: 0.5720 - val_loss: 228.0770 - val_mae: 11.3261 - val_mse: 228.0770\n",
      "Epoch 3106/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5160 - mae: 0.5066 - mse: 0.5160 - val_loss: 216.0415 - val_mae: 11.0858 - val_mse: 216.0415\n",
      "Epoch 3107/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5656 - mae: 0.5351 - mse: 0.5656 - val_loss: 226.1490 - val_mae: 11.2600 - val_mse: 226.1490\n",
      "Epoch 3108/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5573 - mae: 0.5422 - mse: 0.5573 - val_loss: 221.0779 - val_mae: 11.2797 - val_mse: 221.0779\n",
      "Epoch 3109/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5655 - mae: 0.5549 - mse: 0.5655 - val_loss: 222.8385 - val_mae: 11.2169 - val_mse: 222.8385\n",
      "Epoch 3110/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.5693 - mae: 0.5396 - mse: 0.5693 - val_loss: 221.1915 - val_mae: 11.3087 - val_mse: 221.1915\n",
      "Epoch 3111/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7352 - mae: 0.6412 - mse: 0.7352 - val_loss: 222.6506 - val_mae: 11.1771 - val_mse: 222.6506\n",
      "Epoch 3112/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.7848 - mae: 0.6577 - mse: 0.7848 - val_loss: 227.2026 - val_mae: 11.3640 - val_mse: 227.2026\n",
      "Epoch 3113/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.6289 - mae: 0.5895 - mse: 0.6289 - val_loss: 217.3346 - val_mae: 11.1405 - val_mse: 217.3346\n",
      "Epoch 3114/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4872 - mae: 0.5017 - mse: 0.4872 - val_loss: 220.9137 - val_mae: 11.2227 - val_mse: 220.9137\n",
      "Epoch 3115/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5519 - mae: 0.4852 - mse: 0.5519 - val_loss: 229.3499 - val_mae: 11.3846 - val_mse: 229.3499\n",
      "Epoch 3116/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5053 - mae: 0.4908 - mse: 0.5053 - val_loss: 217.7422 - val_mae: 11.1459 - val_mse: 217.7422\n",
      "Epoch 3117/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6456 - mae: 0.5702 - mse: 0.6456 - val_loss: 220.5590 - val_mae: 11.1104 - val_mse: 220.5590\n",
      "Epoch 3118/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.7269 - mae: 0.6215 - mse: 0.7269 - val_loss: 224.5321 - val_mae: 11.2732 - val_mse: 224.5321\n",
      "Epoch 3119/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7294 - mae: 0.6414 - mse: 0.7294 - val_loss: 216.4492 - val_mae: 11.1477 - val_mse: 216.4492\n",
      "Epoch 3120/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5139 - mae: 0.5261 - mse: 0.5139 - val_loss: 221.6906 - val_mae: 11.1836 - val_mse: 221.6906\n",
      "Epoch 3121/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4979 - mae: 0.5017 - mse: 0.4979 - val_loss: 219.5168 - val_mae: 11.2206 - val_mse: 219.5168\n",
      "Epoch 3122/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4578 - mae: 0.4698 - mse: 0.4578 - val_loss: 216.5073 - val_mae: 11.0902 - val_mse: 216.5073\n",
      "Epoch 3123/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.4196 - mae: 0.4222 - mse: 0.4196 - val_loss: 221.5915 - val_mae: 11.1473 - val_mse: 221.5915\n",
      "Epoch 3124/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.5354 - mae: 0.5008 - mse: 0.5354 - val_loss: 218.2017 - val_mae: 11.1705 - val_mse: 218.2017\n",
      "Epoch 3125/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4457 - mae: 0.4432 - mse: 0.4457 - val_loss: 222.1930 - val_mae: 11.2522 - val_mse: 222.1930\n",
      "Epoch 3126/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5077 - mae: 0.4398 - mse: 0.5077 - val_loss: 220.0389 - val_mae: 11.1961 - val_mse: 220.0389\n",
      "Epoch 3127/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.7120 - mae: 0.6575 - mse: 0.7120 - val_loss: 220.4660 - val_mae: 11.1345 - val_mse: 220.4660\n",
      "Epoch 3128/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6943 - mae: 0.5888 - mse: 0.6943 - val_loss: 215.7328 - val_mae: 11.1489 - val_mse: 215.7328\n",
      "Epoch 3129/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6803 - mae: 0.5867 - mse: 0.6803 - val_loss: 217.2648 - val_mae: 11.0818 - val_mse: 217.2648\n",
      "Epoch 3130/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.7183 - mae: 0.6028 - mse: 0.7183 - val_loss: 222.2240 - val_mae: 11.1423 - val_mse: 222.2240\n",
      "Epoch 3131/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6224 - mae: 0.5297 - mse: 0.6224 - val_loss: 216.5649 - val_mae: 11.1583 - val_mse: 216.5649\n",
      "Epoch 3132/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5676 - mae: 0.5701 - mse: 0.5676 - val_loss: 224.9658 - val_mae: 11.2421 - val_mse: 224.9658\n",
      "Epoch 3133/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4490 - mae: 0.4764 - mse: 0.4490 - val_loss: 218.1174 - val_mae: 11.1749 - val_mse: 218.1174\n",
      "Epoch 3134/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4465 - mae: 0.4741 - mse: 0.4465 - val_loss: 220.5922 - val_mae: 11.1411 - val_mse: 220.5922\n",
      "Epoch 3135/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3724 - mae: 0.3979 - mse: 0.3724 - val_loss: 215.9804 - val_mae: 11.1325 - val_mse: 215.9804\n",
      "Epoch 3136/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4109 - mae: 0.3924 - mse: 0.4109 - val_loss: 223.8234 - val_mae: 11.2547 - val_mse: 223.8234\n",
      "Epoch 3137/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3763 - mae: 0.3729 - mse: 0.3763 - val_loss: 220.2502 - val_mae: 11.1882 - val_mse: 220.2502\n",
      "Epoch 3138/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3690 - mae: 0.3583 - mse: 0.3690 - val_loss: 221.0241 - val_mae: 11.2088 - val_mse: 221.0241\n",
      "Epoch 3139/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3027 - mae: 0.3158 - mse: 0.3027 - val_loss: 221.9701 - val_mae: 11.1737 - val_mse: 221.9701\n",
      "Epoch 3140/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3700 - mae: 0.3661 - mse: 0.3700 - val_loss: 215.9421 - val_mae: 11.0783 - val_mse: 215.9421\n",
      "Epoch 3141/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3033 - mae: 0.3273 - mse: 0.3033 - val_loss: 222.0613 - val_mae: 11.2018 - val_mse: 222.0613\n",
      "Epoch 3142/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2988 - mae: 0.3351 - mse: 0.2988 - val_loss: 217.2202 - val_mae: 11.1250 - val_mse: 217.2202\n",
      "Epoch 3143/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3240 - mae: 0.3290 - mse: 0.3240 - val_loss: 220.0276 - val_mae: 11.1633 - val_mse: 220.0276\n",
      "Epoch 3144/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3506 - mae: 0.3553 - mse: 0.3506 - val_loss: 221.6400 - val_mae: 11.1615 - val_mse: 221.6400\n",
      "Epoch 3145/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3492 - mae: 0.3644 - mse: 0.3492 - val_loss: 215.4184 - val_mae: 11.0680 - val_mse: 215.4184\n",
      "Epoch 3146/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3125 - mae: 0.3190 - mse: 0.3125 - val_loss: 223.3114 - val_mae: 11.1798 - val_mse: 223.3114\n",
      "Epoch 3147/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3680 - mae: 0.3364 - mse: 0.3680 - val_loss: 217.5223 - val_mae: 11.1297 - val_mse: 217.5223\n",
      "Epoch 3148/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3243 - mae: 0.3152 - mse: 0.3243 - val_loss: 220.0912 - val_mae: 11.2104 - val_mse: 220.0912\n",
      "Epoch 3149/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2823 - mae: 0.2996 - mse: 0.2823 - val_loss: 224.2339 - val_mae: 11.2140 - val_mse: 224.2339\n",
      "Epoch 3150/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3075 - mae: 0.3282 - mse: 0.3075 - val_loss: 212.9529 - val_mae: 11.0284 - val_mse: 212.9529\n",
      "Epoch 3151/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.4187 - mae: 0.4284 - mse: 0.4187 - val_loss: 225.4815 - val_mae: 11.2387 - val_mse: 225.4815\n",
      "Epoch 3152/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5057 - mae: 0.5148 - mse: 0.5057 - val_loss: 219.8371 - val_mae: 11.2573 - val_mse: 219.8371\n",
      "Epoch 3153/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4800 - mae: 0.4438 - mse: 0.4800 - val_loss: 221.1158 - val_mae: 11.1667 - val_mse: 221.1158\n",
      "Epoch 3154/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4144 - mae: 0.4045 - mse: 0.4144 - val_loss: 223.2426 - val_mae: 11.2573 - val_mse: 223.2426\n",
      "Epoch 3155/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4055 - mae: 0.4170 - mse: 0.4055 - val_loss: 216.6768 - val_mae: 11.1011 - val_mse: 216.6768\n",
      "Epoch 3156/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4855 - mae: 0.4626 - mse: 0.4855 - val_loss: 223.7996 - val_mae: 11.1873 - val_mse: 223.7996\n",
      "Epoch 3157/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5255 - mae: 0.5000 - mse: 0.5255 - val_loss: 217.8622 - val_mae: 11.1084 - val_mse: 217.8622\n",
      "Epoch 3158/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.4135 - mae: 0.4237 - mse: 0.4135 - val_loss: 218.3666 - val_mae: 11.1162 - val_mse: 218.3666\n",
      "Epoch 3159/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.3322 - mae: 0.3257 - mse: 0.3322 - val_loss: 220.4882 - val_mae: 11.1523 - val_mse: 220.4882\n",
      "Epoch 3160/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.3484 - mae: 0.3698 - mse: 0.3484 - val_loss: 218.4615 - val_mae: 11.1655 - val_mse: 218.4615\n",
      "Epoch 3161/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.3578 - mae: 0.3902 - mse: 0.3578 - val_loss: 222.6670 - val_mae: 11.1972 - val_mse: 222.6670\n",
      "Epoch 3162/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3470 - mae: 0.3605 - mse: 0.3470 - val_loss: 216.1333 - val_mae: 11.0716 - val_mse: 216.1333\n",
      "Epoch 3163/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4680 - mae: 0.4162 - mse: 0.4680 - val_loss: 221.2305 - val_mae: 11.1718 - val_mse: 221.2305\n",
      "Epoch 3164/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3614 - mae: 0.3941 - mse: 0.3614 - val_loss: 218.1207 - val_mae: 11.1112 - val_mse: 218.1207\n",
      "Epoch 3165/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4217 - mae: 0.3821 - mse: 0.4217 - val_loss: 215.8594 - val_mae: 11.1110 - val_mse: 215.8594\n",
      "Epoch 3166/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3545 - mae: 0.3758 - mse: 0.3545 - val_loss: 221.9226 - val_mae: 11.1579 - val_mse: 221.9226\n",
      "Epoch 3167/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3503 - mae: 0.3322 - mse: 0.3503 - val_loss: 218.3630 - val_mae: 11.1460 - val_mse: 218.3630\n",
      "Epoch 3168/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3529 - mae: 0.3827 - mse: 0.3529 - val_loss: 219.5226 - val_mae: 11.1277 - val_mse: 219.5226\n",
      "Epoch 3169/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3320 - mae: 0.3458 - mse: 0.3320 - val_loss: 218.0026 - val_mae: 11.0400 - val_mse: 218.0026\n",
      "Epoch 3170/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3346 - mae: 0.3745 - mse: 0.3346 - val_loss: 221.6417 - val_mae: 11.2242 - val_mse: 221.6417\n",
      "Epoch 3171/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3257 - mae: 0.3369 - mse: 0.3257 - val_loss: 220.7097 - val_mae: 11.1903 - val_mse: 220.7097\n",
      "Epoch 3172/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3007 - mae: 0.2910 - mse: 0.3007 - val_loss: 216.9244 - val_mae: 11.0691 - val_mse: 216.9244\n",
      "Epoch 3173/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2683 - mae: 0.2956 - mse: 0.2683 - val_loss: 219.2812 - val_mae: 11.1484 - val_mse: 219.2812\n",
      "Epoch 3174/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2714 - mae: 0.2733 - mse: 0.2714 - val_loss: 221.3798 - val_mae: 11.1600 - val_mse: 221.3798\n",
      "Epoch 3175/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3492 - mae: 0.3911 - mse: 0.3492 - val_loss: 220.2890 - val_mae: 11.1809 - val_mse: 220.2890\n",
      "Epoch 3176/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.3764 - mae: 0.3639 - mse: 0.3764 - val_loss: 220.0534 - val_mae: 11.1573 - val_mse: 220.0534\n",
      "Epoch 3177/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.3759 - mae: 0.3877 - mse: 0.3759 - val_loss: 219.5011 - val_mae: 11.0789 - val_mse: 219.5011\n",
      "Epoch 3178/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4400 - mae: 0.4039 - mse: 0.4400 - val_loss: 214.3380 - val_mae: 11.0621 - val_mse: 214.3380\n",
      "Epoch 3179/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3927 - mae: 0.4146 - mse: 0.3927 - val_loss: 222.5930 - val_mae: 11.1358 - val_mse: 222.5930\n",
      "Epoch 3180/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3835 - mae: 0.3926 - mse: 0.3835 - val_loss: 217.8524 - val_mae: 11.0906 - val_mse: 217.8524\n",
      "Epoch 3181/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4705 - mae: 0.3983 - mse: 0.4705 - val_loss: 221.4042 - val_mae: 11.1804 - val_mse: 221.4042\n",
      "Epoch 3182/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5595 - mae: 0.4785 - mse: 0.5595 - val_loss: 219.7814 - val_mae: 11.1114 - val_mse: 219.7814\n",
      "Epoch 3183/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.5597 - mae: 0.5710 - mse: 0.5597 - val_loss: 214.9287 - val_mae: 11.0974 - val_mse: 214.9287\n",
      "Epoch 3184/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6897 - mae: 0.5956 - mse: 0.6897 - val_loss: 224.3918 - val_mae: 11.1595 - val_mse: 224.3918\n",
      "Epoch 3185/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6658 - mae: 0.6122 - mse: 0.6658 - val_loss: 219.4467 - val_mae: 11.2083 - val_mse: 219.4467\n",
      "Epoch 3186/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.7037 - mae: 0.5389 - mse: 0.7037 - val_loss: 225.3654 - val_mae: 11.2080 - val_mse: 225.3654\n",
      "Epoch 3187/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5686 - mae: 0.4167 - mse: 0.5686 - val_loss: 223.2819 - val_mae: 11.1907 - val_mse: 223.2819\n",
      "Epoch 3188/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.5190 - mae: 0.4955 - mse: 0.5190 - val_loss: 218.1421 - val_mae: 11.1751 - val_mse: 218.1421\n",
      "Epoch 3189/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.7064 - mae: 0.6346 - mse: 0.7064 - val_loss: 220.5450 - val_mae: 11.0945 - val_mse: 220.5450\n",
      "Epoch 3190/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6852 - mae: 0.6085 - mse: 0.6852 - val_loss: 221.9156 - val_mae: 11.2245 - val_mse: 221.9156\n",
      "Epoch 3191/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6601 - mae: 0.5705 - mse: 0.6601 - val_loss: 215.8146 - val_mae: 11.0765 - val_mse: 215.8146\n",
      "Epoch 3192/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5963 - mae: 0.5315 - mse: 0.5963 - val_loss: 223.8889 - val_mae: 11.1707 - val_mse: 223.8889\n",
      "Epoch 3193/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6377 - mae: 0.6273 - mse: 0.6377 - val_loss: 216.8073 - val_mae: 11.1724 - val_mse: 216.8073\n",
      "Epoch 3194/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.7461 - mae: 0.6681 - mse: 0.7461 - val_loss: 223.2356 - val_mae: 11.1562 - val_mse: 223.2356\n",
      "Epoch 3195/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.7714 - mae: 0.6664 - mse: 0.7714 - val_loss: 211.2291 - val_mae: 11.0648 - val_mse: 211.2291\n",
      "Epoch 3196/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.7359 - mae: 0.5726 - mse: 0.7359 - val_loss: 220.9228 - val_mae: 11.1494 - val_mse: 220.9228\n",
      "Epoch 3197/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6202 - mae: 0.5568 - mse: 0.6202 - val_loss: 220.6415 - val_mae: 11.1471 - val_mse: 220.6415\n",
      "Epoch 3198/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6205 - mae: 0.5899 - mse: 0.6205 - val_loss: 222.5065 - val_mae: 11.2598 - val_mse: 222.5065\n",
      "Epoch 3199/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.6243 - mae: 0.5881 - mse: 0.6243 - val_loss: 220.8640 - val_mae: 11.0601 - val_mse: 220.8640\n",
      "Epoch 3200/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4823 - mae: 0.4796 - mse: 0.4823 - val_loss: 211.6782 - val_mae: 10.9808 - val_mse: 211.6782\n",
      "Epoch 3201/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3933 - mae: 0.4161 - mse: 0.3933 - val_loss: 220.0672 - val_mae: 11.1767 - val_mse: 220.0672\n",
      "Epoch 3202/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3360 - mae: 0.3514 - mse: 0.3360 - val_loss: 217.4471 - val_mae: 11.1287 - val_mse: 217.4471\n",
      "Epoch 3203/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3924 - mae: 0.4203 - mse: 0.3924 - val_loss: 221.4311 - val_mae: 11.2101 - val_mse: 221.4311\n",
      "Epoch 3204/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4365 - mae: 0.4355 - mse: 0.4365 - val_loss: 216.8314 - val_mae: 11.0897 - val_mse: 216.8314\n",
      "Epoch 3205/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.3140 - mae: 0.3477 - mse: 0.3140 - val_loss: 220.1308 - val_mae: 11.1286 - val_mse: 220.1308\n",
      "Epoch 3206/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4089 - mae: 0.3871 - mse: 0.4089 - val_loss: 225.0301 - val_mae: 11.2861 - val_mse: 225.0301\n",
      "Epoch 3207/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4731 - mae: 0.4433 - mse: 0.4731 - val_loss: 218.4952 - val_mae: 11.1318 - val_mse: 218.4952\n",
      "Epoch 3208/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.5014 - mae: 0.4807 - mse: 0.5014 - val_loss: 220.4206 - val_mae: 11.1795 - val_mse: 220.4206\n",
      "Epoch 3209/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4371 - mae: 0.4526 - mse: 0.4371 - val_loss: 215.1844 - val_mae: 11.0557 - val_mse: 215.1844\n",
      "Epoch 3210/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4851 - mae: 0.4360 - mse: 0.4851 - val_loss: 220.2873 - val_mae: 11.1354 - val_mse: 220.2873\n",
      "Epoch 3211/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.7844 - mae: 0.5397 - mse: 0.7844 - val_loss: 217.9387 - val_mae: 11.1771 - val_mse: 217.9387\n",
      "Epoch 3212/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.6094 - mae: 0.6751 - mse: 0.609 - 0s 79us/step - loss: 0.8828 - mae: 0.6789 - mse: 0.8828 - val_loss: 218.9705 - val_mae: 11.0809 - val_mse: 218.9705\n",
      "Epoch 3213/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.8637 - mae: 0.6879 - mse: 0.8637 - val_loss: 225.4472 - val_mae: 11.3230 - val_mse: 225.4472\n",
      "Epoch 3214/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6708 - mae: 0.6124 - mse: 0.6708 - val_loss: 217.2722 - val_mae: 11.0946 - val_mse: 217.2722\n",
      "Epoch 3215/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6132 - mae: 0.5753 - mse: 0.6132 - val_loss: 222.4489 - val_mae: 11.2184 - val_mse: 222.4489\n",
      "Epoch 3216/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.5382 - mae: 0.5041 - mse: 0.5382 - val_loss: 221.1528 - val_mae: 11.2514 - val_mse: 221.1528\n",
      "Epoch 3217/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4616 - mae: 0.4813 - mse: 0.4616 - val_loss: 222.0016 - val_mae: 11.1226 - val_mse: 222.0016\n",
      "Epoch 3218/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.6637 - mae: 0.6048 - mse: 0.6637 - val_loss: 222.9756 - val_mae: 11.2970 - val_mse: 222.9756\n",
      "Epoch 3219/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6420 - mae: 0.5474 - mse: 0.6420 - val_loss: 221.0030 - val_mae: 11.1752 - val_mse: 221.0030\n",
      "Epoch 3220/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.9536 - mae: 0.5790 - mse: 0.9536 - val_loss: 224.0702 - val_mae: 11.1900 - val_mse: 224.0702\n",
      "Epoch 3221/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.7190 - mae: 0.5507 - mse: 0.7190 - val_loss: 220.8586 - val_mae: 11.2869 - val_mse: 220.8586\n",
      "Epoch 3222/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5953 - mae: 0.5216 - mse: 0.5953 - val_loss: 221.4872 - val_mae: 11.2045 - val_mse: 221.4872\n",
      "Epoch 3223/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5243 - mae: 0.5005 - mse: 0.5243 - val_loss: 223.3523 - val_mae: 11.2296 - val_mse: 223.3523\n",
      "Epoch 3224/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.6118 - mae: 0.5077 - mse: 0.6118 - val_loss: 219.5298 - val_mae: 11.1706 - val_mse: 219.5298\n",
      "Epoch 3225/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4491 - mae: 0.4177 - mse: 0.4491 - val_loss: 225.1601 - val_mae: 11.2383 - val_mse: 225.1601\n",
      "Epoch 3226/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4444 - mae: 0.4431 - mse: 0.4444 - val_loss: 219.6546 - val_mae: 11.1806 - val_mse: 219.6546\n",
      "Epoch 3227/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4958 - mae: 0.5026 - mse: 0.4958 - val_loss: 217.8625 - val_mae: 11.0955 - val_mse: 217.8625\n",
      "Epoch 3228/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3509 - mae: 0.3990 - mse: 0.3509 - val_loss: 221.5771 - val_mae: 11.2161 - val_mse: 221.5771\n",
      "Epoch 3229/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4042 - mae: 0.3783 - mse: 0.4042 - val_loss: 221.0106 - val_mae: 11.1947 - val_mse: 221.0106\n",
      "Epoch 3230/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3775 - mae: 0.3502 - mse: 0.3775 - val_loss: 220.0662 - val_mae: 11.1484 - val_mse: 220.0662\n",
      "Epoch 3231/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3032 - mae: 0.3227 - mse: 0.3032 - val_loss: 222.3935 - val_mae: 11.1721 - val_mse: 222.3935\n",
      "Epoch 3232/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3129 - mae: 0.3430 - mse: 0.3129 - val_loss: 218.0509 - val_mae: 11.1128 - val_mse: 218.0509\n",
      "Epoch 3233/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2497 - mae: 0.2477 - mse: 0.2497 - val_loss: 224.4016 - val_mae: 11.1868 - val_mse: 224.4016\n",
      "Epoch 3234/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.2849 - mae: 0.2843 - mse: 0.2849 - val_loss: 221.0453 - val_mae: 11.2275 - val_mse: 221.0453\n",
      "Epoch 3235/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2959 - mae: 0.3132 - mse: 0.2959 - val_loss: 218.9781 - val_mae: 11.1364 - val_mse: 218.9781\n",
      "Epoch 3236/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2947 - mae: 0.2834 - mse: 0.2947 - val_loss: 220.3564 - val_mae: 11.1297 - val_mse: 220.3564\n",
      "Epoch 3237/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4049 - mae: 0.3943 - mse: 0.4049 - val_loss: 218.0865 - val_mae: 11.1516 - val_mse: 218.0865\n",
      "Epoch 3238/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3647 - mae: 0.3763 - mse: 0.3647 - val_loss: 224.3799 - val_mae: 11.2120 - val_mse: 224.3799\n",
      "Epoch 3239/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3177 - mae: 0.3573 - mse: 0.3177 - val_loss: 222.3330 - val_mae: 11.2505 - val_mse: 222.3330\n",
      "Epoch 3240/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3414 - mae: 0.3540 - mse: 0.3414 - val_loss: 224.6362 - val_mae: 11.2760 - val_mse: 224.6362\n",
      "Epoch 3241/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3613 - mae: 0.3639 - mse: 0.3613 - val_loss: 220.0814 - val_mae: 11.1371 - val_mse: 220.0814\n",
      "Epoch 3242/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4143 - mae: 0.4083 - mse: 0.4143 - val_loss: 218.6075 - val_mae: 11.1336 - val_mse: 218.6075\n",
      "Epoch 3243/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4171 - mae: 0.4472 - mse: 0.4171 - val_loss: 221.0965 - val_mae: 11.1241 - val_mse: 221.0965\n",
      "Epoch 3244/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3708 - mae: 0.3985 - mse: 0.3708 - val_loss: 221.1408 - val_mae: 11.2044 - val_mse: 221.1408\n",
      "Epoch 3245/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.4354 - mae: 0.4074 - mse: 0.4354 - val_loss: 217.8506 - val_mae: 11.1036 - val_mse: 217.8506\n",
      "Epoch 3246/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4515 - mae: 0.4302 - mse: 0.4515 - val_loss: 227.7006 - val_mae: 11.2795 - val_mse: 227.7006\n",
      "Epoch 3247/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4191 - mae: 0.4211 - mse: 0.4191 - val_loss: 217.9733 - val_mae: 11.1639 - val_mse: 217.9733\n",
      "Epoch 3248/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3748 - mae: 0.3834 - mse: 0.3748 - val_loss: 219.2769 - val_mae: 11.1204 - val_mse: 219.2769\n",
      "Epoch 3249/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3548 - mae: 0.3875 - mse: 0.3548 - val_loss: 223.1175 - val_mae: 11.2618 - val_mse: 223.1175\n",
      "Epoch 3250/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4711 - mae: 0.4504 - mse: 0.4711 - val_loss: 214.8709 - val_mae: 11.0667 - val_mse: 214.8709\n",
      "Epoch 3251/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6183 - mae: 0.5544 - mse: 0.6183 - val_loss: 230.3697 - val_mae: 11.3256 - val_mse: 230.3697\n",
      "Epoch 3252/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.7323 - mae: 0.5839 - mse: 0.7323 - val_loss: 216.3297 - val_mae: 11.2050 - val_mse: 216.3297\n",
      "Epoch 3253/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.7387 - mae: 0.6515 - mse: 0.7387 - val_loss: 224.5550 - val_mae: 11.2346 - val_mse: 224.5550\n",
      "Epoch 3254/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.7167 - mae: 0.6341 - mse: 0.7167 - val_loss: 219.3236 - val_mae: 11.1732 - val_mse: 219.3236\n",
      "Epoch 3255/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 0.7199 - mae: 0.5709 - mse: 0.7199 - val_loss: 219.0351 - val_mae: 11.2244 - val_mse: 219.0351\n",
      "Epoch 3256/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5556 - mae: 0.5448 - mse: 0.5556 - val_loss: 237.4225 - val_mae: 11.4903 - val_mse: 237.4225\n",
      "Epoch 3257/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.8729 - mae: 0.6711 - mse: 0.8729 - val_loss: 214.5821 - val_mae: 11.1835 - val_mse: 214.5821\n",
      "Epoch 3258/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.8808 - mae: 0.7063 - mse: 0.8808 - val_loss: 224.1913 - val_mae: 11.1560 - val_mse: 224.1913\n",
      "Epoch 3259/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.0052 - mae: 0.8540 - mse: 1.0052 - val_loss: 218.3210 - val_mae: 11.1575 - val_mse: 218.3210\n",
      "Epoch 3260/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.2536 - mae: 0.9052 - mse: 1.2536 - val_loss: 230.7342 - val_mae: 11.3277 - val_mse: 230.7342\n",
      "Epoch 3261/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.2965 - mae: 0.8933 - mse: 1.2965 - val_loss: 224.5033 - val_mae: 11.2431 - val_mse: 224.5033\n",
      "Epoch 3262/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.3344 - mae: 0.8044 - mse: 1.3344 - val_loss: 215.6060 - val_mae: 11.1731 - val_mse: 215.6060\n",
      "Epoch 3263/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.5685 - mae: 0.9569 - mse: 1.5685 - val_loss: 224.0439 - val_mae: 11.2146 - val_mse: 224.0439\n",
      "Epoch 3264/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.6122 - mae: 0.9833 - mse: 1.6122 - val_loss: 214.6740 - val_mae: 11.2998 - val_mse: 214.6740\n",
      "Epoch 3265/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.5577 - mae: 0.9706 - mse: 1.5577 - val_loss: 238.5439 - val_mae: 11.5287 - val_mse: 238.5439\n",
      "Epoch 3266/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.3972 - mae: 0.8989 - mse: 1.3972 - val_loss: 225.1839 - val_mae: 11.2741 - val_mse: 225.1839\n",
      "Epoch 3267/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.2317 - mae: 0.8295 - mse: 1.2317 - val_loss: 211.0049 - val_mae: 10.9734 - val_mse: 211.0049\n",
      "Epoch 3268/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.1996 - mae: 0.8215 - mse: 1.1996 - val_loss: 226.6933 - val_mae: 11.2721 - val_mse: 226.6933\n",
      "Epoch 3269/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 1.5800 - mae: 1.0094 - mse: 1.5800 - val_loss: 214.6330 - val_mae: 11.2438 - val_mse: 214.6330\n",
      "Epoch 3270/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.5025 - mae: 0.9355 - mse: 1.5025 - val_loss: 222.2946 - val_mae: 11.2077 - val_mse: 222.2946\n",
      "Epoch 3271/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.4713 - mae: 0.9354 - mse: 1.4713 - val_loss: 223.3333 - val_mae: 11.2748 - val_mse: 223.3333\n",
      "Epoch 3272/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.0827 - mae: 0.7853 - mse: 1.0827 - val_loss: 226.3109 - val_mae: 11.4231 - val_mse: 226.3109\n",
      "Epoch 3273/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.3966 - mae: 0.9493 - mse: 1.3966 - val_loss: 224.0798 - val_mae: 11.2446 - val_mse: 224.0798\n",
      "Epoch 3274/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 1.7825 - mae: 1.0117 - mse: 1.7825 - val_loss: 216.7442 - val_mae: 11.2502 - val_mse: 216.7442\n",
      "Epoch 3275/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.4894 - mae: 0.9849 - mse: 1.4894 - val_loss: 225.3760 - val_mae: 11.1719 - val_mse: 225.3760\n",
      "Epoch 3276/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.4419 - mae: 0.8828 - mse: 1.4419 - val_loss: 216.6390 - val_mae: 11.0911 - val_mse: 216.6390\n",
      "Epoch 3277/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.8066 - mae: 0.9370 - mse: 1.8066 - val_loss: 227.7811 - val_mae: 11.3512 - val_mse: 227.7811\n",
      "Epoch 3278/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.6318 - mae: 0.9556 - mse: 1.6318 - val_loss: 222.1170 - val_mae: 11.2469 - val_mse: 222.1170\n",
      "Epoch 3279/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.4206 - mae: 0.9206 - mse: 1.4206 - val_loss: 217.9329 - val_mae: 11.2088 - val_mse: 217.9329\n",
      "Epoch 3280/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.9313 - mae: 0.7114 - mse: 0.9313 - val_loss: 223.0572 - val_mae: 11.3641 - val_mse: 223.0572\n",
      "Epoch 3281/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 2.6631 - mae: 1.0051 - mse: 2.6631 - val_loss: 221.8224 - val_mae: 11.2364 - val_mse: 221.8224\n",
      "Epoch 3282/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 2.2060 - mae: 1.1759 - mse: 2.2060 - val_loss: 215.3304 - val_mae: 11.2385 - val_mse: 215.3304\n",
      "Epoch 3283/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.6468 - mae: 1.0822 - mse: 1.6468 - val_loss: 236.0149 - val_mae: 11.4025 - val_mse: 236.0149\n",
      "Epoch 3284/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.6120 - mae: 0.9214 - mse: 1.6120 - val_loss: 209.3924 - val_mae: 10.8769 - val_mse: 209.3924\n",
      "Epoch 3285/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.4582 - mae: 0.8377 - mse: 1.4582 - val_loss: 219.7905 - val_mae: 11.1923 - val_mse: 219.7905\n",
      "Epoch 3286/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.3293 - mae: 0.8313 - mse: 1.3293 - val_loss: 223.7536 - val_mae: 11.2774 - val_mse: 223.7536\n",
      "Epoch 3287/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.9773 - mae: 0.7611 - mse: 0.9773 - val_loss: 214.4122 - val_mae: 10.9839 - val_mse: 214.4122\n",
      "Epoch 3288/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.9701 - mae: 0.7613 - mse: 0.9701 - val_loss: 215.4712 - val_mae: 11.2185 - val_mse: 215.4712\n",
      "Epoch 3289/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.0121 - mae: 0.7515 - mse: 1.0121 - val_loss: 223.0801 - val_mae: 11.2495 - val_mse: 223.0801\n",
      "Epoch 3290/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6576 - mae: 0.5561 - mse: 0.6576 - val_loss: 211.6477 - val_mae: 10.9478 - val_mse: 211.6477\n",
      "Epoch 3291/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4892 - mae: 0.4997 - mse: 0.4892 - val_loss: 218.1773 - val_mae: 11.1260 - val_mse: 218.1773\n",
      "Epoch 3292/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4546 - mae: 0.4561 - mse: 0.4546 - val_loss: 220.6629 - val_mae: 11.1727 - val_mse: 220.6629\n",
      "Epoch 3293/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.5651 - mae: 0.5198 - mse: 0.5651 - val_loss: 219.2473 - val_mae: 11.1979 - val_mse: 219.2473\n",
      "Epoch 3294/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5375 - mae: 0.4894 - mse: 0.5375 - val_loss: 220.9743 - val_mae: 11.1651 - val_mse: 220.9743\n",
      "Epoch 3295/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4892 - mae: 0.4760 - mse: 0.4892 - val_loss: 215.4653 - val_mae: 11.0649 - val_mse: 215.4653\n",
      "Epoch 3296/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.6193 - mae: 0.5522 - mse: 0.6193 - val_loss: 221.4832 - val_mae: 11.1418 - val_mse: 221.4832\n",
      "Epoch 3297/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6194 - mae: 0.5595 - mse: 0.6194 - val_loss: 217.8210 - val_mae: 11.1571 - val_mse: 217.8210\n",
      "Epoch 3298/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5067 - mae: 0.5059 - mse: 0.5067 - val_loss: 216.8997 - val_mae: 11.0829 - val_mse: 216.8997\n",
      "Epoch 3299/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4025 - mae: 0.4323 - mse: 0.4025 - val_loss: 214.1833 - val_mae: 11.0464 - val_mse: 214.1833\n",
      "Epoch 3300/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3879 - mae: 0.4011 - mse: 0.3879 - val_loss: 217.5851 - val_mae: 11.1061 - val_mse: 217.5851\n",
      "Epoch 3301/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.3034 - mae: 0.3397 - mse: 0.3034 - val_loss: 219.0933 - val_mae: 11.1215 - val_mse: 219.0933\n",
      "Epoch 3302/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2880 - mae: 0.3168 - mse: 0.2880 - val_loss: 216.7674 - val_mae: 11.1002 - val_mse: 216.7674\n",
      "Epoch 3303/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3101 - mae: 0.3201 - mse: 0.3101 - val_loss: 218.0621 - val_mae: 11.0882 - val_mse: 218.0621\n",
      "Epoch 3304/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3015 - mae: 0.3418 - mse: 0.3015 - val_loss: 216.6432 - val_mae: 11.1132 - val_mse: 216.6432\n",
      "Epoch 3305/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3582 - mae: 0.3860 - mse: 0.3582 - val_loss: 217.8790 - val_mae: 11.0947 - val_mse: 217.8790\n",
      "Epoch 3306/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3054 - mae: 0.3316 - mse: 0.3054 - val_loss: 219.3719 - val_mae: 11.1377 - val_mse: 219.3719\n",
      "Epoch 3307/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2900 - mae: 0.3114 - mse: 0.2900 - val_loss: 217.1075 - val_mae: 11.1085 - val_mse: 217.1075\n",
      "Epoch 3308/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.2815 - mae: 0.2937 - mse: 0.2815 - val_loss: 217.8509 - val_mae: 11.1645 - val_mse: 217.8509\n",
      "Epoch 3309/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2648 - mae: 0.2568 - mse: 0.2648 - val_loss: 218.1833 - val_mae: 11.1013 - val_mse: 218.1833\n",
      "Epoch 3310/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2932 - mae: 0.2970 - mse: 0.2932 - val_loss: 215.2953 - val_mae: 11.0513 - val_mse: 215.2953\n",
      "Epoch 3311/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2788 - mae: 0.2693 - mse: 0.2788 - val_loss: 218.9911 - val_mae: 11.1474 - val_mse: 218.9911\n",
      "Epoch 3312/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.2538 - mae: 0.2299 - mse: 0.2538 - val_loss: 218.1402 - val_mae: 11.1099 - val_mse: 218.1402\n",
      "Epoch 3313/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2626 - mae: 0.2703 - mse: 0.2626 - val_loss: 219.1995 - val_mae: 11.1720 - val_mse: 219.1995\n",
      "Epoch 3314/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2585 - mae: 0.2677 - mse: 0.2585 - val_loss: 217.0302 - val_mae: 11.1055 - val_mse: 217.0302\n",
      "Epoch 3315/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.2130 - mae: 0.2011 - mse: 0.2130 - val_loss: 218.0766 - val_mae: 11.1029 - val_mse: 218.0766\n",
      "Epoch 3316/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2315 - mae: 0.1957 - mse: 0.2315 - val_loss: 219.0602 - val_mae: 11.1553 - val_mse: 219.0602\n",
      "Epoch 3317/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2467 - mae: 0.2097 - mse: 0.2467 - val_loss: 219.8693 - val_mae: 11.1482 - val_mse: 219.8693\n",
      "Epoch 3318/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2693 - mae: 0.2592 - mse: 0.2693 - val_loss: 217.1031 - val_mae: 11.1212 - val_mse: 217.1031\n",
      "Epoch 3319/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2670 - mae: 0.2331 - mse: 0.2670 - val_loss: 217.6176 - val_mae: 11.1112 - val_mse: 217.6176\n",
      "Epoch 3320/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2249 - mae: 0.1724 - mse: 0.2249 - val_loss: 218.5017 - val_mae: 11.1375 - val_mse: 218.5017\n",
      "Epoch 3321/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2155 - mae: 0.1710 - mse: 0.2155 - val_loss: 218.3987 - val_mae: 11.1269 - val_mse: 218.3987\n",
      "Epoch 3322/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2816 - mae: 0.2382 - mse: 0.2816 - val_loss: 217.4339 - val_mae: 11.1299 - val_mse: 217.4339\n",
      "Epoch 3323/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2626 - mae: 0.2799 - mse: 0.2626 - val_loss: 220.9006 - val_mae: 11.1276 - val_mse: 220.9006\n",
      "Epoch 3324/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3321 - mae: 0.3360 - mse: 0.3321 - val_loss: 216.2764 - val_mae: 11.1433 - val_mse: 216.2764\n",
      "Epoch 3325/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3082 - mae: 0.3504 - mse: 0.3082 - val_loss: 219.9215 - val_mae: 11.1568 - val_mse: 219.9215\n",
      "Epoch 3326/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3008 - mae: 0.3153 - mse: 0.3008 - val_loss: 217.0339 - val_mae: 11.1179 - val_mse: 217.0339\n",
      "Epoch 3327/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3120 - mae: 0.3411 - mse: 0.3120 - val_loss: 220.2552 - val_mae: 11.1516 - val_mse: 220.2552\n",
      "Epoch 3328/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3286 - mae: 0.3314 - mse: 0.3286 - val_loss: 216.8674 - val_mae: 11.1225 - val_mse: 216.8674\n",
      "Epoch 3329/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3289 - mae: 0.3371 - mse: 0.3289 - val_loss: 220.6042 - val_mae: 11.1237 - val_mse: 220.6042\n",
      "Epoch 3330/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2934 - mae: 0.3031 - mse: 0.2934 - val_loss: 217.8383 - val_mae: 11.1212 - val_mse: 217.8383\n",
      "Epoch 3331/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.1036 - mae: 0.2228 - mse: 0.103 - 0s 87us/step - loss: 0.2956 - mae: 0.2816 - mse: 0.2956 - val_loss: 217.4309 - val_mae: 11.1053 - val_mse: 217.4309\n",
      "Epoch 3332/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2691 - mae: 0.2678 - mse: 0.2691 - val_loss: 218.7613 - val_mae: 11.1306 - val_mse: 218.7613\n",
      "Epoch 3333/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2827 - mae: 0.2794 - mse: 0.2827 - val_loss: 219.1376 - val_mae: 11.1713 - val_mse: 219.1376\n",
      "Epoch 3334/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3124 - mae: 0.3441 - mse: 0.3124 - val_loss: 220.9458 - val_mae: 11.1638 - val_mse: 220.9458\n",
      "Epoch 3335/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.2862 - mae: 0.3058 - mse: 0.2862 - val_loss: 217.1035 - val_mae: 11.1407 - val_mse: 217.1035\n",
      "Epoch 3336/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2966 - mae: 0.3273 - mse: 0.2966 - val_loss: 221.8290 - val_mae: 11.1989 - val_mse: 221.8290\n",
      "Epoch 3337/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3284 - mae: 0.3309 - mse: 0.3284 - val_loss: 217.0937 - val_mae: 11.1608 - val_mse: 217.0937\n",
      "Epoch 3338/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.2878 - mae: 0.3183 - mse: 0.2878 - val_loss: 217.9473 - val_mae: 11.0837 - val_mse: 217.9473\n",
      "Epoch 3339/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3263 - mae: 0.3407 - mse: 0.3263 - val_loss: 217.5496 - val_mae: 11.1817 - val_mse: 217.5496\n",
      "Epoch 3340/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3336 - mae: 0.3695 - mse: 0.3336 - val_loss: 221.0759 - val_mae: 11.2204 - val_mse: 221.0759\n",
      "Epoch 3341/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3060 - mae: 0.3041 - mse: 0.3060 - val_loss: 217.3683 - val_mae: 11.0871 - val_mse: 217.3683\n",
      "Epoch 3342/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2884 - mae: 0.3015 - mse: 0.2884 - val_loss: 217.8182 - val_mae: 11.1087 - val_mse: 217.8182\n",
      "Epoch 3343/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3230 - mae: 0.3168 - mse: 0.3230 - val_loss: 218.1793 - val_mae: 11.0855 - val_mse: 218.1793\n",
      "Epoch 3344/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.2933 - mae: 0.3066 - mse: 0.2933 - val_loss: 217.4140 - val_mae: 11.1090 - val_mse: 217.4140\n",
      "Epoch 3345/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2805 - mae: 0.2902 - mse: 0.2805 - val_loss: 221.2285 - val_mae: 11.1896 - val_mse: 221.2285\n",
      "Epoch 3346/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2709 - mae: 0.2862 - mse: 0.2709 - val_loss: 215.6524 - val_mae: 11.0864 - val_mse: 215.6524\n",
      "Epoch 3347/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2632 - mae: 0.2618 - mse: 0.2632 - val_loss: 220.6021 - val_mae: 11.1507 - val_mse: 220.6021\n",
      "Epoch 3348/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2566 - mae: 0.2461 - mse: 0.2566 - val_loss: 217.6844 - val_mae: 11.1209 - val_mse: 217.6844\n",
      "Epoch 3349/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2780 - mae: 0.2675 - mse: 0.2780 - val_loss: 217.1679 - val_mae: 11.0473 - val_mse: 217.1679\n",
      "Epoch 3350/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.2827 - mae: 0.2935 - mse: 0.2827 - val_loss: 217.7202 - val_mae: 11.1239 - val_mse: 217.7202\n",
      "Epoch 3351/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2789 - mae: 0.2924 - mse: 0.2789 - val_loss: 222.2400 - val_mae: 11.2021 - val_mse: 222.2400\n",
      "Epoch 3352/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3233 - mae: 0.3474 - mse: 0.3233 - val_loss: 218.3486 - val_mae: 11.1891 - val_mse: 218.3486\n",
      "Epoch 3353/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3348 - mae: 0.3576 - mse: 0.3348 - val_loss: 218.2835 - val_mae: 11.1227 - val_mse: 218.2835\n",
      "Epoch 3354/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3010 - mae: 0.2992 - mse: 0.3010 - val_loss: 218.3021 - val_mae: 11.1700 - val_mse: 218.3021\n",
      "Epoch 3355/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2825 - mae: 0.3272 - mse: 0.2825 - val_loss: 218.6384 - val_mae: 11.1388 - val_mse: 218.6384\n",
      "Epoch 3356/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2742 - mae: 0.2735 - mse: 0.2742 - val_loss: 218.1765 - val_mae: 11.1130 - val_mse: 218.1765\n",
      "Epoch 3357/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.2774 - mae: 0.2852 - mse: 0.2774 - val_loss: 221.8335 - val_mae: 11.2026 - val_mse: 221.8335\n",
      "Epoch 3358/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2663 - mae: 0.2534 - mse: 0.2663 - val_loss: 216.3286 - val_mae: 11.1136 - val_mse: 216.3286\n",
      "Epoch 3359/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2897 - mae: 0.2711 - mse: 0.2897 - val_loss: 221.4505 - val_mae: 11.1835 - val_mse: 221.4505\n",
      "Epoch 3360/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2749 - mae: 0.2760 - mse: 0.2749 - val_loss: 219.0131 - val_mae: 11.1412 - val_mse: 219.0131\n",
      "Epoch 3361/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.2886 - mae: 0.3020 - mse: 0.2886 - val_loss: 220.2651 - val_mae: 11.1723 - val_mse: 220.2651\n",
      "Epoch 3362/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3117 - mae: 0.3243 - mse: 0.3117 - val_loss: 220.5004 - val_mae: 11.1267 - val_mse: 220.5004\n",
      "Epoch 3363/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3808 - mae: 0.3955 - mse: 0.3808 - val_loss: 216.1870 - val_mae: 11.1166 - val_mse: 216.1870\n",
      "Epoch 3364/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.5482 - mae: 0.5147 - mse: 0.5482 - val_loss: 221.1996 - val_mae: 11.1877 - val_mse: 221.1996\n",
      "Epoch 3365/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4167 - mae: 0.4312 - mse: 0.4167 - val_loss: 218.0888 - val_mae: 11.1203 - val_mse: 218.0888\n",
      "Epoch 3366/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3642 - mae: 0.3877 - mse: 0.3642 - val_loss: 217.1208 - val_mae: 11.1002 - val_mse: 217.1208\n",
      "Epoch 3367/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3225 - mae: 0.3482 - mse: 0.3225 - val_loss: 220.4094 - val_mae: 11.1955 - val_mse: 220.4094\n",
      "Epoch 3368/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2867 - mae: 0.2979 - mse: 0.2867 - val_loss: 218.7394 - val_mae: 11.1450 - val_mse: 218.7394\n",
      "Epoch 3369/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.2415 - mae: 0.2443 - mse: 0.2415 - val_loss: 217.2449 - val_mae: 11.0981 - val_mse: 217.2449\n",
      "Epoch 3370/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.2512 - mae: 0.2474 - mse: 0.2512 - val_loss: 216.7795 - val_mae: 11.1033 - val_mse: 216.7795\n",
      "Epoch 3371/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2612 - mae: 0.2299 - mse: 0.2612 - val_loss: 222.5157 - val_mae: 11.1942 - val_mse: 222.5157\n",
      "Epoch 3372/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2644 - mae: 0.2726 - mse: 0.2644 - val_loss: 219.2644 - val_mae: 11.1691 - val_mse: 219.2644\n",
      "Epoch 3373/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2569 - mae: 0.2722 - mse: 0.2569 - val_loss: 221.7454 - val_mae: 11.1616 - val_mse: 221.7454\n",
      "Epoch 3374/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2622 - mae: 0.2650 - mse: 0.2622 - val_loss: 216.7164 - val_mae: 11.1205 - val_mse: 216.7164\n",
      "Epoch 3375/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2466 - mae: 0.2445 - mse: 0.2466 - val_loss: 220.0619 - val_mae: 11.1520 - val_mse: 220.0619\n",
      "Epoch 3376/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2192 - mae: 0.2062 - mse: 0.2192 - val_loss: 220.7641 - val_mae: 11.2214 - val_mse: 220.7641\n",
      "Epoch 3377/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2530 - mae: 0.2406 - mse: 0.2530 - val_loss: 218.9880 - val_mae: 11.1541 - val_mse: 218.9880\n",
      "Epoch 3378/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2576 - mae: 0.2464 - mse: 0.2576 - val_loss: 222.6677 - val_mae: 11.2231 - val_mse: 222.6677\n",
      "Epoch 3379/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2541 - mae: 0.2482 - mse: 0.2541 - val_loss: 217.4723 - val_mae: 11.1154 - val_mse: 217.4723\n",
      "Epoch 3380/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3445 - mae: 0.3183 - mse: 0.3445 - val_loss: 215.8548 - val_mae: 11.1107 - val_mse: 215.8548\n",
      "Epoch 3381/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.3852 - mae: 0.3772 - mse: 0.3852 - val_loss: 223.1539 - val_mae: 11.2332 - val_mse: 223.1539\n",
      "Epoch 3382/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3739 - mae: 0.3956 - mse: 0.3739 - val_loss: 215.5069 - val_mae: 11.0397 - val_mse: 215.5069\n",
      "Epoch 3383/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4244 - mae: 0.4319 - mse: 0.4244 - val_loss: 216.2272 - val_mae: 11.0649 - val_mse: 216.2272\n",
      "Epoch 3384/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4585 - mae: 0.4561 - mse: 0.4585 - val_loss: 222.7428 - val_mae: 11.1585 - val_mse: 222.7428\n",
      "Epoch 3385/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4054 - mae: 0.4154 - mse: 0.4054 - val_loss: 215.1296 - val_mae: 11.0641 - val_mse: 215.1296\n",
      "Epoch 3386/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3944 - mae: 0.4155 - mse: 0.3944 - val_loss: 221.1382 - val_mae: 11.1245 - val_mse: 221.1382\n",
      "Epoch 3387/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.3292 - mae: 0.3291 - mse: 0.3292 - val_loss: 216.8288 - val_mae: 11.0909 - val_mse: 216.8288\n",
      "Epoch 3388/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3936 - mae: 0.3899 - mse: 0.3936 - val_loss: 217.9277 - val_mae: 11.1478 - val_mse: 217.9277\n",
      "Epoch 3389/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3539 - mae: 0.3975 - mse: 0.3539 - val_loss: 222.6029 - val_mae: 11.1528 - val_mse: 222.6029\n",
      "Epoch 3390/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3910 - mae: 0.4073 - mse: 0.3910 - val_loss: 218.1269 - val_mae: 11.1948 - val_mse: 218.1269\n",
      "Epoch 3391/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4130 - mae: 0.3970 - mse: 0.4130 - val_loss: 221.5113 - val_mae: 11.1993 - val_mse: 221.5113\n",
      "Epoch 3392/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3207 - mae: 0.3393 - mse: 0.3207 - val_loss: 218.7248 - val_mae: 11.1147 - val_mse: 218.7248\n",
      "Epoch 3393/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3144 - mae: 0.3289 - mse: 0.3144 - val_loss: 217.7975 - val_mae: 11.1827 - val_mse: 217.7975\n",
      "Epoch 3394/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2494 - mae: 0.2640 - mse: 0.2494 - val_loss: 221.0555 - val_mae: 11.1752 - val_mse: 221.0555\n",
      "Epoch 3395/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2808 - mae: 0.2832 - mse: 0.2808 - val_loss: 219.3344 - val_mae: 11.1724 - val_mse: 219.3344\n",
      "Epoch 3396/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2743 - mae: 0.2601 - mse: 0.2743 - val_loss: 219.5320 - val_mae: 11.1693 - val_mse: 219.5320\n",
      "Epoch 3397/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2422 - mae: 0.2038 - mse: 0.2422 - val_loss: 220.5518 - val_mae: 11.1447 - val_mse: 220.5518\n",
      "Epoch 3398/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2239 - mae: 0.2165 - mse: 0.2239 - val_loss: 219.7712 - val_mae: 11.1792 - val_mse: 219.7712\n",
      "Epoch 3399/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2502 - mae: 0.2422 - mse: 0.2502 - val_loss: 221.3927 - val_mae: 11.1756 - val_mse: 221.3927\n",
      "Epoch 3400/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.2394 - mae: 0.2397 - mse: 0.2394 - val_loss: 221.1209 - val_mae: 11.2239 - val_mse: 221.1209\n",
      "Epoch 3401/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2402 - mae: 0.2398 - mse: 0.2402 - val_loss: 220.6868 - val_mae: 11.1674 - val_mse: 220.6868\n",
      "Epoch 3402/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3019 - mae: 0.2934 - mse: 0.3019 - val_loss: 219.8689 - val_mae: 11.1569 - val_mse: 219.8689\n",
      "Epoch 3403/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2926 - mae: 0.3174 - mse: 0.2926 - val_loss: 219.2834 - val_mae: 11.1441 - val_mse: 219.2834\n",
      "Epoch 3404/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2895 - mae: 0.3172 - mse: 0.2895 - val_loss: 223.4633 - val_mae: 11.1955 - val_mse: 223.4633\n",
      "Epoch 3405/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3222 - mae: 0.3609 - mse: 0.3222 - val_loss: 217.9250 - val_mae: 11.1733 - val_mse: 217.9250\n",
      "Epoch 3406/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3164 - mae: 0.3597 - mse: 0.3164 - val_loss: 220.3387 - val_mae: 11.1687 - val_mse: 220.3387\n",
      "Epoch 3407/3500\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.1426 - mae: 0.2991 - mse: 0.142 - 0s 71us/step - loss: 0.3278 - mae: 0.3249 - mse: 0.3278 - val_loss: 220.8044 - val_mae: 11.1994 - val_mse: 220.8044\n",
      "Epoch 3408/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3446 - mae: 0.3721 - mse: 0.3446 - val_loss: 220.0676 - val_mae: 11.1600 - val_mse: 220.0676\n",
      "Epoch 3409/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3247 - mae: 0.3518 - mse: 0.3247 - val_loss: 220.3221 - val_mae: 11.1379 - val_mse: 220.3221\n",
      "Epoch 3410/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3521 - mae: 0.3493 - mse: 0.3521 - val_loss: 223.5988 - val_mae: 11.2546 - val_mse: 223.5988\n",
      "Epoch 3411/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3300 - mae: 0.3403 - mse: 0.3300 - val_loss: 219.1099 - val_mae: 11.1395 - val_mse: 219.1099\n",
      "Epoch 3412/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3300 - mae: 0.3259 - mse: 0.3300 - val_loss: 218.9214 - val_mae: 11.1218 - val_mse: 218.9214\n",
      "Epoch 3413/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3180 - mae: 0.2921 - mse: 0.3180 - val_loss: 221.2386 - val_mae: 11.2363 - val_mse: 221.2386\n",
      "Epoch 3414/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3067 - mae: 0.3084 - mse: 0.3067 - val_loss: 221.9308 - val_mae: 11.2318 - val_mse: 221.9308\n",
      "Epoch 3415/3500\n",
      "126/126 [==============================] - 0s 253us/step - loss: 0.3355 - mae: 0.3404 - mse: 0.3355 - val_loss: 219.7622 - val_mae: 11.1642 - val_mse: 219.7622\n",
      "Epoch 3416/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3772 - mae: 0.3656 - mse: 0.3772 - val_loss: 220.7819 - val_mae: 11.1463 - val_mse: 220.7819\n",
      "Epoch 3417/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2558 - mae: 0.2832 - mse: 0.2558 - val_loss: 218.8568 - val_mae: 11.1418 - val_mse: 218.8568\n",
      "Epoch 3418/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2900 - mae: 0.2934 - mse: 0.2900 - val_loss: 223.5206 - val_mae: 11.1989 - val_mse: 223.5206\n",
      "Epoch 3419/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3358 - mae: 0.3778 - mse: 0.3358 - val_loss: 220.8815 - val_mae: 11.1953 - val_mse: 220.8815\n",
      "Epoch 3420/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3701 - mae: 0.3837 - mse: 0.3701 - val_loss: 220.2528 - val_mae: 11.1171 - val_mse: 220.2528\n",
      "Epoch 3421/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.3312 - mae: 0.3425 - mse: 0.3312 - val_loss: 219.4768 - val_mae: 11.1425 - val_mse: 219.4768\n",
      "Epoch 3422/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2586 - mae: 0.2427 - mse: 0.2586 - val_loss: 219.3760 - val_mae: 11.1412 - val_mse: 219.3760\n",
      "Epoch 3423/3500\n",
      "126/126 [==============================] - 0s 80us/step - loss: 0.3065 - mae: 0.2462 - mse: 0.3065 - val_loss: 219.9408 - val_mae: 11.1953 - val_mse: 219.9408\n",
      "Epoch 3424/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2386 - mae: 0.2379 - mse: 0.2386 - val_loss: 220.5347 - val_mae: 11.1294 - val_mse: 220.5347\n",
      "Epoch 3425/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2610 - mae: 0.2701 - mse: 0.2610 - val_loss: 221.8181 - val_mae: 11.1901 - val_mse: 221.8181\n",
      "Epoch 3426/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2791 - mae: 0.2674 - mse: 0.2791 - val_loss: 221.0286 - val_mae: 11.1906 - val_mse: 221.0286\n",
      "Epoch 3427/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.2322 - mae: 0.2309 - mse: 0.2322 - val_loss: 220.1775 - val_mae: 11.1666 - val_mse: 220.1775\n",
      "Epoch 3428/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2427 - mae: 0.2256 - mse: 0.2427 - val_loss: 220.8754 - val_mae: 11.2108 - val_mse: 220.8754\n",
      "Epoch 3429/3500\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.2661 - mae: 0.2258 - mse: 0.2661 - val_loss: 220.4383 - val_mae: 11.1640 - val_mse: 220.4383\n",
      "Epoch 3430/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.2206 - mae: 0.2263 - mse: 0.2206 - val_loss: 219.4535 - val_mae: 11.1835 - val_mse: 219.4535\n",
      "Epoch 3431/3500\n",
      "126/126 [==============================] - 0s 135us/step - loss: 0.2720 - mae: 0.2494 - mse: 0.2720 - val_loss: 222.4090 - val_mae: 11.1813 - val_mse: 222.4090\n",
      "Epoch 3432/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2743 - mae: 0.2656 - mse: 0.2743 - val_loss: 222.4572 - val_mae: 11.2212 - val_mse: 222.4572\n",
      "Epoch 3433/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.2816 - mae: 0.3003 - mse: 0.2816 - val_loss: 219.5776 - val_mae: 11.1488 - val_mse: 219.5776\n",
      "Epoch 3434/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.2926 - mae: 0.2967 - mse: 0.2926 - val_loss: 220.7493 - val_mae: 11.1921 - val_mse: 220.7493\n",
      "Epoch 3435/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.2855 - mae: 0.2950 - mse: 0.2855 - val_loss: 220.8561 - val_mae: 11.1474 - val_mse: 220.8561\n",
      "Epoch 3436/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2939 - mae: 0.2939 - mse: 0.2939 - val_loss: 222.2693 - val_mae: 11.2221 - val_mse: 222.2693\n",
      "Epoch 3437/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.2588 - mae: 0.2415 - mse: 0.2588 - val_loss: 219.5412 - val_mae: 11.1357 - val_mse: 219.5412\n",
      "Epoch 3438/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2743 - mae: 0.2544 - mse: 0.2743 - val_loss: 218.3578 - val_mae: 11.1154 - val_mse: 218.3578\n",
      "Epoch 3439/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.2489 - mae: 0.2283 - mse: 0.2489 - val_loss: 224.0087 - val_mae: 11.2387 - val_mse: 224.0087\n",
      "Epoch 3440/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.2822 - mae: 0.2829 - mse: 0.2822 - val_loss: 220.3377 - val_mae: 11.1488 - val_mse: 220.3377\n",
      "Epoch 3441/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.3919 - mae: 0.3677 - mse: 0.3919 - val_loss: 220.3745 - val_mae: 11.1780 - val_mse: 220.3745\n",
      "Epoch 3442/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3645 - mae: 0.3816 - mse: 0.3645 - val_loss: 218.7661 - val_mae: 11.1215 - val_mse: 218.7661\n",
      "Epoch 3443/3500\n",
      "126/126 [==============================] - 0s 142us/step - loss: 0.5334 - mae: 0.5267 - mse: 0.5334 - val_loss: 223.5505 - val_mae: 11.1532 - val_mse: 223.5505\n",
      "Epoch 3444/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.7720 - mae: 0.6518 - mse: 0.7720 - val_loss: 218.1597 - val_mae: 11.1633 - val_mse: 218.1597\n",
      "Epoch 3445/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 0.6345 - mae: 0.5447 - mse: 0.6345 - val_loss: 218.8497 - val_mae: 11.1342 - val_mse: 218.8497\n",
      "Epoch 3446/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.6555 - mae: 0.5954 - mse: 0.6555 - val_loss: 228.6096 - val_mae: 11.3286 - val_mse: 228.6096\n",
      "Epoch 3447/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4906 - mae: 0.4962 - mse: 0.4906 - val_loss: 217.3741 - val_mae: 11.1601 - val_mse: 217.3741\n",
      "Epoch 3448/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5392 - mae: 0.5167 - mse: 0.5392 - val_loss: 228.8822 - val_mae: 11.2854 - val_mse: 228.8822\n",
      "Epoch 3449/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3612 - mae: 0.4062 - mse: 0.3612 - val_loss: 219.4601 - val_mae: 11.1562 - val_mse: 219.4601\n",
      "Epoch 3450/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.4345 - mae: 0.4650 - mse: 0.4345 - val_loss: 225.0649 - val_mae: 11.2557 - val_mse: 225.0649\n",
      "Epoch 3451/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.4057 - mae: 0.4294 - mse: 0.4057 - val_loss: 217.1238 - val_mae: 11.0708 - val_mse: 217.1238\n",
      "Epoch 3452/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4058 - mae: 0.4258 - mse: 0.4058 - val_loss: 221.6932 - val_mae: 11.2002 - val_mse: 221.6932\n",
      "Epoch 3453/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5028 - mae: 0.4799 - mse: 0.5028 - val_loss: 226.5110 - val_mae: 11.2155 - val_mse: 226.5110\n",
      "Epoch 3454/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.5627 - mae: 0.5734 - mse: 0.5627 - val_loss: 218.8702 - val_mae: 11.1125 - val_mse: 218.8702\n",
      "Epoch 3455/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.4310 - mae: 0.4679 - mse: 0.4310 - val_loss: 225.5122 - val_mae: 11.2846 - val_mse: 225.5122\n",
      "Epoch 3456/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.5029 - mae: 0.4992 - mse: 0.5029 - val_loss: 223.2206 - val_mae: 11.2342 - val_mse: 223.2206\n",
      "Epoch 3457/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 0.5267 - mae: 0.5328 - mse: 0.5267 - val_loss: 221.7109 - val_mae: 11.2910 - val_mse: 221.7109\n",
      "Epoch 3458/3500\n",
      "126/126 [==============================] - 0s 111us/step - loss: 0.8920 - mae: 0.7173 - mse: 0.8920 - val_loss: 220.0617 - val_mae: 11.1816 - val_mse: 220.0617\n",
      "Epoch 3459/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.8176 - mae: 0.6866 - mse: 0.8176 - val_loss: 221.3807 - val_mae: 11.1359 - val_mse: 221.3807\n",
      "Epoch 3460/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.9193 - mae: 0.7031 - mse: 0.9193 - val_loss: 213.4341 - val_mae: 11.0937 - val_mse: 213.4341\n",
      "Epoch 3461/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.9286 - mae: 0.7060 - mse: 0.9286 - val_loss: 228.0161 - val_mae: 11.2061 - val_mse: 228.0161\n",
      "Epoch 3462/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.9669 - mae: 0.7846 - mse: 0.9669 - val_loss: 218.8233 - val_mae: 11.1601 - val_mse: 218.8233\n",
      "Epoch 3463/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.6103 - mae: 0.5636 - mse: 0.6103 - val_loss: 219.4474 - val_mae: 11.0905 - val_mse: 219.4474\n",
      "Epoch 3464/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4601 - mae: 0.4377 - mse: 0.4601 - val_loss: 222.9308 - val_mae: 11.1715 - val_mse: 222.9308\n",
      "Epoch 3465/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4298 - mae: 0.4217 - mse: 0.4298 - val_loss: 221.2505 - val_mae: 11.2135 - val_mse: 221.2505\n",
      "Epoch 3466/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.4404 - mae: 0.4288 - mse: 0.4404 - val_loss: 227.5403 - val_mae: 11.2444 - val_mse: 227.5403\n",
      "Epoch 3467/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4193 - mae: 0.4231 - mse: 0.4193 - val_loss: 216.3459 - val_mae: 11.0559 - val_mse: 216.3459\n",
      "Epoch 3468/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3493 - mae: 0.3867 - mse: 0.3493 - val_loss: 221.8660 - val_mae: 11.1698 - val_mse: 221.8660\n",
      "Epoch 3469/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 0.3831 - mae: 0.3810 - mse: 0.3831 - val_loss: 222.8523 - val_mae: 11.2138 - val_mse: 222.8523\n",
      "Epoch 3470/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3895 - mae: 0.4235 - mse: 0.3895 - val_loss: 220.5000 - val_mae: 11.1116 - val_mse: 220.5000\n",
      "Epoch 3471/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 0.3933 - mae: 0.3896 - mse: 0.3933 - val_loss: 223.1712 - val_mae: 11.2495 - val_mse: 223.1712\n",
      "Epoch 3472/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3595 - mae: 0.3868 - mse: 0.3595 - val_loss: 220.7403 - val_mae: 11.1559 - val_mse: 220.7403\n",
      "Epoch 3473/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.4088 - mae: 0.3877 - mse: 0.4088 - val_loss: 225.9564 - val_mae: 11.2577 - val_mse: 225.9564\n",
      "Epoch 3474/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.3881 - mae: 0.4179 - mse: 0.3881 - val_loss: 218.3028 - val_mae: 11.1303 - val_mse: 218.3028\n",
      "Epoch 3475/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 0.3935 - mae: 0.4118 - mse: 0.3935 - val_loss: 221.8766 - val_mae: 11.1129 - val_mse: 221.8766\n",
      "Epoch 3476/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4082 - mae: 0.4306 - mse: 0.4082 - val_loss: 223.5789 - val_mae: 11.2310 - val_mse: 223.5789\n",
      "Epoch 3477/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.4602 - mae: 0.4158 - mse: 0.4602 - val_loss: 219.9589 - val_mae: 11.1422 - val_mse: 219.9589\n",
      "Epoch 3478/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.6287 - mae: 0.4665 - mse: 0.6287 - val_loss: 219.1567 - val_mae: 11.1049 - val_mse: 219.1567\n",
      "Epoch 3479/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.7180 - mae: 0.4884 - mse: 0.7180 - val_loss: 222.3094 - val_mae: 11.2819 - val_mse: 222.3094\n",
      "Epoch 3480/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 0.8852 - mae: 0.7389 - mse: 0.8852 - val_loss: 227.8403 - val_mae: 11.2760 - val_mse: 227.8403\n",
      "Epoch 3481/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.2543 - mae: 0.7955 - mse: 1.2543 - val_loss: 216.2694 - val_mae: 11.1414 - val_mse: 216.2694\n",
      "Epoch 3482/3500\n",
      "126/126 [==============================] - 0s 119us/step - loss: 1.0355 - mae: 0.7975 - mse: 1.0355 - val_loss: 219.4532 - val_mae: 11.0963 - val_mse: 219.4532\n",
      "Epoch 3483/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.1399 - mae: 0.8062 - mse: 1.1399 - val_loss: 220.4343 - val_mae: 11.1428 - val_mse: 220.4343\n",
      "Epoch 3484/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.2835 - mae: 0.8841 - mse: 1.2835 - val_loss: 226.9806 - val_mae: 11.2940 - val_mse: 226.9806\n",
      "Epoch 3485/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.5955 - mae: 0.9533 - mse: 1.5955 - val_loss: 219.0798 - val_mae: 11.1342 - val_mse: 219.0798\n",
      "Epoch 3486/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.4186 - mae: 0.8896 - mse: 1.4186 - val_loss: 230.6705 - val_mae: 11.3197 - val_mse: 230.6705\n",
      "Epoch 3487/3500\n",
      "126/126 [==============================] - 0s 71us/step - loss: 1.3209 - mae: 0.9205 - mse: 1.3209 - val_loss: 210.7575 - val_mae: 11.0091 - val_mse: 210.7575\n",
      "Epoch 3488/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.1639 - mae: 0.8436 - mse: 1.1639 - val_loss: 221.6969 - val_mae: 11.0270 - val_mse: 221.6969\n",
      "Epoch 3489/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.4927 - mae: 0.9699 - mse: 1.4927 - val_loss: 217.2301 - val_mae: 11.1536 - val_mse: 217.2301\n",
      "Epoch 3490/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.7311 - mae: 1.0160 - mse: 1.7311 - val_loss: 230.2464 - val_mae: 11.3147 - val_mse: 230.2464\n",
      "Epoch 3491/3500\n",
      "126/126 [==============================] - 0s 127us/step - loss: 2.1374 - mae: 1.0601 - mse: 2.1374 - val_loss: 221.5075 - val_mae: 11.2235 - val_mse: 221.5075\n",
      "Epoch 3492/3500\n",
      "126/126 [==============================] - 0s 95us/step - loss: 1.8727 - mae: 0.9973 - mse: 1.8727 - val_loss: 214.4225 - val_mae: 11.0823 - val_mse: 214.4225\n",
      "Epoch 3493/3500\n",
      "126/126 [==============================] - 0s 103us/step - loss: 1.9958 - mae: 1.1381 - mse: 1.9958 - val_loss: 232.9176 - val_mae: 11.2600 - val_mse: 232.9176\n",
      "Epoch 3494/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 2.0864 - mae: 1.1300 - mse: 2.0864 - val_loss: 208.6053 - val_mae: 11.1351 - val_mse: 208.6053\n",
      "Epoch 3495/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 2.1017 - mae: 1.1585 - mse: 2.1017 - val_loss: 235.0299 - val_mae: 11.3902 - val_mse: 235.0299\n",
      "Epoch 3496/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 1.8811 - mae: 1.0646 - mse: 1.8811 - val_loss: 213.8523 - val_mae: 11.0488 - val_mse: 213.8523\n",
      "Epoch 3497/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.7564 - mae: 0.9986 - mse: 1.7564 - val_loss: 216.5511 - val_mae: 11.0085 - val_mse: 216.5511\n",
      "Epoch 3498/3500\n",
      "126/126 [==============================] - 0s 63us/step - loss: 1.6292 - mae: 0.9525 - mse: 1.6292 - val_loss: 229.7513 - val_mae: 11.3186 - val_mse: 229.7513\n",
      "Epoch 3499/3500\n",
      "126/126 [==============================] - 0s 79us/step - loss: 1.4274 - mae: 0.9546 - mse: 1.4274 - val_loss: 214.9492 - val_mae: 11.2685 - val_mse: 214.9492\n",
      "Epoch 3500/3500\n",
      "126/126 [==============================] - 0s 87us/step - loss: 0.9805 - mae: 0.7149 - mse: 0.9805 - val_loss: 222.9411 - val_mae: 11.1335 - val_mse: 222.9411\n"
     ]
    }
   ],
   "source": [
    "# predicting our ANN\n",
    "# xtrain and y train must be a list so convert it\n",
    "#we have epoch of 1500 i.e it will pass batch size of 32 1500 times\n",
    "# the data will stored in model 1 to see visulaize in tensor board\n",
    "model.fit(x_train,y_train, batch_size=32, nb_epoch=3500, validation_split=0.2, callbacks=[get_tensorboard('Model 1')])\n",
    "\n",
    "#saving our model\n",
    "model.save(\"model_test_score.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4 predicting the test data using our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[104.32881 ],\n",
       "       [ 78.875374],\n",
       "       [ 83.506584],\n",
       "       [ 85.72881 ],\n",
       "       [ 59.40713 ]], dtype=float32)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading our model\n",
    "model= keras.models.load_model(\"model_test_score.h5\")\n",
    "\n",
    "#predicting the model using our testing data\n",
    "#Predicing entire test data\n",
    "prediction=model.predict(x_test)\n",
    "prediction[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189    82.6\n",
       "127    58.9\n",
       "67     73.3\n",
       "11     63.7\n",
       "162    54.6\n",
       "Name: Mem_Score_After, dtype: float64"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.5 Comaring predicted data with actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 PREDICTED_SCORE\n",
      "Mem_Score_After                 \n",
      "82.6                  104.328812\n",
      "58.9                   78.875374\n",
      "73.3                   83.506584\n",
      "63.7                   85.728813\n",
      "54.6                   59.407131\n",
      "39.0                   31.389380\n",
      "65.2                   98.168259\n",
      "66.6                   46.347561\n",
      "56.1                   59.365570\n",
      "32.8                   84.556358\n",
      "61.5                   68.329018\n",
      "47.0                   44.157715\n",
      "90.0                   68.416389\n",
      "47.1                   39.904900\n",
      "44.2                   40.280529\n",
      "38.2                   33.565891\n",
      "60.8                   57.712635\n",
      "40.3                   13.526177\n",
      "44.1                   49.982922\n",
      "63.0                   50.150425\n",
      "40.7                   34.634052\n",
      "83.6                  101.272148\n",
      "44.5                   22.682634\n",
      "55.5                   34.886753\n",
      "63.6                   78.630508\n",
      "49.2                   34.084431\n",
      "56.0                   68.687828\n",
      "90.0                  113.626198\n",
      "59.2                   50.400620\n",
      "53.4                   33.345200\n",
      "54.0                   64.742058\n",
      "72.5                   56.767803\n",
      "61.5                   63.590652\n",
      "73.3                   71.329102\n",
      "83.1                   95.179726\n",
      "71.9                   83.088005\n",
      "96.0                  120.109138\n",
      "42.2                   51.971928\n",
      "88.1                   81.092819\n",
      "52.1                   46.254299\n"
     ]
    }
   ],
   "source": [
    "# comparing our actual test score vs predicted score of x_test data\n",
    "predict_vs_actual=pd.DataFrame(prediction, y_test, columns=[\"PREDICTED_SCORE\"])\n",
    "print(predict_vs_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.85972695, -0.85972695, -0.90453403,  1.62368828, -0.61588176,\n",
       "         0.491325  ,  1.20432067,  0.02436952]])"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#single test row\n",
    "x_test[1:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[78.87537]]\n"
     ]
    }
   ],
   "source": [
    "#passing in a single data\n",
    "prediction=model.predict(x_test[1:2])\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a costom data function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_stats=np.zeros((1, 8))\n",
    "\n",
    "def score_predictor(status, Drug, age, dosage, score_before):\n",
    "    if status=='H':\n",
    "        our_stats[0][0]=1\n",
    "    else:\n",
    "        our_stats[0][1]=1\n",
    "    if Drug=='A':\n",
    "        our_stats[0][2]=1\n",
    "    elif Drug=='S':\n",
    "        our_stats[0][3]=1\n",
    "    else:\n",
    "        our_stats[0][4]=1\n",
    "        \n",
    "        #updating our last 3 elements\n",
    "    our_stats[:,5:]=[age,dosage,score_before]\n",
    "\n",
    "    return sc.transform(our_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.85972695 -0.85972695  1.1055416  -0.61588176 -0.61588176 -0.69259066\n",
      "  -1.14557332  0.86708333]]\n"
     ]
    }
   ],
   "source": [
    "#our manual data\n",
    "new_stats=score_predictor('H','A',30,1,70.3)\n",
    "#our model Before prediction\n",
    "print(new_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[60.696754]]\n"
     ]
    }
   ],
   "source": [
    "new_prediction=model.predict(new_stats)\n",
    "print(new_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
